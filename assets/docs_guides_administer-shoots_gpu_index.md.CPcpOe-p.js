import{_ as i,c as a,o as e,a2 as n}from"./chunks/framework.Bfq10Vlj.js";const c=JSON.parse('{"title":"AI/ML Workloads on GPU Clusters","description":"Setting up a GPU Enabled Cluster for AI/ML Workloads","frontmatter":{"authors":[{"email":"vedran.lerenc@sap.com"}],"category":"Setup","description":"Setting up a GPU Enabled Cluster for AI/ML Workloads","github_repo":"https://github.com/gardener/documentation","github_subdir":"website/documentation/guides/administer-shoots","level":"intermediate","linkTitle":"AI/ML Workloads on GPU Clusters","newsSubtitle":"Setting up a GPU Enabled Cluster for AI/ML Workloads","params":{"github_branch":"master"},"path_base_for_github_subdir":{"from":"content/docs/guides/administer-shoots/gpu.md","to":"gpu.md"},"publishdate":"2025-10-26","scope":"app-developer","title":"AI/ML Workloads on GPU Clusters","prev":false,"next":false},"headers":[],"relativePath":"docs/guides/administer-shoots/gpu/index.md","filePath":"docs/guides/administer-shoots/gpu.md","lastUpdated":null}'),t={name:"docs/guides/administer-shoots/gpu/index.md"};function l(r,s,p,h,o,k){return e(),a("div",null,s[0]||(s[0]=[n(`<h1 id="ai-ml-workloads-on-gpu-clusters" tabindex="-1">AI/ML Workloads on GPU Clusters <a class="header-anchor" href="#ai-ml-workloads-on-gpu-clusters" aria-label="Permalink to &quot;AI/ML Workloads on GPU Clusters&quot;">​</a></h1><div class="note custom-block github-alert"><p class="custom-block-title">NOTE</p><p>This guide focuses on NVIDIA accelerators, as Gardener provides first-class support for them, enabling a robust and conformant environment for GPU-powered applications.</p></div><h2 id="introduction" tabindex="-1">Introduction <a class="header-anchor" href="#introduction" aria-label="Permalink to &quot;Introduction&quot;">​</a></h2><p>Gardener has joined the <a href="https://github.com/cncf/ai-conformance" target="_blank" rel="noreferrer">CNCF Kubernetes AI Conformance</a> program. This certification validates that Gardener provides a standardized, reliable platform capable of handling the demanding requirements of modern AI and machine learning workloads, ensuring that your GPU-accelerated applications run on a proven and conformant infrastructure.</p><p>This guide walks you through provisioning GPU-enabled Kubernetes clusters with Gardener and configuring them for production AI/ML workloads.</p><h2 id="prerequisites" tabindex="-1">Prerequisites <a class="header-anchor" href="#prerequisites" aria-label="Permalink to &quot;Prerequisites&quot;">​</a></h2><p>Before you begin, ensure you have:</p><ul><li>Access to a Gardener project with appropriate permissions</li><li>Requested GPU quota from your cloud provider (e.g., <code>g4dn</code> instances on AWS, <code>Standard_NC</code> series on Azure, or <code>n1-standard</code> with GPUs on GCP)</li><li><code>kubectl</code> and <code>helm</code> installed locally</li></ul><div class="important custom-block github-alert"><p class="custom-block-title">IMPORTANT</p><p>Due to high demand for GPU resources, availability may be limited in certain regions. Plan your cluster location accordingly and request quota in advance.</p></div><h2 id="step-1-provision-a-gpu-enabled-cluster" tabindex="-1">Step 1: Provision a GPU-Enabled Cluster <a class="header-anchor" href="#step-1-provision-a-gpu-enabled-cluster" aria-label="Permalink to &quot;Step 1: Provision a GPU-Enabled Cluster&quot;">​</a></h2><p>To create a Gardener-managed Kubernetes cluster with GPU-enabled worker nodes, you need to configure the <code>provider.workers</code> section in your <code>shoot</code> manifest with a GPU-enabled machine type and a compatible operating system.</p><h3 id="shoot-configuration-example" tabindex="-1">Shoot Configuration Example <a class="header-anchor" href="#shoot-configuration-example" aria-label="Permalink to &quot;Shoot Configuration Example&quot;">​</a></h3><p>Here&#39;s an example <code>shoot</code> manifest that defines a worker pool with AWS <code>g4dn.xlarge</code> instances running Garden Linux:</p><div class="language-yaml vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">yaml</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">apiVersion</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">core.gardener.cloud/v1beta1</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">kind</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Shoot</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">metadata</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  name</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">my-cluster</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  namespace</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">garden-my-project</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">spec</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  cloudProfileName</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">aws</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  region</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">eu-central-1</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  kubernetes</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    version</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;1.33.0&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  provider</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">aws</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    workers</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">      - </span><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">name</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">worker-gpu</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        minimum</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        maximum</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        machine</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">          type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">g4dn.xlarge</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # GPU-enabled machine type</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">          image</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">            name</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">gardenlinux</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">            version</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;1877.5.0&quot;</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # Use a supported Garden Linux version</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">          architecture</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">amd64</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        zones</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">          - </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">eu-central-1a</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  ...</span></span></code></pre></div><p>Key configuration points:</p><ul><li><strong>Machine Type</strong>: Select a GPU-enabled machine type from your cloud provider (check our <code>cloudprofiles</code> for available options)</li><li><strong>Operating System</strong>: Garden Linux is fully supported by the NVIDIA GPU Operator</li><li><strong>Zones</strong>: Ensure GPU instances are available in your selected zones</li></ul><p>After applying this manifest, Gardener will provision your cluster with GPU-enabled worker nodes.</p><h2 id="step-2-install-the-nvidia-gpu-operator" tabindex="-1">Step 2: Install the NVIDIA GPU Operator <a class="header-anchor" href="#step-2-install-the-nvidia-gpu-operator" aria-label="Permalink to &quot;Step 2: Install the NVIDIA GPU Operator&quot;">​</a></h2><p>Once your cluster is running, the GPU hardware is not yet usable by Kubernetes workloads. You must install the <strong>NVIDIA GPU Operator</strong>, which automates the management of all necessary NVIDIA software components:</p><ul><li>NVIDIA drivers</li><li>NVIDIA Container Toolkit</li><li>Kubernetes device plugin for GPUs</li><li>DCGM (Data Center GPU Manager) for monitoring</li><li>GPU Feature Discovery</li></ul><h3 id="installation-with-helm" tabindex="-1">Installation with Helm <a class="header-anchor" href="#installation-with-helm" aria-label="Permalink to &quot;Installation with Helm&quot;">​</a></h3><p>The recommended installation method uses the NVIDIA Helm chart with Garden Linux-specific configuration:</p><ol><li><p><strong>Add the NVIDIA Helm repository:</strong></p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">helm</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> repo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> add</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> nvidia</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://helm.ngc.nvidia.com/nvidia</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">helm</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> repo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> update</span></span></code></pre></div></li><li><p><strong>Install the operator with Garden Linux configuration:</strong></p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">helm</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> upgrade</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --install</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --create-namespace</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> gpu-operator</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> gpu-operator</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> nvidia/gpu-operator</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  --values</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://raw.githubusercontent.com/gardenlinux/gardenlinux-nvidia-installer/refs/heads/main/helm/gpu-operator-values.yaml</span></span></code></pre></div></li></ol><h3 id="verification" tabindex="-1">Verification <a class="header-anchor" href="#verification" aria-label="Permalink to &quot;Verification&quot;">​</a></h3><p>After installation completes, verify that GPU resources are available to Kubernetes:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Check that GPU resources are visible</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">kubectl</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> describe</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> nodes</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> |</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> grep</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;nvidia.com/gpu&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Verify GPU operator pods are running</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">kubectl</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> get</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> pods</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> gpu-operator</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Check GPU capacity on nodes</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">kubectl</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> get</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> nodes</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -o</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> jsonpath=&#39;{range .items[?(@.status.allocatable.nvidia\\.com/gpu)]}{.metadata.name}: {.status.allocatable.nvidia\\.com/gpu}{&quot;\\n&quot;}{end}&#39;</span></span></code></pre></div><p>You should see <code>nvidia.com/gpu</code> listed in the <code>Capacity</code> and <code>Allocatable</code> resources of your GPU nodes.</p><h2 id="step-3-run-a-test-workload" tabindex="-1">Step 3: Run a Test Workload <a class="header-anchor" href="#step-3-run-a-test-workload" aria-label="Permalink to &quot;Step 3: Run a Test Workload&quot;">​</a></h2><p>To confirm everything is working, deploy a simple pod that requests a GPU and runs <code>nvidia-smi</code>:</p><div class="language-yaml vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">yaml</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">apiVersion</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">v1</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">kind</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Pod</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">metadata</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  name</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">gpu-test</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">spec</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  restartPolicy</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Never</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  containers</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  - </span><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">name</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">cuda-container</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    image</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">nvcr.io/nvidia/cuda:12.1.1-base-ubuntu22.04</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    command</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;nvidia-smi&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    resources</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">      limits</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        nvidia.com/gpu</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span></span></code></pre></div><p>Apply and check the logs:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">kubectl</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> apply</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> gpu-test-pod.yaml</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">kubectl</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> logs</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> gpu-test</span></span></code></pre></div><p>You should see the <code>nvidia-smi</code> output showing your GPU details.</p><h2 id="advanced-topics" tabindex="-1">Advanced Topics <a class="header-anchor" href="#advanced-topics" aria-label="Permalink to &quot;Advanced Topics&quot;">​</a></h2><h3 id="secure-accelerator-access" tabindex="-1">Secure Accelerator Access <a class="header-anchor" href="#secure-accelerator-access" aria-label="Permalink to &quot;Secure Accelerator Access&quot;">​</a></h3><p>Gardener clusters ensure that GPU access is properly isolated. This provides critical security guarantees:</p><ul><li><strong>No Unauthorized Access</strong>: Pods that do not explicitly request <code>nvidia.com/gpu</code> resources cannot see or access any GPU devices, even if scheduled on a GPU node</li><li><strong>Workload Isolation</strong>: A container that requests GPUs will only see and use the specific devices allocated to it, preventing interference with other workloads</li></ul><p>This isolation is enforced by the NVIDIA device plugin and container runtime, ensuring secure multi-tenant GPU usage.</p><p>For detailed validation of these security properties, see the <a href="https://github.com/gardener/gardener-ai-conformance/tree/main/v1.33/secure_accelerator_access" target="_blank" rel="noreferrer">Secure Accelerator Access example</a> in our AI Conformance repository.</p><h3 id="exposing-gpu-metrics" tabindex="-1">Exposing GPU Metrics <a class="header-anchor" href="#exposing-gpu-metrics" aria-label="Permalink to &quot;Exposing GPU Metrics&quot;">​</a></h3><p>The NVIDIA GPU Operator automatically deploys the <strong>DCGM Exporter</strong>, which exposes comprehensive GPU metrics in Prometheus format, including:</p><ul><li>GPU utilization</li><li>Memory usage (used/free)</li><li>Temperature</li><li>Power consumption</li><li>Clock speeds</li></ul><h4 id="integrating-with-your-monitoring-stack" tabindex="-1">Integrating with Your Monitoring Stack <a class="header-anchor" href="#integrating-with-your-monitoring-stack" aria-label="Permalink to &quot;Integrating with Your Monitoring Stack&quot;">​</a></h4><p>The DCGM Exporter service is available at <code>nvidia-dcgm-exporter.gpu-operator.svc</code> on port <code>9400</code> and can be scraped by any Prometheus-compatible monitoring solution.</p><p>If you&#39;re using <strong>Prometheus Operator</strong>, you can create a <code>ServiceMonitor</code> to automatically configure scraping:</p><div class="language-yaml vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">yaml</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">apiVersion</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">monitoring.coreos.com/v1</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">kind</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">ServiceMonitor</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">metadata</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  name</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">nvidia-dcgm-exporter</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  namespace</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">gpu-operator</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">spec</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  selector</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    matchLabels</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">      app</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">nvidia-dcgm-exporter</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  endpoints</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  - </span><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">port</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">metrics</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    interval</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">15s</span></span></code></pre></div><p>For other monitoring solutions, configure them to scrape the endpoint directly:</p><ul><li><strong>Service</strong>: <code>nvidia-dcgm-exporter.gpu-operator.svc:9400</code></li><li><strong>Metrics path</strong>: <code>/metrics</code></li><li><strong>Format</strong>: Prometheus exposition format</li></ul><p>For a complete example, see the <a href="https://github.com/gardener/gardener-ai-conformance/tree/main/v1.33/accelerator_metrics" target="_blank" rel="noreferrer">Accelerator Metrics guide</a>.</p><h3 id="autoscaling-gpu-workloads-with-hpa" tabindex="-1">Autoscaling GPU Workloads with HPA <a class="header-anchor" href="#autoscaling-gpu-workloads-with-hpa" aria-label="Permalink to &quot;Autoscaling GPU Workloads with HPA&quot;">​</a></h3><p>You can use GPU metrics to autoscale your AI/ML workloads with a HorizontalPodAutoscaler (HPA), allowing you to scale applications based on actual GPU utilization rather than just CPU or memory.</p><p>HPA autoscaling with custom GPU metrics requires a complete metrics pipeline. Understanding this architecture is crucial:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>DCGM Exporter → Prometheus → prometheus-adapter → custom.metrics.k8s.io API → HPA</span></span></code></pre></div><p>The HPA controller (part of kube-controller-manager) can only query the Kubernetes API Server for metrics. The API Server itself doesn&#39;t collect or store metrics - it delegates to registered metrics APIs:</p><ul><li><strong>metrics.k8s.io</strong> (resource metrics like CPU/memory, provided by metrics-server)</li><li><strong>custom.metrics.k8s.io</strong> (custom metrics, requires an adapter)</li><li><strong>external.metrics.k8s.io</strong> (external metrics, requires an adapter)</li></ul><p>For GPU metrics to be available to HPA, you <strong>must</strong> deploy a solution that implements the <code>custom.metrics.k8s.io</code> API - it&#39;s a fundamental part of Kubernetes&#39; metrics architecture.</p><p><strong>Common alternatives to prometheus-adapter:</strong></p><ul><li><a href="https://keda.sh/" target="_blank" rel="noreferrer">KEDA</a> - Event-driven autoscaling with support for many metric sources</li><li>Custom metrics adapter - Build your own adapter that reads directly from DCGM</li><li>Cloud provider-specific adapters (AWS CloudWatch, Azure Monitor, GCP Monitoring)</li></ul><p>However, <strong>prometheus-adapter</strong> is the most common and well-supported solution for custom metrics in Kubernetes.</p><p>The complete setup involves four main components:</p><ol><li><p><strong>ServiceMonitor</strong>: Configure Prometheus to scrape DCGM Exporter metrics (see <a href="/docs/guides/administer-shoots/gpu/#integrating-with-your-monitoring-stack">&quot;Integrating with Your Monitoring Stack&quot;</a> above)</p></li><li><p><strong>PrometheusRule</strong>: Create recording rules to aggregate raw DCGM metrics into stable, pod-level custom metrics:</p><div class="language-yaml vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">yaml</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">apiVersion</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">monitoring.coreos.com/v1</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">kind</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">PrometheusRule</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">metadata</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  name</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">gpu-custom-metrics</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  namespace</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">monitoring</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">spec</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  groups</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  - </span><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">name</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">gpu-metrics</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    rules</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    - </span><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">record</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">pod_gpu_utilization</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">      expr</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">label_replace(label_replace(DCGM_FI_DEV_GPU_UTIL, &quot;pod&quot;, &quot;$1&quot;, &quot;exported_pod&quot;, &quot;(.*)&quot;), &quot;namespace&quot;, &quot;$1&quot;, &quot;exported_namespace&quot;, &quot;(.*)&quot;)</span></span></code></pre></div></li><li><p><strong>Prometheus Adapter</strong>: Deploy prometheus-adapter to expose Prometheus metrics via the <code>custom.metrics.k8s.io</code> API:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">helm</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> prometheus-adapter</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> prometheus-community/prometheus-adapter</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  --namespace</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> monitoring</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  --set</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> prometheus.url=http://prometheus-server.monitoring.svc</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  --set</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> rules.custom[0].seriesQuery=&#39;pod_gpu_utilization{pod!=&quot;&quot;}&#39;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  --set</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> rules.custom[0].name.as=&#39;pod_gpu_utilization&#39;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  --set</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> rules.custom[0].metricsQuery=&#39;avg(&lt;&lt;.Series&gt;&gt;{&lt;&lt;.LabelMatchers&gt;&gt;}) by (&lt;&lt;.GroupBy&gt;&gt;)&#39;</span></span></code></pre></div></li><li><p><strong>HPA</strong>: Configure an HPA to use the custom GPU metric for scaling decisions:</p><div class="language-yaml vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">yaml</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">apiVersion</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">autoscaling/v2</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">kind</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">HorizontalPodAutoscaler</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">metadata</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  name</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">gpu-workload-hpa</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  namespace</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">your-namespace</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">spec</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  scaleTargetRef</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    apiVersion</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">apps/v1</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    kind</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Deployment</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    name</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">gpu-workload</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  minReplicas</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  maxReplicas</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  metrics</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  - </span><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Pods</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    pods</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">      metric</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        name</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">pod_gpu_utilization</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">      target</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">AverageValue</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        averageValue</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;10&quot;</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # Target 10% GPU utilization</span></span></code></pre></div></li></ol><p>For a complete, step-by-step guide with all necessary manifests, see the <a href="https://github.com/gardener/gardener-ai-conformance/tree/main/v1.33/pod_autoscaling" target="_blank" rel="noreferrer">Pod Autoscaling example</a>.</p><h3 id="dynamic-resource-allocation-dra" tabindex="-1">Dynamic Resource Allocation (DRA) <a class="header-anchor" href="#dynamic-resource-allocation-dra" aria-label="Permalink to &quot;Dynamic Resource Allocation (DRA)&quot;">​</a></h3><p>For more flexible and fine-grained GPU management beyond simple device counts, you can enable <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/" target="_blank" rel="noreferrer"><strong>Dynamic Resource Allocation (DRA)</strong></a>. DRA enables advanced resource claims, flexible device filtering using common expression language (CEL), or sharing GPUs across workloads.</p><p>To enable DRA, activate the corresponding feature gates in your <code>shoot</code> manifest:</p><div class="language-yaml vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">yaml</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">apiVersion</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">core.gardener.cloud/v1beta1</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">kind</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Shoot</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">spec</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  kubernetes</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    version</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;1.33.0&quot;</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    kubeAPIServer</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">      featureGates</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        DynamicResourceAllocation</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">true</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">      runtimeConfig</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        resource.k8s.io/v1beta1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">true</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    kubeControllerManager</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">      featureGates</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        DynamicResourceAllocation</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">true</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    kubeScheduler</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">      featureGates</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        DynamicResourceAllocation</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">true</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    kubelet</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">      featureGates</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        DynamicResourceAllocation</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">true</span></span></code></pre></div><div class="note custom-block github-alert"><p class="custom-block-title">NOTE</p><p>In Kubernetes v1.33, DRA is available via the <code>v1beta1</code> API. The <code>v1</code> API will be available in Kubernetes v1.34 and later.</p></div><p>For more details, see the <a href="https://github.com/gardener/gardener-ai-conformance/tree/main/v1.33/dra_support" target="_blank" rel="noreferrer">DRA Support example</a>.</p><h3 id="ai-ml-workload-controllers" tabindex="-1">AI/ML Workload Controllers <a class="header-anchor" href="#ai-ml-workload-controllers" aria-label="Permalink to &quot;AI/ML Workload Controllers&quot;">​</a></h3><p>For running distributed AI/ML applications, using a specialized workload controller is highly recommended. <strong>KubeRay</strong>, for example, simplifies the deployment and management of your AI/ML workload on Kubernetes, enabling distributed training and inference workloads.</p><p>Gardener provides a conformant environment to run such operators. The operator handles:</p><ul><li>Cluster lifecycle management</li><li>Autoscaling of worker nodes</li><li>Job scheduling and execution</li><li>Service exposure</li></ul><p>For a detailed example of installing and using KubeRay on a Gardener cluster, see the <a href="https://github.com/gardener/gardener-ai-conformance/tree/main/v1.33/robust_controller" target="_blank" rel="noreferrer">GPU Workload Controller example</a>.</p><h3 id="gang-scheduling" tabindex="-1">Gang Scheduling <a class="header-anchor" href="#gang-scheduling" aria-label="Permalink to &quot;Gang Scheduling&quot;">​</a></h3><p>Distributed training jobs often require all of their pods (the &quot;gang&quot;) to be scheduled simultaneously. This &quot;all-or-nothing&quot; behavior prevents deadlocks and wasted resources when some pods are scheduled but others cannot be due to resource constraints.</p><p>Kubernetes does not provide gang scheduling out-of-the-box, but it can be added with schedulers like <a href="https://kueue.sigs.k8s.io/" target="_blank" rel="noreferrer"><strong>Kueue</strong></a>, <a href="https://github.com/ai-dynamo/grove" target="_blank" rel="noreferrer"><strong>Grove</strong></a>, or similar solutions.</p><p>Gang scheduling is critical for:</p><ul><li>Multi-GPU training frameworks (PyTorch DDP, Horovod, DeepSpeed)</li><li>Distributed training systems (TensorFlow, JAX, MPI)</li><li>Efficient GPU utilization in multi-tenant environments</li></ul><p>For a complete example of setting up Kueue for gang scheduling on a Gardener cluster, refer to the <a href="https://github.com/gardener/gardener-ai-conformance/tree/main/v1.33/gang_scheduling" target="_blank" rel="noreferrer">Gang Scheduling example</a>.</p><h3 id="advanced-inference-with-gateway-api" tabindex="-1">Advanced Inference with Gateway API <a class="header-anchor" href="#advanced-inference-with-gateway-api" aria-label="Permalink to &quot;Advanced Inference with Gateway API&quot;">​</a></h3><p>For production AI inference services, the Kubernetes <strong>Gateway API</strong> provides advanced traffic management capabilities beyond standard Ingress resources:</p><ul><li><strong>Weighted traffic splitting</strong> for A/B testing and canary deployments</li><li><strong>Header-based routing</strong> (e.g., <code>X-Model-Version</code> for model version selection)</li><li><strong>Gradual rollouts</strong> between model versions</li><li><strong>Cross-namespace routing</strong> with ReferenceGrant for secure multi-tenant deployments</li></ul><p>You need to install a Gateway controller (e.g., Traefik, Envoy Gateway, Istio) separately.</p><p>For a complete example, see the <a href="https://github.com/gardener/gardener-ai-conformance/tree/main/v1.33/ai_inference" target="_blank" rel="noreferrer">AI Inference example</a>.</p><h2 id="troubleshooting" tabindex="-1">Troubleshooting <a class="header-anchor" href="#troubleshooting" aria-label="Permalink to &quot;Troubleshooting&quot;">​</a></h2><h3 id="gpu-operator-installation-issues" tabindex="-1">GPU Operator Installation Issues <a class="header-anchor" href="#gpu-operator-installation-issues" aria-label="Permalink to &quot;GPU Operator Installation Issues&quot;">​</a></h3><p><strong>Driver pod fails to start:</strong></p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Check driver pod logs</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">kubectl</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> logs</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> gpu-operator</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -l</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> app=nvidia-driver-daemonset</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Verify Garden Linux values file was used</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">helm</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> get</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> values</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> gpu-operator</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> gpu-operator</span></span></code></pre></div><p><strong>GPU resources not visible:</strong></p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Check device plugin status</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">kubectl</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> get</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> pods</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> gpu-operator</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -l</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> app=nvidia-device-plugin-daemonset</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">kubectl</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> logs</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> gpu-operator</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -l</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> app=nvidia-device-plugin-daemonset</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Verify driver completed successfully first</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">kubectl</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> get</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> pods</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> gpu-operator</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> |</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> grep</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> nvidia-driver</span></span></code></pre></div><p><strong>Components stuck in Init state:</strong></p><p>This is normal behavior during driver installation. Components wait for the driver to be ready before starting. Driver installation can take 5-15 minutes depending on network speed and node resources.</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Monitor driver installation progress</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">kubectl</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> logs</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> gpu-operator</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -l</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> app=nvidia-driver-daemonset</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -f</span></span></code></pre></div><h3 id="workload-issues" tabindex="-1">Workload Issues <a class="header-anchor" href="#workload-issues" aria-label="Permalink to &quot;Workload Issues&quot;">​</a></h3><p><strong>Pod stuck in Pending state:</strong></p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Check if GPU resources are available</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">kubectl</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> describe</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> node</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> &lt;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">gpu-node-nam</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">e</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> |</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> grep</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -A</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 5</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;Allocated resources&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Check pod events</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">kubectl</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> describe</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> pod</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> &lt;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">pod-nam</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">e</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span></span></code></pre></div><p><strong>GPU not accessible in container:</strong></p><p>Ensure your pod spec includes a GPU resource request:</p><div class="language-yaml vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">yaml</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">resources</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  limits</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    nvidia.com/gpu</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span></span></code></pre></div><h2 id="related-links" tabindex="-1">Related Links <a class="header-anchor" href="#related-links" aria-label="Permalink to &quot;Related Links&quot;">​</a></h2><h3 id="gardener-ai-conformance" tabindex="-1">Gardener AI Conformance <a class="header-anchor" href="#gardener-ai-conformance" aria-label="Permalink to &quot;Gardener AI Conformance&quot;">​</a></h3><ul><li><a href="https://github.com/gardener/gardener-ai-conformance" target="_blank" rel="noreferrer">Gardener AI Conformance Repository</a></li><li><a href="https://github.com/cncf/ai-conformance" target="_blank" rel="noreferrer">CNCF AI Conformance Working Group</a></li></ul><h3 id="examples-and-guides" tabindex="-1">Examples and Guides <a class="header-anchor" href="#examples-and-guides" aria-label="Permalink to &quot;Examples and Guides&quot;">​</a></h3><ul><li><a href="https://github.com/gardener/gardener-ai-conformance/tree/main/v1.33/secure_accelerator_access" target="_blank" rel="noreferrer">Secure Accelerator Access</a></li><li><a href="https://github.com/gardener/gardener-ai-conformance/tree/main/v1.33/accelerator_metrics" target="_blank" rel="noreferrer">Exposing GPU Metrics</a></li><li><a href="https://github.com/gardener/gardener-ai-conformance/tree/main/v1.33/cluster_autoscaling" target="_blank" rel="noreferrer">Cluster Autoscaling</a></li><li><a href="https://github.com/gardener/gardener-ai-conformance/tree/main/v1.33/pod_autoscaling" target="_blank" rel="noreferrer">Pod Autoscaling with GPU Metrics</a></li><li><a href="https://github.com/gardener/gardener-ai-conformance/tree/main/v1.33/dra_support" target="_blank" rel="noreferrer">Dynamic Resource Allocation (DRA)</a></li><li><a href="https://github.com/gardener/gardener-ai-conformance/tree/main/v1.33/robust_controller" target="_blank" rel="noreferrer">GPU Workload Controller (KubeRay)</a></li><li><a href="https://github.com/gardener/gardener-ai-conformance/tree/main/v1.33/gang_scheduling" target="_blank" rel="noreferrer">Gang Scheduling (Kueue)</a></li><li><a href="https://github.com/gardener/gardener-ai-conformance/tree/main/v1.33/ai_inference" target="_blank" rel="noreferrer">AI Inference with Gateway API</a></li></ul><h3 id="external-resources" tabindex="-1">External Resources <a class="header-anchor" href="#external-resources" aria-label="Permalink to &quot;External Resources&quot;">​</a></h3><ul><li><a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/overview.html" target="_blank" rel="noreferrer">NVIDIA GPU Operator Documentation</a></li><li><a href="https://github.com/gardenlinux/gardenlinux-nvidia-installer" target="_blank" rel="noreferrer">Garden Linux NVIDIA Installer</a></li></ul>`,106)]))}const g=i(t,[["render",l]]);export{c as __pageData,g as default};
