import{d as h,c as s,o as n,F as d,B as p,e as c,j as a,t as r,k as o,g,_ as u,a2 as k,G as m}from"./chunks/framework.Bfq10Vlj.js";const y=JSON.parse(`[{"title":"Gardener Achieves CNCF AI Conformance for Kubernetes","url":"/blog/2025/11/11-12-gardener-ai-conformance","excerpt":"","date":{"time":1762948800000,"string":"November 12, 2025"}},{"title":"PromCon EU 2025 Highlights","url":"/blog/2025/11/11-13-promcon-eu-2025","excerpt":"","date":{"time":1762948800000,"string":"November 12, 2025"}},{"title":"Unifying DNS Behavior: Custom CoreDNS Configurations Now Supported in node-local-dns","url":"/blog/2025/10/10-27-unifying-dns-behavior-custom-coredns-configurations-now-supported-in-node-local-dns","excerpt":"","date":{"time":1761566400000,"string":"October 27, 2025"}},{"title":"Unifying HTTP Proxy Infrastructure in Gardener","url":"/blog/2025/10/10-22-useunifiedhttpproxy-feature-gate","excerpt":"","date":{"time":1761134400000,"string":"October 22, 2025"}},{"title":"New Shared File Storage Options on AWS and GCP","url":"/blog/2025/10/10-08-efs-filestore-csi-drivers","excerpt":"","date":{"time":1759924800000,"string":"October 8, 2025"}},{"title":"Explicit Internal DNS Configuration for Seeds","url":"/blog/2025/09/09-10-explicit-internal-dns-configuration-for-seeds","excerpt":"<h1 id=\\"explicit-internal-dns-configuration-for-seeds\\" tabindex=\\"-1\\">Explicit Internal DNS Configuration for Seeds <a class=\\"header-anchor\\" href=\\"#explicit-internal-dns-configuration-for-seeds\\" aria-label=\\"Permalink to &quot;Explicit Internal DNS Configuration for Seeds&quot;\\">&ZeroWidthSpace;</a></h1>\\n<p>Gardener's DNS management capabilities have been enhanced to provide a more explicit, secure, and flexible method for configuring internal DNS for <code>Seed</code> clusters. This change moves away from a global, label-based secret selection to a direct configuration within the <code>Seed</code> API.</p>\\n<h3 id=\\"a-new-api-for-per-seed-configuration\\" tabindex=\\"-1\\">A New API for Per-Seed Configuration <a class=\\"header-anchor\\" href=\\"#a-new-api-for-per-seed-configuration\\" aria-label=\\"Permalink to &quot;A New API for Per-Seed Configuration&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>Previously, internal DNS settings were configured globally for an entire Gardener landscape via a single, specially labeled <code>Secret</code>. With the recent changes, the <code>Seed</code> specification has been extended with a new <code>.spec.dns.internal</code> field. This allows operators to define internal DNS settings on a per-seed basis.</p>\\n<p>The new <code>SeedDNSProviderConfig</code> object includes the following fields:</p>\\n<ul>\\n<li><code>type</code>: The DNS provider type (e.g., <code>aws-route53</code>, <code>local</code>).</li>\\n<li><code>domain</code>: The internal domain name to be used by the provider.</li>\\n<li><code>zone</code> (optional): The specific zone where DNS records are managed.</li>\\n<li><code>credentialsRef</code>: A reference to a <code>Secret</code> that holds the credentials for authenticating with the DNS provider.</li>\\n</ul>\\n<p>This new approach offers several advantages:</p>\\n<ul>\\n<li><strong>Granular Control:</strong> Operators can now configure different internal domains and credentials for each <code>Seed</code>, reducing the blast radius in case a secret is compromised. This also helps in avoiding potential rate limits from DNS providers.</li>\\n<li><strong>Explicit and Validated:</strong> By moving the configuration into an explicit API field, it becomes easier to validate and is less prone to errors than the previous implicit, label-based discovery mechanism.</li>\\n<li><strong>Future-Ready:</strong> The API is designed to support other credential types in the future, such as <code>WorkloadIdentity</code>.</li>\\n</ul>\\n<h3 id=\\"migration-path-for-operators\\" tabindex=\\"-1\\">Migration Path for Operators <a class=\\"header-anchor\\" href=\\"#migration-path-for-operators\\" aria-label=\\"Permalink to &quot;Migration Path for Operators&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>To ensure a smooth transition, Gardener provides an automatic migration path. On startup, a <code>gardenlet</code> will check if the <code>.spec.dns.internal</code> field is set for its <code>Seed</code>. If the field is empty, <code>gardenlet</code> will automatically populate it by reading the configuration from the existing globally-defined internal domain <code>Secret</code>.</p>\\n<p>However, this automatic population is a temporary measure to facilitate migration. Operators are required to adapt their <code>Seed</code> manifests and configuration templates to explicitly define the <code>.spec.dns.internal</code> block.</p>\\n<p><strong>Important:</strong> The <code>.spec.dns.internal</code> field will become a <strong>mandatory configuration</strong> after the release of Gardener <strong>v1.129.0</strong>.</p>\\n<p>This enhancement is the first step in improving DNS configuration management. A similar change is also planned for the default domain configuration in a future release.</p>\\n","date":{"time":1757505600000,"string":"September 10, 2025"}},{"title":"Modernizing Gardener's Logging Stack with OpenTelemetry","url":"/blog/2025/09/09-10-modernizing-gardeners-logging-stack-with-opentelemetry","excerpt":"","date":{"time":1757505600000,"string":"September 10, 2025"}},{"title":"A Deep Dive into Gardener's IPv6 Journey","url":"/blog/2025/09/09-05-ipv6-update","excerpt":"","date":{"time":1757073600000,"string":"September 5, 2025"}},{"title":"Enabling Node-Local DNS Without Node Rollouts","url":"/blog/2025/08/08-27-enabling-node-local-dns-without-node-rollouts","excerpt":"","date":{"time":1756296000000,"string":"August 27, 2025"}},{"title":"New Emergency Brake for Gardener Shoot Reconciliations","url":"/blog/2025/08/08-27-new-emergency-brake-for-gardener-shoot-reconciliations","excerpt":"","date":{"time":1756296000000,"string":"August 27, 2025"}},{"title":"Keeping Track of Your Cloud Resources with Gardener Inventory","url":"/blog/2025/08/08-13-keeping-track-of-your-resources-with-inventory","excerpt":"<h1 id=\\"keeping-track-of-your-cloud-resources-with-gardener-inventory\\" tabindex=\\"-1\\">Keeping Track of Your Cloud Resources with Gardener Inventory <a class=\\"header-anchor\\" href=\\"#keeping-track-of-your-cloud-resources-with-gardener-inventory\\" aria-label=\\"Permalink to &quot;Keeping Track of Your Cloud Resources with Gardener Inventory&quot;\\">&ZeroWidthSpace;</a></h1>\\n<p>Running Kubernetes clusters at scale comes with its own challenges.</p>\\n<p>How do you keep track of the resources under your control? How can you tell if a\\nresource has become orphaned and has started adding up to your monthly cloud cost?</p>\\n<p>This post introduces <a href=\\"https://github.com/gardener/inventory\\" target=\\"_blank\\" rel=\\"noreferrer\\">gardener/inventory</a>\\nand shows how the Gardener project keeps track of the cloud resources it manages\\nin the various cloud providers such as GCP, AWS, Azure, and OpenStack.</p>\\n<p>It will also show how we can identify orphaned resources through a simple, yet\\neffective mechanism, by collecting and reporting from the respective cloud\\nproviders.</p>\\n<h2 id=\\"what-is-gardener-inventory\\" tabindex=\\"-1\\">What is Gardener Inventory? <a class=\\"header-anchor\\" href=\\"#what-is-gardener-inventory\\" aria-label=\\"Permalink to &quot;What is Gardener Inventory?&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>The Gardener Inventory is a system that collects resources from various data\\nsources and persists them in a database, establishing relationships between\\nthe collected resources.</p>\\n<p>The collected data can be later analyzed to show the relationship and\\ndependencies between the various resources.</p>\\n<p>The diagram below provides a high-level overview of the data model for the\\nInventory system.</p>\\n<p><img src=\\"/blog/2025/08/images/inventory/data-model.svg\\" alt=\\"Inventory Data Model High Level Overview\\"></p>\\n<p>Each Gardener resource tracked by Inventory is mapped to its corresponding\\nhyperscaler resource, e.g., a <code>Machine</code> resource on the Gardener side maps to a\\nvirtual machine instance in GCP, AWS, Azure or OpenStack.</p>\\n<p>This allows us to ask different questions to the system:</p>\\n<ul>\\n<li>Do all my hyperscaler virtual machines map to a Gardener Machine?</li>\\n<li>Do I have any unknown hyperscaler virtual machines?</li>\\n<li>etc.</li>\\n</ul>\\n<p>For more details about the design, please refer to the <a href=\\"https://github.com/gardener/inventory/blob/main/docs/design.md\\" target=\\"_blank\\" rel=\\"noreferrer\\">Design\\nGoals</a> document.</p>\\n<h2 id=\\"how-does-resource-collection-work\\" tabindex=\\"-1\\">How does resource collection work? <a class=\\"header-anchor\\" href=\\"#how-does-resource-collection-work\\" aria-label=\\"Permalink to &quot;How does resource collection work?&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>This diagram shows a high-level overview of the collection mechanism implemented\\nby Inventory.</p>\\n<p><img src=\\"/blog/2025/08/images/inventory/collection-flow.svg\\" alt=\\"Inventory Collection Flow\\"></p>\\n<p>The Inventory <code>scheduler</code> is responsible for enqueuing tasks on regular basis to\\na message queue. These tasks are then consumed and processed by the Inventory\\n<code>workers</code>. Each task is self-contained and independent from the rest of the\\ntasks in the system.</p>\\n<p>With this approach, that means that a single task failure does not affect any of\\nthe other tasks in the system. That also means that we can run collection of\\ndifferent resources at the same time (e.g., EC2 instances, VPCs, buckets, etc.)\\nand let the system take care of the automatic retry and recovery for us.</p>\\n<p>This is a typical <a href=\\"https://en.wikipedia.org/wiki/Event-driven_architecture\\" target=\\"_blank\\" rel=\\"noreferrer\\">Event-driven architecture</a>.</p>\\n<p>Another benefit of this architecture is that we can assign priorities for our\\ntasks, e.g., we may want to prioritize collection of virtual machines over\\nanything else.</p>\\n<h2 id=\\"requirements\\" tabindex=\\"-1\\">Requirements <a class=\\"header-anchor\\" href=\\"#requirements\\" aria-label=\\"Permalink to &quot;Requirements&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>In order to build and start using Inventory locally, you would need:</p>\\n<ul>\\n<li>Go 1.24.x</li>\\n<li>Docker</li>\\n</ul>\\n<p>When running Inventory locally, it is recommended that you use the provided\\n<a href=\\"https://github.com/gardener/inventory/blob/main/docker-compose.yaml\\" target=\\"_blank\\" rel=\\"noreferrer\\">docker-compose.yaml</a>\\nmanifest, which will start up all services for you. In order to start the\\nservices locally, first clone the <code>gardener/inventory</code> repo and then run the\\nfollowing command:</p>\\n<div class=\\"language-shell vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">shell</span><pre class=\\"shiki shiki-themes github-light github-dark vp-code\\" tabindex=\\"0\\" v-pre=\\"\\"><code><span class=\\"line\\"><span style=\\"--shiki-light:#6F42C1;--shiki-dark:#B392F0\\">make</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> docker-compose-up</span></span></code></pre>\\n</div><p>Alternatively, you can build the <code>inventory</code> CLI locally by using the <code>build</code>\\nmakefile target:</p>\\n<div class=\\"language-yaml vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">yaml</span><pre class=\\"shiki shiki-themes github-light github-dark vp-code\\" tabindex=\\"0\\" v-pre=\\"\\"><code><span class=\\"line\\"><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">make build</span></span></code></pre>\\n</div><p>For running Inventory in a Kubernetes cluster, please refer to the provided\\n<a href=\\"https://github.com/gardener/inventory/tree/main/deployment/kustomize\\" target=\\"_blank\\" rel=\\"noreferrer\\">kustomize manifests</a>.</p>\\n<p>When using the kustomize manifests, make sure that you also create a separate\\n<code>config</code> overlay, which will provide your specific configuration for\\n<code>scheduler</code>, <code>worker</code>, <code>database</code>, etc. For more details, please check the\\n<a href=\\"https://github.com/gardener/inventory/tree/main/deployment/kustomize/config\\" target=\\"_blank\\" rel=\\"noreferrer\\">deployment/kustomize/config</a>\\noverlay, which provides an example config file that you can customize.</p>\\n<p>When running Inventory in production environments, it is recommended that you\\ndeploy Inventory in a Kubernetes cluster and configure <a href=\\"https://en.wikipedia.org/wiki/OpenID\\" target=\\"_blank\\" rel=\\"noreferrer\\">OIDC\\ntrust</a> between your Kubernetes workload\\nand the respective hyperscaler. With OIDC federation configured, the Inventory\\nworkers will use short-lived credentials, so you don't need to worry about\\nmaintaining any static long-lived credentials.</p>\\n<p>For more details on how to setup OIDC trust between your Inventory cluster and\\nthe respective hyperscalers, please refer to the following documents:</p>\\n<ul>\\n<li><a href=\\"https://github.com/gardener/inventory/blob/main/docs/oidc-aws.md\\" target=\\"_blank\\" rel=\\"noreferrer\\">OIDC trust between Inventory and AWS</a></li>\\n<li><a href=\\"https://github.com/gardener/inventory/blob/main/docs/oidc-gcp.md\\" target=\\"_blank\\" rel=\\"noreferrer\\">OIDC trust between Inventory and GCP</a></li>\\n<li><a href=\\"https://github.com/gardener/inventory/blob/main/docs/oidc-azure.md\\" target=\\"_blank\\" rel=\\"noreferrer\\">OIDC trust between Inventory and Azure</a></li>\\n</ul>\\n<p>Please make sure that you also check the following documents, which provide\\nadditional details on how to get started with the developing Inventory and\\noperating it:</p>\\n<ul>\\n<li><a href=\\"https://github.com/gardener/inventory/blob/main/docs/development.md\\" target=\\"_blank\\" rel=\\"noreferrer\\">Inventory Development Guide</a></li>\\n<li><a href=\\"https://github.com/gardener/inventory/blob/main/docs/ops-guide.md\\" target=\\"_blank\\" rel=\\"noreferrer\\">Inventory Operations Guide</a></li>\\n</ul>\\n<h2 id=\\"configuration\\" tabindex=\\"-1\\">Configuration <a class=\\"header-anchor\\" href=\\"#configuration\\" aria-label=\\"Permalink to &quot;Configuration&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>Inventory is flexible enough and you can configure it to collect from specific\\ndatasources only. For example, if you are running your workloads in AWS only,\\nthen you would configure Inventory to collect from AWS, and leave the rest of\\nthe datasources deactivated.</p>\\n<p>Before we start collecting and mapping resources with Inventory, we would need a\\nGardener Project and a hyperscaler account from which resources will be\\ncollected.</p>\\n<p>While Gardener resources collection can be deactivated as well, it is not\\nrecommended to do so, since we want to map hyperscaler resources with Gardener\\nresources. This way, Inventory can map resources together and detect when orphan\\nresources (e.g., virtual machines, disks, etc.) exist in the hyperscaler.</p>\\n<p>If your source-of-truth system is different from Gardener, then Inventory can be\\nextended to support other data sources, which provide additional resources.</p>\\n<p>An example of such an extension is the\\n<a href=\\"https://github.com/gardener/inventory-extension-odg\\" target=\\"_blank\\" rel=\\"noreferrer\\">gardener/inventory-extension-odg</a>,\\nwhich plugs into an existing Inventory deployment in order to integrate with\\n<a href=\\"https://github.com/open-component-model\\" target=\\"_blank\\" rel=\\"noreferrer\\">Open Component Model</a> for reporting\\ncompliance issues caused by leaked resources.</p>\\n<p>It is also recommended that you check the <a href=\\"https://github.com/gardener/inventory/blob/main/examples/config.yaml\\" target=\\"_blank\\" rel=\\"noreferrer\\">example config\\nfile</a>,\\nwhich is well documented and provides details about each config section. In this\\npost we will focus on specific config sections only, but please make sure to\\nrefer to the sample config for more details on each setting.</p>\\n<p>In order to keep things simple and easier to follow, we will be configuring\\nInventory to collect from Gardener and AWS only. For configuring Inventory with\\nother datasources (e.g., GCP, Azure, OpenStack, etc.), please refer to the\\ndocumentation provided in the\\n<a href=\\"https://github.com/gardener/inventory\\" target=\\"_blank\\" rel=\\"noreferrer\\">gardener/inventory</a> repository.</p>\\n<div class=\\"note custom-block github-alert\\"><p class=\\"custom-block-title\\">NOTE</p>\\n<p>When configuring Inventory for collection against remote systems, bear in mind\\nthat Inventory itself requires read-only permissions. Keep that in mind when\\nconfiguring your RBAC roles for Inventory in the respective system and grant\\nit read-only access.</p>\\n</div>\\n<p>First, we will configure Inventory and grant it access to our Gardener\\nProject. In order to do that, navigate to the Gardener Dashboard, select your\\nproject and go to the <code>Members</code> section. Then add a new service account.</p>\\n<p><img src=\\"/blog/2025/08/images/inventory/inventory-sa.png\\" alt=\\"Inventory Service Account\\"></p>\\n<p>After the service account has been created, you should download its\\n<code>kubeconfig</code>, which we will configure in the next step. This snippet represents\\nthe <code>gardener</code> config section of our configuration file.</p>\\n<div class=\\"language-yaml vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">yaml</span><pre class=\\"shiki shiki-themes github-light github-dark vp-code\\" tabindex=\\"0\\" v-pre=\\"\\"><code><span class=\\"line\\"><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\"># Gardener specific configuration</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">gardener</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">:</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">  is_enabled</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\\">true</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\">  # Specifies the endpoint of the Gardener APIs.</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">  endpoint</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">https://api.dev.gardener.cloud.sap</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\">  # User-Agent to set for the API clients</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">  user_agent</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">gardener-inventory/0.1.0</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\">  # Authentication mechanism to use when communicating with the Gardener APIs.</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\">  # Supported mechanisms are \`in_cluster', \`token_path' and \`kubeconfig'.</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">  authentication</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">kubeconfig</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\">  # Path to kubeconfig file to use. Should be used with \`kubeconfig'</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\">  # authentication only.</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">  kubeconfig</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">/path/to/inventory-sa-kubeconfig.yaml</span></span></code></pre>\\n</div><div class=\\"note custom-block github-alert\\"><p class=\\"custom-block-title\\">NOTE</p>\\n<p>Other authentication mechanisms exist as well. For example, instead of using a\\nstatic kubeconfig when accessing your Gardener Project, you could configure\\nInventory to leverage OIDC trust instead and use a token signed by a trusted\\nIdP. For more details and examples, please refer to the examples/config.yaml\\nfile.</p>\\n</div>\\n<p>Next, we will configure Inventory for collecting resources from AWS. Here is a\\nsample configuration file for AWS, which configures the <code>worker</code> to use the\\nshared AWS credentials file (<code>~/.aws/credentials</code>) when authenticating against\\nthe AWS APIs.</p>\\n<div class=\\"language-yaml vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">yaml</span><pre class=\\"shiki shiki-themes github-light github-dark vp-code\\" tabindex=\\"0\\" v-pre=\\"\\"><code><span class=\\"line\\"><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\"># AWS specific configuration</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">aws</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">:</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\">  # Setting \`is_enabled' to false would not create API clients for AWS, and as a</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\">  # result Inventory will not process any of the AWS collection tasks.</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">  is_enabled</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\\">true</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">  region</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">eu-central-1</span><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\">  # Frankfurt</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">  default_region</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">eu-central-1</span><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\">  # Frankfurt</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">  app_id</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">gardener-inventory</span><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\">  # Optional application specific identifier</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\">  # This section provides configuration specific to each AWS service and which</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\">  # named credentials are used for each service. This allows Inventory to</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\">  # connect to multiple AWS accounts based on the named credentials which are</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\">  # used. Inventory will connect to all configured named credentials (accounts)</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\">  # during collection from the respective AWS service.</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">  services</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">:</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">    ec2</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">:</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">      use_credentials</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">:</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">        - </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">my-aws-account</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">    elb</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">:</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">      use_credentials</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">:</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">        - </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">my-aws-account</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">    elbv2</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">:</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">      use_credentials</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">:</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">        - </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">my-aws-account</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">    s3</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">:</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">      use_credentials</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">:</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">        - </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">my-aws-account</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\">  # The \`credentials' section provides named credentials, which are used by the</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\">  # various AWS services. The currently supported token retrievers are \`none',</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\">  # \`kube_sa_token', and \`token_file'. See docs/oidc-aws.md for more details.</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">  credentials</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">:</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">    my-aws-account</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">:</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\">      # When using \`none' as the token retriever, only the shared AWS</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\">      # credentials file (~/.aws/credentials) is used.</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">      token_retriever</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">none</span></span></code></pre>\\n</div><div class=\\"note custom-block github-alert\\"><p class=\\"custom-block-title\\">NOTE</p>\\n<p>The example AWS config file above uses a named credential, which authenticates\\nagainst AWS using the shared credentials file ~/.aws/credentials. This\\napproach is good enough for local testing during development, but when running\\nin production environments it is recommended that you leverage OIDC trust and\\nuse short-lived access tokens instead.  Make sure that you check the\\nexamples/config.yaml file for additional examples on how to configure OIDC\\ntrust between an Inventory cluster and AWS.</p>\\n</div>\\n<p>Collection may be configured for multiple AWS accounts as well. In case you need\\nto collect from multiple AWS accounts, make sure to configure them in the\\n<code>aws.credentials</code> section, and enable these accounts in the respective\\n<code>aws.services</code> section.</p>\\n<p>If you are using the provided\\n<a href=\\"https://github.com/gardener/inventory/blob/main/docker-compose.yaml\\" target=\\"_blank\\" rel=\\"noreferrer\\">docker-compose.yaml</a>\\nfile for local development, then all required services will be started up for\\nyou. This includes the PostgreSQL database, Valkey, <code>scheduler</code>, <code>worker</code>,\\netc. In case you need to connect Inventory to an external PostgreSQL database or\\na Redis/Valkey instance, then make sure to configure them in their respective\\nconfiguration sections. This would be the <code>database</code> and <code>redis</code> config\\nsections.</p>\\n<p>Also, make sure that you check the <a href=\\"https://github.com/gardener/inventory/blob/main/docs/ops-guide.md\\" target=\\"_blank\\" rel=\\"noreferrer\\">Inventory Operations\\nGuide</a>, which\\nprovides additional details on how to configure Inventory to use multiple config\\nfiles, use environment variables, and how to interface with the <code>inventory</code> CLI.</p>\\n<h2 id=\\"getting-started\\" tabindex=\\"-1\\">Getting Started <a class=\\"header-anchor\\" href=\\"#getting-started\\" aria-label=\\"Permalink to &quot;Getting Started&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>Before we start collecting data, we need to initialize the database. The commands\\nbelow assume that you already have everything configured and your config file is\\nexposed via the <code>INVENTORY_CONFIG</code> environment variable.</p>\\n<div class=\\"language-shell vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">shell</span><pre class=\\"shiki shiki-themes github-light github-dark vp-code\\" tabindex=\\"0\\" v-pre=\\"\\"><code><span class=\\"line\\"><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">export</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> INVENTORY_CONFIG</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">/path/to/inventory/config.yaml</span></span></code></pre>\\n</div><p>Initialize and migrate the database.</p>\\n<div class=\\"note custom-block github-alert\\"><p class=\\"custom-block-title\\">NOTE</p>\\n<p>Database initialization and migration is handled automatically by the provided\\nDocker and Kubernetes manifests. You need to run these commands only if you\\nare running a local environment of Inventory.</p>\\n</div>\\n<div class=\\"language-shell vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">shell</span><pre class=\\"shiki shiki-themes github-light github-dark vp-code\\" tabindex=\\"0\\" v-pre=\\"\\"><code><span class=\\"line\\"><span style=\\"--shiki-light:#6F42C1;--shiki-dark:#B392F0\\">$</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> inventory</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> db</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> init</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6F42C1;--shiki-dark:#B392F0\\">$</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> inventory</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> db</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> migrate</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6F42C1;--shiki-dark:#B392F0\\">database</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> migrated</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> to</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> group</span><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\"> #1 (200 migrations (20240522121536 ... 20250715110847))</span></span></code></pre>\\n</div><p>Check the status of the database.</p>\\n<div class=\\"language-shell vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">shell</span><pre class=\\"shiki shiki-themes github-light github-dark vp-code\\" tabindex=\\"0\\" v-pre=\\"\\"><code><span class=\\"line\\"><span style=\\"--shiki-light:#6F42C1;--shiki-dark:#B392F0\\">$</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> inventory</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> db</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> status</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6F42C1;--shiki-dark:#B392F0\\">pending</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> migration</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">(</span><span style=\\"--shiki-light:#6F42C1;--shiki-dark:#B392F0\\">s</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">)</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">:</span><span style=\\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\\"> 0</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6F42C1;--shiki-dark:#B392F0\\">database</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> version:</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> group</span><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\"> #1 (200 migrations (20240522121536 ... 20250715110847))</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6F42C1;--shiki-dark:#B392F0\\">database</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> is</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> up-to-date</span></span></code></pre>\\n</div><p>We can now start up the <code>worker</code>. In order to start the <code>worker</code>, run the\\nfollowing command.</p>\\n<div class=\\"language-shell vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">shell</span><pre class=\\"shiki shiki-themes github-light github-dark vp-code\\" tabindex=\\"0\\" v-pre=\\"\\"><code><span class=\\"line\\"><span style=\\"--shiki-light:#6F42C1;--shiki-dark:#B392F0\\">$</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> inventory</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> worker</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> start</span></span></code></pre>\\n</div><p>You should see a similar output showing the worker being initialized and ready\\nto process tasks.</p>\\n<div class=\\"language-shell vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">shell</span><pre class=\\"shiki shiki-themes github-light github-dark vp-code\\" tabindex=\\"0\\" v-pre=\\"\\"><code><span class=\\"line\\"><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">time</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">2025-08-08T13:20:50.675+03:00</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> level</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">INFO</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> msg</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">\\"configuring Gardener API client\\"</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> authentication</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">kubeconfig</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> kubeconfig</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">/path/to/inventory-sa-kubeconfig.yaml</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">time</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">2025-08-08T13:20:50.675+03:00</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> level</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">INFO</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> msg</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">\\"configured Gardener API client\\"</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> host</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">https://api.dev.gardener.cloud.sap</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">time</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">2025-08-08T13:20:50.675+03:00</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> level</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">INFO</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> msg</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">\\"configuring db client\\"</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">time</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">2025-08-08T13:20:50.675+03:00</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> level</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">INFO</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> msg</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">\\"configuring asynq client\\"</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">time</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">2025-08-08T13:20:50.675+03:00</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> level</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">INFO</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> msg</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">\\"configuring asynq inspector\\"</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">time</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">2025-08-08T13:20:50.675+03:00</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> level</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">WARN</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> msg</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">\\"Vault is not enabled, will not create API clients\\"</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">time</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">2025-08-08T13:20:50.675+03:00</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> level</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">WARN</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> msg</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">\\"GCP is not enabled, will not create API clients\\"</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">time</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">2025-08-08T13:20:50.675+03:00</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> level</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">WARN</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> msg</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">\\"Azure is not enabled, will not create API clients\\"</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">time</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">2025-08-08T13:20:50.675+03:00</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> level</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">WARN</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> msg</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">\\"OpenStack is not enabled, will not create API clients\\"</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">time</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">2025-08-08T13:20:50.675+03:00</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> level</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">INFO</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> msg</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">\\"registered task\\"</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> name</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">aux:task:delete-archived-tasks</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">time</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">2025-08-08T13:20:50.675+03:00</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> level</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">INFO</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> msg</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">\\"registered task\\"</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> name</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">gcp:task:collect-all</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">time</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">2025-08-08T13:20:50.675+03:00</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> level</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">INFO</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> msg</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">\\"registered task\\"</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> name</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">aws:task:collect-azs</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">time</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">2025-08-08T13:20:50.675+03:00</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> level</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">INFO</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> msg</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">\\"registered task\\"</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> name</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">aws:task:collect-instances</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\\">...</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\\">...</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\\">...</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">time</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">2025-08-08T13:20:50.675+03:00</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> level</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">INFO</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> msg</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">\\"registered task\\"</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> name</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">g:task:collect-aws-machine-images</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">time</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">2025-08-08T13:20:50.675+03:00</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> level</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">INFO</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> msg</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">\\"registered task\\"</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> name</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">gcp:task:collect-vpcs</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">time</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">2025-08-08T13:20:50.675+03:00</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> level</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">INFO</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> msg</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">\\"worker concurrency\\"</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> level</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">100</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">time</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">2025-08-08T13:20:50.675+03:00</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> level</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">INFO</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> msg</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">\\"queue priority\\"</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> strict</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">false</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">time</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">2025-08-08T13:20:50.675+03:00</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> level</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">INFO</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> msg</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">\\"queue configuration\\"</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> name</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">default</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> priority</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">1</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6F42C1;--shiki-dark:#B392F0\\">asynq:</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> pid=</span><span style=\\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\\">9332</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> 2025/08/08</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> 10:20:50.675999</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> INFO:</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> Starting</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> processing</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6F42C1;--shiki-dark:#B392F0\\">asynq:</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> pid=</span><span style=\\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\\">9332</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> 2025/08/08</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> 10:20:50.676050</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> INFO:</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> Send</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> signal</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> TSTP</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> to</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> stop</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> processing</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> new</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> tasks</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6F42C1;--shiki-dark:#B392F0\\">asynq:</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> pid=</span><span style=\\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\\">9332</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> 2025/08/08</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> 10:20:50.676052</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> INFO:</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> Send</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> signal</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> TERM</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> or</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> INT</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> to</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> terminate</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> the</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> process</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">time</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">2025-08-08T13:20:50.676+03:00</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> level</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">INFO</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> msg</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">\\"starting metrics server\\"</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> address</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">:6080</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\"> path</span><span style=\\"--shiki-light:#D73A49;--shiki-dark:#F97583\\">=</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">/metrics</span></span></code></pre>\\n</div><p>We can list the currently running workers by using the <code>worker list</code>\\nsub-command, e.g.:</p>\\n<div class=\\"language-shell vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">shell</span><pre class=\\"shiki shiki-themes github-light github-dark vp-code\\" tabindex=\\"0\\" v-pre=\\"\\"><code><span class=\\"line\\"><span style=\\"--shiki-light:#6F42C1;--shiki-dark:#B392F0\\">$</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> inventory</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> worker</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> list</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6F42C1;--shiki-dark:#B392F0\\"> HOST</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">       </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> PID</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">  </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> CONCURRENCY</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> STATUS</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> PROCESSING</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> UPTIME</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">       </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> QUEUES</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6F42C1;--shiki-dark:#B392F0\\"></span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#6F42C1;--shiki-dark:#B392F0\\"> some.node</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">  </span><span style=\\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\\"> 9332</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> </span><span style=\\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\\"> 100</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">         </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> active</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> </span><span style=\\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\\"> 0</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">          </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> 4m10.282643s</span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\"> default:1</span></span></code></pre>\\n</div><p>We can submit tasks to the Inventory workers by using the <code>inventory task submit</code>\\nsub-command. Usually, tasks are enqueued by the <code>scheduler</code> on\\nregular basis, so that collection happens periodically, but here we will show\\nhow to trigger an ad-hoc task execution.</p>\\n<p>Make sure that you also check the <code>examples/config.yaml</code> file which provides\\ndetails about the existing tasks and the payloads they expect. The following\\nspecifies a task payload, which will collect Gardener resources from a specific\\nproject only.</p>\\n<div class=\\"language-yaml vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">yaml</span><pre class=\\"shiki shiki-themes github-light github-dark vp-code\\" tabindex=\\"0\\" v-pre=\\"\\"><code><span class=\\"line\\"></span></code></pre>\\n</div>","date":{"time":1755086400000,"string":"August 13, 2025"}},{"title":"Announcing cluster-api-provider-gardener: Manage Gardener Clusters with Cluster API","url":"/blog/2025/08/08-04-cluster-api-provider-gardener","excerpt":"<h1 id=\\"announcing-cluster-api-provider-gardener-manage-gardener-clusters-with-cluster-api\\" tabindex=\\"-1\\">Announcing cluster-api-provider-gardener: Manage Gardener Clusters with Cluster API <a class=\\"header-anchor\\" href=\\"#announcing-cluster-api-provider-gardener-manage-gardener-clusters-with-cluster-api\\" aria-label=\\"Permalink to &quot;Announcing cluster-api-provider-gardener: Manage Gardener Clusters with Cluster API&quot;\\">&ZeroWidthSpace;</a></h1>\\n<p>We're pleased to announce the release of <a href=\\"https://github.com/gardener/cluster-api-provider-gardener/\\" target=\\"_blank\\" rel=\\"noreferrer\\">cluster-api-provider-gardener (CAPGa)</a>, an open-source <a href=\\"https://cluster-api.sigs.k8s.io/\\" target=\\"_blank\\" rel=\\"noreferrer\\">Cluster API</a> provider that leverages Gardener as the underlying platform for cluster lifecycle management.</p>\\n<!-- truncate -->\\n<h3 id=\\"what-is-cluster-api-provider-gardener-capga\\" tabindex=\\"-1\\">What is cluster-api-provider-gardener (CAPGa)? <a class=\\"header-anchor\\" href=\\"#what-is-cluster-api-provider-gardener-capga\\" aria-label=\\"Permalink to &quot;What is cluster-api-provider-gardener (CAPGa)?&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>CAPGa allows you to manage Kubernetes clusters using declarative <code>cluster.x-k8s.io</code> Cluster API manifests, with Gardener acting as the cloud-agnostic provider and cluster orchestrator.</p>\\n<p>Specifically, CAPGa implements the Cluster API's provider interfaces to manage Gardener's <a href=\\"https://gardener.cloud/docs/getting-started/shoots/\\" target=\\"_blank\\" rel=\\"noreferrer\\">Shoot clusters</a> as Cluster API <code>Cluster</code> resources. This allows users to provision, update, and delete clusters managed by Gardener via standard Cluster API tooling and workflows.</p>\\n<p>The following figure illustrates the semantic bi-directional mapping of Cluster API (CAPI) resources to Gardener API (GAPI) resources and vice versa:</p>\\n<p><img src=\\"/blog/2025/08/static/capi-interaction-gardener-capga.svg\\" alt=\\"Image showing the interaction of CAPI and Gardener through CAPGa\\"></p>\\n<h3 id=\\"what-is-the-difference-between-gardener-api-and-cluster-api\\" tabindex=\\"-1\\">What is the difference between Gardener API and Cluster API? <a class=\\"header-anchor\\" href=\\"#what-is-the-difference-between-gardener-api-and-cluster-api\\" aria-label=\\"Permalink to &quot;What is the difference between Gardener API and Cluster API?&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>Gardener and Cluster API both aim to automate Kubernetes cluster management, but they approach the problem from different directions and target different user groups.</p>\\n<h4 id=\\"gardener-api-platform-first-and-top-down\\" tabindex=\\"-1\\">Gardener API: Platform-first and top-down <a class=\\"header-anchor\\" href=\\"#gardener-api-platform-first-and-top-down\\" aria-label=\\"Permalink to &quot;Gardener API: Platform-first and top-down&quot;\\">&ZeroWidthSpace;</a></h4>\\n<p>Gardener is a top-down, platform-oriented solution designed to deliver homogeneous Kubernetes clusters across clouds as a service. It was built for application or service teams who require consistent, production-ready Kubernetes clusters without managing internal details.</p>\\n<ul>\\n<li>A single manifest defines an entire cluster (<code>Shoot</code>) with platform-specific defaults.</li>\\n<li>Users benefit from a user experience layer, sensible defaults, and well-integrated extensions (e.g., DNS, worker pools, cloud-specific settings).</li>\\n<li>Cluster lifecycle is abstracted: versioning, networking, and OS images are managed centrally.</li>\\n<li>Clusters can be managed across multiple cloud providers simultaneously.</li>\\n</ul>\\n<p>This setup is depicted in the following illustration, where the Gardener API is used to manage clusters as a service:</p>\\n<p><img src=\\"/blog/2025/08/static/gardener-managed.drawio.svg\\" alt=\\"Illustration: Gardener managed by service team\\"></p>\\n<h4 id=\\"cluster-api-infrastructure-centric-and-bottom-up\\" tabindex=\\"-1\\">Cluster API: Infrastructure-centric and bottom-up <a class=\\"header-anchor\\" href=\\"#cluster-api-infrastructure-centric-and-bottom-up\\" aria-label=\\"Permalink to &quot;Cluster API: Infrastructure-centric and bottom-up&quot;\\">&ZeroWidthSpace;</a></h4>\\n<p>Cluster API (CAPI), by contrast, is a bottom-up framework for building cluster management solutions. It targets infrastructure teams who need fine-grained control over every component of a Kubernetes cluster and understand how to assemble them.</p>\\n<ul>\\n<li>A cluster is defined via multiple resource manifests (Cluster, MachineDeployment, KubeadmConfig, etc.).</li>\\n<li>Each resource is reconciled by dedicated controllers.</li>\\n<li>There is no opinionated user interface; users must understand the internals of Kubernetes cluster bootstrapping.</li>\\n<li>It is powerful for building your own platform but requires substantial operational ownership.</li>\\n</ul>\\n<p>This setup is illustrated below, where the end user manages the full lifecycle of the Cluster API management plane:</p>\\n<p><img src=\\"/blog/2025/08/static/user-managed-capi.drawio.svg\\" alt=\\"Illustration: Cluster API self-managed\\"></p>\\n<div class=\\"note custom-block github-alert\\"><p class=\\"custom-block-title\\">NOTE</p>\\n<p>You can think of Gardener as a second evolutionary stage of Cluster API: more opinionated, more integrated, and focused on platform-level concerns.</p>\\n</div>\\n<h3 id=\\"when-to-use-which\\" tabindex=\\"-1\\">When to use which? <a class=\\"header-anchor\\" href=\\"#when-to-use-which\\" aria-label=\\"Permalink to &quot;When to use which?&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>This brings us to the question of when to use <code>CAPI</code> and when to use <code>GAPI</code>.\\nThe following table outlines some helpful criteria for your evaluation:</p>\\n<p>|  | Cluster API | Gardener API |\\n|</p>\\n","date":{"time":1754308800000,"string":"August 4, 2025"}},{"title":"Enhanced Network Flexibility: CIDR Overlap Now Supported for HA Shoots","url":"/blog/2025/07/07-30-enhanced-network-flexibility-cidr-overlap-now-supported-for-ha-shoots","excerpt":"","date":{"time":1753876800000,"string":"July 30, 2025"}},{"title":"Enhancing Data Protection with Immutable Backup Buckets","url":"/blog/2025/07/07-16-enhancing-data-protection-with-immutable-backup-buckets","excerpt":"","date":{"time":1752667200000,"string":"July 16, 2025"}},{"title":"Getting Started with OpenTelemetry on a Gardener Shoot Cluster","url":"/blog/2025/06/06-30-getting-started-with-opentelemetry-on-gardener-shoot-cluster","excerpt":"","date":{"time":1751284800000,"string":"June 30, 2025"}},{"title":"Enabling Seamless IPv4 to Dual-Stack Migration for Kubernetes Clusters on GCP","url":"/blog/2025/06/06-18-enabling-seamless-ipv4-to-dual-stack-migration-for-kubernetes-clusters-on-gcp","excerpt":"","date":{"time":1750852800000,"string":"June 25, 2025"}},{"title":"Enhanced Health Checks for Node Rolling Updates","url":"/blog/2025/06/06-25-enhanced-health-checks-for-node-rolling-updates","excerpt":"<h1 id=\\"enhanced-health-checks-for-node-rolling-updates\\" tabindex=\\"-1\\">Enhanced Health Checks for Node Rolling Updates <a class=\\"header-anchor\\" href=\\"#enhanced-health-checks-for-node-rolling-updates\\" aria-label=\\"Permalink to &quot;Enhanced Health Checks for Node Rolling Updates&quot;\\">&ZeroWidthSpace;</a></h1>\\n<p>For operators managing Kubernetes clusters, clear and accurate health status is essential for stability and efficient troubleshooting. A recent enhancement to Gardener's <code>shoot-care</code> controller improves the precision of health checks during one of the most common operational tasks: rolling updates of worker nodes.</p>\\n<h3 id=\\"the-challenge-with-rolling-update-status\\" tabindex=\\"-1\\">The Challenge with Rolling Update Status <a class=\\"header-anchor\\" href=\\"#the-challenge-with-rolling-update-status\\" aria-label=\\"Permalink to &quot;The Challenge with Rolling Update Status&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>Previously, when a Shoot cluster's worker nodes underwent a rolling updatefor example, during an OS or Kubernetes version upgradethe health check could report a misleading status. A rolling update involves creating new machines before terminating old ones, causing the total machine count to temporarily exceed the desired replica count. The health check logic would often interpret this state as a scale-down event, reporting the reason <code>NodesScalingDown</code>. This was confusing because the cluster was actively rolling out new infrastructure, not reducing its capacity.</p>\\n<h3 id=\\"a-more-precise-health-check\\" tabindex=\\"-1\\">A More Precise Health Check <a class=\\"header-anchor\\" href=\\"#a-more-precise-health-check\\" aria-label=\\"Permalink to &quot;A More Precise Health Check&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>To resolve this ambiguity, the health check logic has been refined to be more context-aware. The controller now specifically detects when a <code>MachineDeployment</code> is in the middle of a rolling update by inspecting its <code>MachineDeploymentProgressing</code> condition.</p>\\n<p>When this condition is active and indicates that a new machine set is being rolled out, the controller assigns a new, more descriptive status: <code>NodesRollOutScalingUp</code>. This status is prioritized over other scaling-related conditions, ensuring that operators receive the most accurate information possible. Instead of seeing a confusing <code>NodesScalingDown</code> message, they will now see a clear indication that a planned node rollout is in progress.</p>\\n<p>This improvement provides operators with a much clearer understanding of the cluster's state. The enhanced accuracy reduces confusion during maintenance activities and helps distinguish a normal rolling update from an unintended scaling issue, leading to more confident and effective cluster management.</p>\\n","date":{"time":1750852800000,"string":"June 25, 2025"}},{"title":"Enhancing Meltdown Protection with Dependency-Watchdog Annotations","url":"/blog/2025/06/06-25-enhancing-meltdown-protection-with-dependency-watchdog-annotations","excerpt":"","date":{"time":1750852800000,"string":"June 25, 2025"}},{"title":"Improving Credential Management for Seed Backups","url":"/blog/2025/06/06-25-improving-credential-management-for-seed-backups","excerpt":"","date":{"time":1750852800000,"string":"June 25, 2025"}},{"title":"Introducing \`gardenadm bootstrap\` for Autonomous Shoots","url":"/blog/2025/06/06-25-introducing-gardenadm-bootstrap-for-autonomous-shoots","excerpt":"","date":{"time":1750852800000,"string":"June 25, 2025"}},{"title":"Enhanced Extension Management: Introducing \`autoEnable\` and \`clusterCompatibility\`","url":"/blog/2025/06/06-18-enhanced-extension-management-introducing-autoenable-and-clustercompatibility","excerpt":"","date":{"time":1750248000000,"string":"June 18, 2025"}},{"title":"Enhanced Internal Traffic Management: L7 Load Balancing for kube-apiservers in Gardener","url":"/blog/2025/06/06-18-enhanced-internal-traffic-management-l7-load-balancing-for-kube-apiservers-in-gardener","excerpt":"","date":{"time":1750248000000,"string":"June 18, 2025"}},{"title":"Gardener Enhances Observability with OpenTelemetry Integration for Logging","url":"/blog/2025/06/06-18-gardener-enhances-observability-with-opentelemetry-integration-for-logging","excerpt":"","date":{"time":1750248000000,"string":"June 18, 2025"}},{"title":"Taking Gardener to the Next Level: Highlights from the 7th Gardener Community Hackathon in Schelklingen","url":"/blog/2025/06/06-17-Taking-Gardener-to-the-Next-Level-Highlights-from-the-7th-Gardener-Community-Hackathon-in-Schelklingen","excerpt":"","date":{"time":1750161600000,"string":"June 17, 2025"}},{"title":"Fine-Tuning kube-proxy Readiness: Ensuring Accurate Health Checks During Node Scale-Down","url":"/blog/2025/05/05-21-fine-tuning-kube-proxy-readiness-ensuring-accurate-health-checks-during-node-scale-down","excerpt":"","date":{"time":1747828800000,"string":"May 21, 2025"}},{"title":"New in Gardener: Forceful Redeployment of gardenlets for Enhanced Operational Control","url":"/blog/2025/05/05-21-new-in-gardener-forceful-redeployment-of-gardenlets-for-enhanced-operational-control","excerpt":"","date":{"time":1747828800000,"string":"May 21, 2025"}},{"title":"Streamlined Node Onboarding: Introducing \`gardenadm token\` and \`gardenadm join\`","url":"/blog/2025/05/05-21-streamlined-node-onboarding-introducing-gardenadm-token-and-gardenadm-join","excerpt":"","date":{"time":1747828800000,"string":"May 21, 2025"}},{"title":"Enhanced Network Flexibility: Gardener Now Supports CIDR Overlap for Non-HA Shoots","url":"/blog/2025/05/05-19-enhanced-network-flexibility-gardener-now-supports-cidr-overlap-for-non-ha-shoots","excerpt":"","date":{"time":1747656000000,"string":"May 19, 2025"}},{"title":"Enhanced Node Management: Introducing In-Place Updates in Gardener","url":"/blog/2025/05/05-19-enhanced-node-management-introducing-in-place-updates-in-gardener","excerpt":"","date":{"time":1747656000000,"string":"May 19, 2025"}},{"title":"Gardener Dashboard 1.80: Streamlined Credentials, Enhanced Cluster Views, and Real-Time Updates","url":"/blog/2025/05/05-19-gardener-dashboard-180-streamlined-credentials-enhanced-cluster-views-and-real-time-updates","excerpt":"","date":{"time":1747656000000,"string":"May 19, 2025"}},{"title":"Gardener: Powering Enterprise Kubernetes at Scale and Europe's Sovereign Cloud Future","url":"/blog/2025/05/05-12-Gardener-NeoNephos","excerpt":"","date":{"time":1747051200000,"string":"May 12, 2025"}},{"title":"Leaner Clusters, Lower Bills: How Gardener Optimized Kubernetes Compute Costs","url":"/blog/2025/04/04-17-Leaner-Clusters-Lower-Bills","excerpt":"<h1 id=\\"leaner-clusters-lower-bills-how-gardener-optimized-kubernetes-compute-costs\\" tabindex=\\"-1\\">Leaner Clusters, Lower Bills: How Gardener Optimized Kubernetes Compute Costs <a class=\\"header-anchor\\" href=\\"#leaner-clusters-lower-bills-how-gardener-optimized-kubernetes-compute-costs\\" aria-label=\\"Permalink to &quot;Leaner Clusters, Lower Bills: How Gardener Optimized Kubernetes Compute Costs&quot;\\">&ZeroWidthSpace;</a></h1>\\n<p>As organizations embrace Kubernetes for managing containerized applications at scale, the underlying infrastructure costs, particularly for compute resources, become a critical factor. Gardener, the open-source Kubernetes management platform, empowers organizations like SAP, STACKIT, T-Systems, and others (see <a href=\\"https://gardener.cloud/adopter\\" target=\\"_blank\\" rel=\\"noreferrer\\">adopters</a>) to operate tens of thousands of Kubernetes clusters efficiently across diverse environments. Gardener's role as a core technology in initiatives like <a href=\\"https://neonephos.org/projects\\" target=\\"_blank\\" rel=\\"noreferrer\\">NeoNephos</a>, aimed at advancing digital autonomy in Europe (see <a href=\\"https://www.youtube.com/watch?v=85MDID9Ju04&amp;t=621s\\" target=\\"_blank\\" rel=\\"noreferrer\\">KubeCon London 2025 Keynote</a> and <a href=\\"https://linuxfoundation.eu/newsroom/the-linux-foundation-announces-the-launch-of-neonephos-to-advance-digital-autonomy-in-europe\\" target=\\"_blank\\" rel=\\"noreferrer\\">press announcement</a>), further underscores the need for cost-effective and sustainable operations.</p>\\n<p>At the heart of Gardener's architecture is the concept of &quot;Kubeception&quot; (see <a href=\\"https://github.com/gardener/gardener?tab=readme-ov-file#gardener\\" target=\\"_blank\\" rel=\\"noreferrer\\">readme</a> and <a href=\\"/docs/gardener/concepts/architecture/\\">architecture</a>): Gardener runs <em>on</em> Kubernetes (called a <strong>runtime cluster</strong>), facilitates access <em>through</em> a self-managed node-less Kubernetes cluster (called the <strong>garden cluster</strong>), manages Kubernetes control planes as pods <em>within</em> self-managed Kubernetes clusters that provide high scalability to Gardener (called <strong>seed clusters</strong>), and <em>provisions</em> end-user Kubernetes clusters (called <strong>shoot clusters</strong>). Therefore, optimizing Gardener's own Kubernetes-related resource consumption directly translates into cost savings across all these layers, benefiting both Gardener service providers and the end-users consuming the managed clusters.</p>\\n<p>While infrastructure costs span compute, storage, and networking, compute resources (the virtual machines running Kubernetes nodes) typically represent the largest share of the bill. Over the past years, the Gardener team has undertaken a significant effort to optimize these costs. This blog post details our journey, focusing heavily on the compute optimizations that go beyond standard autoscaling practices, ultimately delivering substantial savings that benefit the entire Gardener ecosystem.</p>\\n<p>We'll build upon the foundations laid out in our <a href=\\"/docs/guides/applications/shoot-pod-autoscaling-best-practices/\\">Pod Autoscaling Best Practices Guide</a>. You may want to check it out beforehand, as we'll only touch upon a few key recommendations from it in this blog post, not delving into the full depth required for effective pod autoscaling  a prerequisite for the compute optimizations discussed here.</p>\\n<h2 id=\\"visibility-and-initial-measures\\" tabindex=\\"-1\\">Visibility and Initial Measures <a class=\\"header-anchor\\" href=\\"#visibility-and-initial-measures\\" aria-label=\\"Permalink to &quot;Visibility and Initial Measures&quot;\\">&ZeroWidthSpace;</a></h2>\\n<h3 id=\\"know-your-spending-leveraging-observability-and-iaas-cost-tools\\" tabindex=\\"-1\\">Know Your Spending: Leveraging Observability and IaaS Cost Tools <a class=\\"header-anchor\\" href=\\"#know-your-spending-leveraging-observability-and-iaas-cost-tools\\" aria-label=\\"Permalink to &quot;Know Your Spending: Leveraging Observability and IaaS Cost Tools&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>You can't optimize what you can't measure. Our first step was to gain deep visibility into our spending patterns. We leveraged:</p>\\n<ul>\\n<li><strong>IaaS Cost Reports &amp; Alerts:</strong> Regularly analyzing detailed cost breakdowns from cloud providers (AWS Cost Explorer, Azure Cost Management, GCP Billing Reports) helped us identify major cost drivers across compute, storage, and network usage. Setting up alerts for cost anomalies makes us aware of regressions and unexpected budget overruns.</li>\\n<li><strong>Cloud Provider Recommendation Tools:</strong> Tools like AWS Trusted Advisor, Azure Advisor's Cost recommendations, and Google Cloud's machine type rightsizing recommendations provided initial, manual pointers towards obvious inefficiencies like underutilized virtual machines or suboptimal instance types.</li>\\n<li><strong>Internal Usage Reports:</strong> We generated custom reports detailing our own resource consumption. This helped identify and drive down the number and uptime of development and other non-production clusters. Automating the configuration of Gardener's <a href=\\"/docs/gardener/shoot/shoot_hibernate/\\">cluster hibernation feature</a> or reporting on clusters with poor hibernation schedules further curbed unnecessary spending. These insights are now integrated into the Gardener Dashboard (our GUI).</li>\\n</ul>\\n<h3 id=\\"the-reserved-instance-savings-plan-imperative-planning-for-discounts\\" tabindex=\\"-1\\">The Reserved Instance / Savings Plan Imperative: Planning for Discounts <a class=\\"header-anchor\\" href=\\"#the-reserved-instance-savings-plan-imperative-planning-for-discounts\\" aria-label=\\"Permalink to &quot;The Reserved Instance / Savings Plan Imperative: Planning for Discounts&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>Cloud providers offer significant discounts for commitment: Reserved Instances (RIs) on AWS/Azure, Savings Plans (SPs) on AWS/Azure, and Committed Use Discounts (CUDs) on GCP. However, maximizing their benefit requires careful planning, which is not the primary subject of this blog post. Companies typically have tools that generate recommendations from cost reports, suggesting the purchase of new RIs, SPs, or CUDs if on-demand usage consistently increases. Two key learnings emerged in this context, though:</p>\\n<ul>\\n<li><strong>Coordination between Operations and Controlling:</strong> We discovered that technical optimizations and discount commitment purchases <em>must</em> go hand-in-hand. A significant 20% utilization improvement can be completely negated if the remaining workload runs on expensive on-demand instances because the RI/SP/CUD purchase didn't account for the change. On-demand pricing can easily be twice or more expensive than committed pricing.</li>\\n<li><strong>Commitments vs. Spot Pricing:</strong> While Spot Instances/Preemptible virtual machines offer deep discounts, their ephemeral nature makes them unsuitable for critical control plane components. For predictable baseline workloads, well-planned RIs/SPs/CUDs provide substantial, reliable savings and are often more beneficial overall. Spot Instance/Preemptible VM discounts are generally not higher than, and often less than, RI/SP/CUD discounts for comparable commitment levels.</li>\\n</ul>\\n<h3 id=\\"early-wins-finding-and-eliminating-resource-waste\\" tabindex=\\"-1\\">Early Wins: Finding and Eliminating Resource Waste <a class=\\"header-anchor\\" href=\\"#early-wins-finding-and-eliminating-resource-waste\\" aria-label=\\"Permalink to &quot;Early Wins: Finding and Eliminating Resource Waste&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>We also actively looked for waste, specifically orphaned resources. Development and experimentation inevitably lead to forgotten resources (virtual machines, disks, load balancers, etc.). We implemented processes like requiring all resources to include a personal identifier in the name or as a label/tag to facilitate later cleanup. Initially, we generated simple reports, but it became clear that this task required a more professional approach. Unaccounted-for resources aren't just costly; they can also pose security risks or indicate security incidents. Therefore, we developed the <a href=\\"https://github.com/gardener/inventory\\" target=\\"_blank\\" rel=\\"noreferrer\\"><code>gardener/inventory</code></a> tool. This tool understands Gardener installations and cross-references expected cloud provider resources (based on Gardener's desired state and implementation) against actually existing resources. It acts as an additional safety net, alerting on discrepancies (e.g., unexpected load balancers for a seed, unmanaged virtual machines in a VPC) which could indicate either cost leakage or a potential security issue, complementing Gardener's existing security measures like high-frequency credentials rotation, image signing and admission, network policies, Falco, etc.</p>\\n<h3 id=\\"consolidation-avoiding-a-fragmented-seed-landscape\\" tabindex=\\"-1\\">Consolidation: Avoiding a Fragmented Seed Landscape <a class=\\"header-anchor\\" href=\\"#consolidation-avoiding-a-fragmented-seed-landscape\\" aria-label=\\"Permalink to &quot;Consolidation: Avoiding a Fragmented Seed Landscape&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>If possible, avoid operating too many small seeds unless required by regulations or driven by end-user demand. As Gardener supports control plane migration, you can consolidate your control planes into fewer, larger seeds where reasonable. Since starting Gardener in production in 2017, we've encountered technological advancements (e.g., Azure Availability Sets to Zones) and corrected initial misconfigurations (e.g., too-small CIDR ranges limiting pod/node counts) that necessitated recreating seeds. While hard conflicts (like seed/shoot cluster IP address overlaps) can sometimes block migration to differently configured seeds, you can often at least merge multiple seeds into one or fewer. The key takeaway is that a less fragmented seed landscape generally leads to better efficiency.</p>\\n<p>However, there is a critical caveat: Gardener allows control planes to reside in different regions (or even different cloud providers) than their worker nodes. This flexibility comes at the cost of inter-regional or internet network traffic. These additional network-related costs can easily negate efficiency gains from seed consolidation. Therefore, consolidate thoughtfully, being mindful that excessive consolidation across regions can significantly increase network costs (intra-region traffic is cheaper than inter-region traffic, and internet traffic is usually the most expensive).</p>\\n<h2 id=\\"quick-wins-in-networking-and-storage\\" tabindex=\\"-1\\">Quick Wins in Networking and Storage <a class=\\"header-anchor\\" href=\\"#quick-wins-in-networking-and-storage\\" aria-label=\\"Permalink to &quot;Quick Wins in Networking and Storage&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>While compute was our main focus, we also addressed significant cost drivers in networking and storage early on.</p>\\n<h3 id=\\"centralized-ingress-caching\\" tabindex=\\"-1\\">Centralized Ingress &amp; Caching <a class=\\"header-anchor\\" href=\\"#centralized-ingress-caching\\" aria-label=\\"Permalink to &quot;Centralized Ingress &amp; Caching&quot;\\">&ZeroWidthSpace;</a></h3>\\n<ul>\\n<li><strong>Centralized Ingress:</strong> In Gardener's early days, each shoot control plane had its own Load Balancer (LB), plus another for the reverse tunnel connection to worker nodes (to reach webhooks, scrape metrics, stream logs, <code>exec</code> into pods, etc.). This proliferation of LBs was expensive. We transitioned to a model using a central Istio ingress-gateway per seed cluster with a single LB, leveraging SNI (Server Name Indication) routing to direct traffic to the correct control plane API servers. We also reversed the connection direction: shoots now connect <em>to</em> seed clusters, and seeds connect <em>to</em> the garden cluster. This reduced the need for LBs exposing seed components and enabled <em>private</em> shoots or even <em>private</em> seeds behind firewalls.</li>\\n<li><strong>Registry Cache:</strong> Pulling container images for essential components (like CNI, CSI drivers, kube-proxy) on every new node startup generated significant network traffic and costs. We implemented a <a href=\\"https://github.com/gardener/gardener-extension-registry-cache\\" target=\\"_blank\\" rel=\\"noreferrer\\">registry cache extension</a>, drastically reducing external image pulls (see <a href=\\"/blog/2024/04-22-gardener's-registry-cache-extension-another-cost-saving-win-and-more/\\">blog post</a>).</li>\\n</ul>\\n<h3 id=\\"smarter-networking-habits\\" tabindex=\\"-1\\">Smarter Networking Habits <a class=\\"header-anchor\\" href=\\"#smarter-networking-habits\\" aria-label=\\"Permalink to &quot;Smarter Networking Habits&quot;\\">&ZeroWidthSpace;</a></h3>\\n<ul>\\n<li><strong>Efficient API Usage:</strong> Well-implemented controllers use <code>watch</code> requests rather than frequent <code>list</code> requests to minimize API server load and improve responsiveness. Leveraging server-side filtering via <a href=\\"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors\\" target=\\"_blank\\" rel=\\"noreferrer\\">label selectors</a> and <a href=\\"https://kubernetes.io/docs/concepts/overview/working-with-objects/field-selectors\\" target=\\"_blank\\" rel=\\"noreferrer\\">field selectors</a> reduces the amount of data transferred.</li>\\n<li><strong>Reducing Cross-Zonal Traffic:</strong> Data transfer between availability zones, necessary for highly available control planes, is generally more expensive than within a single zone. We enabled Kubernetes' <a href=\\"https://kubernetes.io/docs/concepts/services-networking/topology-aware-hints\\" target=\\"_blank\\" rel=\\"noreferrer\\">Topology Aware Routing</a> to help route API server traffic within the same zone where possible, reducing cross-zonal traffic and therefore costs (see <a href=\\"https://github.com/gardener/gardener/issues/6718\\" target=\\"_blank\\" rel=\\"noreferrer\\">Gardener Issue #6718</a>).</li>\\n<li><strong>Avoiding Large Resources:</strong> Storing large amounts of data directly in Kubernetes resources (ConfigMaps, Secrets) is inefficient and strains etcd and the network. We utilize blob stores for large payloads, such as control plane etcd or state backups used for automated restoration or control plane migration (with data compressed and encrypted in transit and at rest).</li>\\n<li><strong>Regression Monitoring:</strong> Implementing regression monitoring for network traffic helped catch seemingly innocent code changes that could inadvertently cause massive spikes in data transfer costs.</li>\\n</ul>\\n<h3 id=\\"conscious-storage-consumption\\" tabindex=\\"-1\\">Conscious Storage Consumption <a class=\\"header-anchor\\" href=\\"#conscious-storage-consumption\\" aria-label=\\"Permalink to &quot;Conscious Storage Consumption&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>Storage costs were addressed by being mindful of Persistent Volume Claim (PVC) size and performance tiers (e.g., standard HDD vs. premium SSD). Choosing the right storage class based on actual workload needs prevents overspending on unused capacity or unnecessary IOPS.</p>\\n<h2 id=\\"deep-dive-into-compute-cost-optimization\\" tabindex=\\"-1\\">Deep Dive into Compute Cost Optimization <a class=\\"header-anchor\\" href=\\"#deep-dive-into-compute-cost-optimization\\" aria-label=\\"Permalink to &quot;Deep Dive into Compute Cost Optimization&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>This is where the most significant savings were realized. Optimizing compute utilization in Kubernetes is a multi-faceted challenge involving the interplay of several components.</p>\\n<h3 id=\\"understanding-utilization-the-interplay-of-scheduler-cluster-autoscaler-hpa-and-vpa\\" tabindex=\\"-1\\">Understanding Utilization: The Interplay of Scheduler, Cluster Autoscaler, HPA, and VPA <a class=\\"header-anchor\\" href=\\"#understanding-utilization-the-interplay-of-scheduler-cluster-autoscaler-hpa-and-vpa\\" aria-label=\\"Permalink to &quot;Understanding Utilization: The Interplay of Scheduler, Cluster Autoscaler, HPA, and VPA&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>We think of utilization optimization in two stages:</p>\\n<ol>\\n<li><strong>Packing Pods onto Nodes (Requests vs. Allocatable):</strong> How efficiently are the resource <em>requests</em> of your pods filling up the <em>allocatable</em> capacity of your nodes? This is primarily influenced by the Kube-Scheduler and the Cluster Autoscaler (CA).</li>\\n<li><strong>Right-Sizing Pods (Usage vs. Requests):</strong> How closely does the actual resource <em>usage</em> of your pods match their <em>requests</em>? This is where Horizontal Pod Autoscaler (HPA) and Vertical Pod Autoscaler (VPA) come in.</li>\\n</ol>\\n<p>You need to optimize <em>both</em> stages for maximum efficiency.</p>\\n<h3 id=\\"optimizing-scheduling-bin-packing-and-pod-priorities-with-kube-scheduler\\" tabindex=\\"-1\\">Optimizing Scheduling: Bin-Packing and Pod Priorities with Kube-Scheduler <a class=\\"header-anchor\\" href=\\"#optimizing-scheduling-bin-packing-and-pod-priorities-with-kube-scheduler\\" aria-label=\\"Permalink to &quot;Optimizing Scheduling: Bin-Packing and Pod Priorities with Kube-Scheduler&quot;\\">&ZeroWidthSpace;</a></h3>\\n<ul>\\n<li><strong>Bin-Packing:</strong> By default, Kube-Scheduler tries to spread pods across nodes (using the <code>LeastAllocated</code> strategy). For cost optimization, <em>packing</em> pods tightly onto fewer nodes (using the <code>MostAllocated</code> strategy, often called bin-packing) is more effective. Gardener runs Kubernetes control planes as pods on seed clusters. Switching the Kube-Scheduler profile in our seed clusters to prioritize bin-packing yielded over 20% reduction in machine costs for these clusters simply by requiring fewer nodes. We also made this scheduling profile available for shoot clusters (see <a href=\\"https://github.com/gardener/gardener/pull/6251\\" target=\\"_blank\\" rel=\\"noreferrer\\">Gardener PR #6251</a>).</li>\\n<li><strong>Pod Priorities:</strong> Assigning proper <a href=\\"https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption\\" target=\\"_blank\\" rel=\\"noreferrer\\">Pod Priorities</a> is important not just for stability but also for cost. High-priority pods (like control plane components) can preempt lower-priority pods if necessary, reducing the need to maintain excess capacity just in case a critical pod needs scheduling space. This avoids unnecessary over-provisioning.</li>\\n</ul>\\n<h3 id=\\"voluntary-disruptions-pod-disruption-budgets\\" tabindex=\\"-1\\">Voluntary Disruptions: Pod Disruption Budgets <a class=\\"header-anchor\\" href=\\"#voluntary-disruptions-pod-disruption-budgets\\" aria-label=\\"Permalink to &quot;Voluntary Disruptions: Pod Disruption Budgets&quot;\\">&ZeroWidthSpace;</a></h3>\\n<ul>\\n<li><strong>Pod Disruption Budgets:</strong> Defining proper <a href=\\"https://kubernetes.io/docs/tasks/run-application/configure-pdb\\" target=\\"_blank\\" rel=\\"noreferrer\\">Pod Disruption Budgets (PDBs)</a> helps manage and steer voluntary disruptions safely. We define them consistently for all Gardener components. This provides the necessary control to rebalance, compact, or generally replace underlying machines as needed by us or our automation, contributing to cost efficiency by enabling node consolidation.</li>\\n</ul>\\n<h3 id=\\"enabling-higher-pod-density-per-node\\" tabindex=\\"-1\\">Enabling Higher Pod Density per Node <a class=\\"header-anchor\\" href=\\"#enabling-higher-pod-density-per-node\\" aria-label=\\"Permalink to &quot;Enabling Higher Pod Density per Node&quot;\\">&ZeroWidthSpace;</a></h3>\\n<ul>\\n<li><strong>Node Configuration</strong>: To effectively utilize larger instance types and enable better bin-packing, nodes must be configured to handle more pods. We observed nodes becoming pod-bound (unable to schedule more pods despite available CPU/memory). To prevent this, ensure you configure the following:\\n<ul>\\n<li><strong>Sufficient IP Addresses</strong>: Provide a large enough <code>--node-cidr-mask-size</code> (e.g., <code>/22</code> for ~1024 IPs, though assume ~80% effective due to IP reuse; see <a href=\\"https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager\\" target=\\"_blank\\" rel=\\"noreferrer\\">kube-controller-manager docs</a>) to allocate sufficient IPs per node.</li>\\n<li><strong>Kubelet Capacity</strong>: Set an increased <code>--max-pods</code> value (see <a href=\\"https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet\\" target=\\"_blank\\" rel=\\"noreferrer\\">kubelet docs</a>) to inform the kubelet and scheduler of the node's actual pod capacity.</li>\\n<li><strong>Reserved Resources</strong>: Allocate sufficient <code>--kube-reserved</code> resources (again, see <a href=\\"https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet\\" target=\\"_blank\\" rel=\\"noreferrer\\">kubelet docs</a>) to account for system overhead from the increased pod count.</li>\\n<li><strong>ARP Cache Tuning</strong>: When running a high number of pods (e.g., 400 or more), the default kernel ARP cache size may be insufficient, leading to an &quot;ARP cache overflow.&quot; This triggers constant, CPU-intensive garbage collection, which can destabilize the node and cause networking failures. To mitigate this, you should increase the ARP cache garbage collection thresholds. In a Gardener <code>Shoot</code> specification, you can apply these settings via <code>sysctls</code>:<div class=\\"language-yaml vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">yaml</span><pre class=\\"shiki shiki-themes github-light github-dark vp-code\\" tabindex=\\"0\\" v-pre=\\"\\"><code><span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">spec</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">:</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">  provider</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">:</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">    workers</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">:</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">    - </span><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">name</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">worker-pool-name</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">      (...)</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">      sysctls</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">:</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">        net.ipv4.neigh.default.gc_thresh2</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">'1024'</span><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\"> # Default is 512</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">        net.ipv4.neigh.default.gc_thresh3</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">'2048'</span><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\"> # Default is 1024</span></span></code></pre>\\n</div></li>\\n</ul>\\n</li>\\n</ul>\\n<h3 id=\\"fine-tuning-the-cluster-autoscaler-scaling-nodes-efficiently\\" tabindex=\\"-1\\">Fine-Tuning the Cluster Autoscaler: Scaling Nodes Efficiently <a class=\\"header-anchor\\" href=\\"#fine-tuning-the-cluster-autoscaler-scaling-nodes-efficiently\\" aria-label=\\"Permalink to &quot;Fine-Tuning the Cluster Autoscaler: Scaling Nodes Efficiently&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>The cluster autoscaler (CA) adds or removes nodes based on pending pods and node utilization. We tuned its behavior for better cost efficiency:</p>\\n<ul>\\n<li><code>--scale-down-unneeded-time=15m</code>: Time a node must be underutilized before CA considers it for removal, allowing removal of persistently unneeded capacity.</li>\\n<li><code>--scale-down-delay-after-add=30m</code>: Prevents CA from removing a node too soon after adding one, reducing potential node thrashing during fluctuating load.</li>\\n<li><code>--scale-down-utilization-threshold=0.9</code>: We significantly increased this threshold (default is 0.5). It instructs CA to attempt removing any node running below 90% utilization <em>if</em> it can safely reschedule the existing pods onto other available nodes; otherwise, it does nothing. We have run with this setting successfully for a long time, supported by properly tuned pod priorities, PDBs managing voluntary disruptions, highly available control planes, and Kubernetes' level-triggered, asynchronous nature.</li>\\n</ul>\\n<h3 id=\\"mastering-pod-autoscaling-hpa-vpa-and-best-practices\\" tabindex=\\"-1\\">Mastering Pod Autoscaling: HPA, VPA, and Best Practices <a class=\\"header-anchor\\" href=\\"#mastering-pod-autoscaling-hpa-vpa-and-best-practices\\" aria-label=\\"Permalink to &quot;Mastering Pod Autoscaling: HPA, VPA, and Best Practices&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>Right-sizing pods dynamically is key. Kubernetes offers HPA and VPA:</p>\\n<ul>\\n<li><strong>Horizontal Pod Autoscaling (HPA):</strong> Scales the <em>number</em> of pod replicas based on metrics (CPU/memory utilization, custom metrics). Ideal for stateless applications handling variable request loads.</li>\\n<li><strong>Vertical Pod Autoscaler (VPA):</strong> Adjusts the CPU/memory <em>requests</em> of existing pods. Ideal for stateless and also stateful applications or workloads with fluctuating resource needs over time, without changing replica count.</li>\\n</ul>\\n<h3 id=\\"our-best-practices-learnings\\" tabindex=\\"-1\\">Our Best Practices &amp; Learnings: <a class=\\"header-anchor\\" href=\\"#our-best-practices-learnings\\" aria-label=\\"Permalink to &quot;Our Best Practices &amp; Learnings:&quot;\\">&ZeroWidthSpace;</a></h3>\\n<ul>\\n<li><strong>Combine HPA and VPA for API Servers Safely:</strong> You <em>can</em> use HPA and VPA together, even on the same metric (like CPU), but careful configuration is essential. The key is to configure HPA to scale based on the <em>average value</em> (<code>target.type: AverageValue</code>) rather than <em>utilization percentage</em> (<code>target.type: Utilization</code>). This prevents conflicts where VPA changes the requests, which would otherwise immediately invalidate HPA's utilization calculation.\\n<ul>\\n<li><em>Example HPA targeting average CPU/Memory values:</em><div class=\\"language-yaml vp-adaptive-theme\\"><button title=\\"Copy Code\\" class=\\"copy\\"></button><span class=\\"lang\\">yaml</span><pre class=\\"shiki shiki-themes github-light github-dark vp-code\\" tabindex=\\"0\\" v-pre=\\"\\"><code><span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">spec</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">:</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">  minReplicas</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\\">3</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">  maxReplicas</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\\">12</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">  metrics</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">:</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">  - </span><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">resource</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">:</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">      name</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">cpu</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">      target</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">:</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">        averageValue</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\\">6</span><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\"> # Target 6 cores average usage per pod (Note: String value often required)</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">        type</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">AverageValue</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">    type</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">Resource</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">  - </span><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">resource</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">:</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">      name</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">memory</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">      target</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">:</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">        averageValue</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">24Gi</span><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\"> # Target 24Gi average usage per pod</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">        type</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">AverageValue</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">    type</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">Resource</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">  behavior</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#6A737D;--shiki-dark:#6A737D\\"># Fine-tune scaling behavior</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">    scaleDown</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">:</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">      policies</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">:</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">      - </span><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">periodSeconds</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\\">300</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">        type</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">Pods</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">        value</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\\">1</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">      selectPolicy</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">Max</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">      stabilizationWindowSeconds</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\\">1800</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">    scaleUp</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">:</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">      policies</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">:</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">      - </span><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">periodSeconds</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\\">60</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">        type</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">Percent</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">        value</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\\">100</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">      selectPolicy</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">Max</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">      stabilizationWindowSeconds</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#005CC5;--shiki-dark:#79B8FF\\">60</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">  scaleTargetRef</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">:</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">    apiVersion</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">apps/v1</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">    kind</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">Deployment</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#22863A;--shiki-dark:#85E89D\\">    name</span><span style=\\"--shiki-light:#24292E;--shiki-dark:#E1E4E8\\">: </span><span style=\\"--shiki-light:#032F62;--shiki-dark:#9ECBFF\\">kube-apiserver</span></span></code></pre>\\n</div></li>\\n</ul>\\n</li>\\n<li><strong>Tune VPA Configuration:</strong>\\n<ul>\\n<li>We adjusted VPA parameters like <code>--target-cpu-percentile</code> / <code>--target-memory-percentile</code> (determining the percentile of historical usage data to include in target recommendations, ignoring spikes above) and margin/bound parameters to make VPA less sensitive to tiny spikes and react faster and more accurately to sustained changes.</li>\\n<li>We also tuned parameters like <code>--cpu-histogram-decay-half-life</code> (from 24h to 15m) and <code>--recommendation-lower-bound-cpu-percentile</code> (from 0.5 to 0.7) to follow changes in CPU utilization more closely (work on memory is ongoing).</li>\\n<li><strong>VPA <code>minAllowed</code>:</strong> We set <code>minAllowed</code> (per VPA resource) based on observed usage patterns and historical outage data related to VPA scaling down too aggressively.</li>\\n<li><strong>VPA <code>maxAllowed</code>:</strong> We set <code>maxAllowed</code> (per VPA controller) to prevent request recommendations from exceeding node capacity. We found <code>maxAllowed</code> couldn't be configured centrally in the VPA controller, so we contributed this feature upstream (see <a href=\\"https://github.com/kubernetes/autoscaler/issues/7147\\" target=\\"_blank\\" rel=\\"noreferrer\\">Kubernetes Autoscaler Issue #7147</a> and <a href=\\"https://github.com/kubernetes/autoscaler/pull/7560\\" target=\\"_blank\\" rel=\\"noreferrer\\">corresponding PR</a>).</li>\\n</ul>\\n</li>\\n<li><strong>Set Pod Requests:</strong> We always set CPU and memory requests for our containers or let VPA manage those.</li>\\n<li><strong>Tune Pod Requests:</strong> We systematically processed hundreds of components:\\n<ul>\\n<li>Some deployments were placed under VPA management. Others (very small, below VPA's resolution of ~10m cores / 10Mi memory) were removed from VPA and given static requests.</li>\\n<li><strong>&quot;Initial&quot; Requests:</strong> For pods managed by VPA, we set initial requests to the observed P5 (5th percentile) of historical usage. This provides a reasonable starting point for VPA.</li>\\n<li><strong>&quot;Static&quot; Requests:</strong> For pods not managed by VPA, we set requests to the P95 (95th percentile). This ensures they generally have enough resources; only exceptional spikes might cause issues, where VPA wouldn't typically help either.</li>\\n</ul>\\n</li>\\n<li><strong>Quality of Service (QoS):</strong> Prefer the <code>Burstable</code> QoS class (requests set, ideally no limits) for most workloads. Avoid <code>BestEffort</code> (no requests/limits), as these pods are the first to be evicted under pressure. Avoid <code>Guaranteed</code> (requests match limits), as limits often cause more harm than good. See our <a href=\\"/docs/guides/applications/shoot-pod-autoscaling-best-practices/#quality-of-service-qos\\">Pod Autoscaling Best Practices Guide</a>. Pods in the <code>Guaranteed</code> QoS class, or generally those with limits, will be actively CPU-throttled and can be OOMKilled even if the node has ample spare capacity. Worse, if containers in the pod are under VPA, their CPU requests/limits often won't scale up appropriately because CPU throttling goes unnoticed by VPA.\\n<ul>\\n<li><strong>Avoid Limits:</strong> In Gardener's context (and often also elsewhere), setting CPU limits offers few advantages and significant disadvantages, primarily unnecessary throttling. Setting memory limits <em>can</em> prevent runaway processes but may also prematurely kill pods. We generally avoid setting limits unless the theoretical maximum resource consumption of a component is well understood. When unsure, let VPA manage requests and rely on monitoring/alerting for excessive usage.</li>\\n</ul>\\n</li>\\n</ul>\\n<h2 id=\\"data-driven-machine-type-selection\\" tabindex=\\"-1\\">Data-Driven Machine Type Selection <a class=\\"header-anchor\\" href=\\"#data-driven-machine-type-selection\\" aria-label=\\"Permalink to &quot;Data-Driven Machine Type Selection&quot;\\">&ZeroWidthSpace;</a></h2>\\n<h3 id=\\"continuous-monitoring-understanding-how-well-our-machines-are-utilized\\" tabindex=\\"-1\\">Continuous Monitoring: Understanding How Well Our Machines are Utilized <a class=\\"header-anchor\\" href=\\"#continuous-monitoring-understanding-how-well-our-machines-are-utilized\\" aria-label=\\"Permalink to &quot;Continuous Monitoring: Understanding How Well Our Machines are Utilized&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>Before optimizing machine type selection, we established comprehensive machine utilization monitoring. This was important during individual improvement steps to validate their effectiveness. We collect key metrics per Gardener installation, cloud provider, seed, and worker pool, and created dashboards to visualize and monitor our machine costs. These dashboards include:</p>\\n<ul>\\n<li>Total CPU [in thousand cores], Total Memory [in TB], Total Number of Control Planes [count]</li>\\n<li>Used Capacity CPU [%], Used Capacity Memory [%], Unused vs. Capacity Cost [Currency]</li>\\n<li>Requested Allocatable CPU [%], Requested Allocatable Memory [%], Unrequested vs. Allocatable Cost [Currency]</li>\\n<li>Used Requested CPU [%], Used Requested Memory [%], Unused vs. Requested Cost [Currency]</li>\\n<li>Used Reserved CPU [%, can exceed 100%], Used Reserved Memory [%, can exceed 100%], Unused vs. Reserved Cost [Currency]</li>\\n<li>Nodes with &gt;99% filling levels, broken down by CPU, memory, volumes, and pods (to identify the most critical resource blocking further usage)</li>\\n<li>Effective CPU:memory ratio of the workload (more on that later)</li>\\n</ul>\\n<h3 id=\\"why-machine-types-matter-size-ratios-generations-and-hidden-constraints\\" tabindex=\\"-1\\">Why Machine Types Matter: Size, Ratios, Generations, and Hidden Constraints <a class=\\"header-anchor\\" href=\\"#why-machine-types-matter-size-ratios-generations-and-hidden-constraints\\" aria-label=\\"Permalink to &quot;Why Machine Types Matter: Size, Ratios, Generations, and Hidden Constraints&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>Selecting the right machine type is critical for cost efficiency. Several factors come into play:</p>\\n<ul>\\n<li><strong>Size:</strong> Larger machines generally lead to less fragmentation (less wasted CPU/memory remainder per node) and better overhead efficiency (system components like kubelet/containerd consume a smaller percentage of total resources). However, smaller machines can be better for low-load scenarios while meeting high-availability constraints (e.g., needing to spread critical pods across 3 zones requires at least 3 nodes).</li>\\n<li><strong>CPU:Memory Ratio:</strong> Cloud providers offer instance families with different CPU:memory ratios (e.g., high-cpu 1:2, standard 1:4, high-memory 1:8). Matching the instance ratio to your workload's aggregate CPU:memory request ratio minimizes waste.</li>\\n<li><strong>Generations:</strong> Newer instance generations usually offer better performance and, crucially, better price-performance. This can also shift the effective CPU:memory ratio required by the workload due to performance differences.</li>\\n<li><strong>Hidden Constraints: Volume Limits:</strong> This proved to be a <em>major</em> factor, especially on AWS and Azure. Each instance type has a maximum number of network-attached volumes it can support. Gardener control planes, each with its own etcd cluster requiring persistent volumes for each replica, are heavily impacted. We often found ourselves limited by volume attachments long before hitting CPU or memory limits. Interestingly, ARM-based instance types on Azure support a slightly higher volume limit.</li>\\n</ul>\\n<h3 id=\\"the-case-for-dedicated-pools-isolating-workloads\\" tabindex=\\"-1\\">The Case for Dedicated Pools: Isolating Workloads <a class=\\"header-anchor\\" href=\\"#the-case-for-dedicated-pools-isolating-workloads\\" aria-label=\\"Permalink to &quot;The Case for Dedicated Pools: Isolating Workloads&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>While mixing diverse workloads seems efficient at first glance, dedicated node pools for specific workload types proved beneficial for several reasons:</p>\\n<ul>\\n<li><strong>Handling <a href=\\"https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-types-of-pods-can-prevent-ca-from-removing-a-node\\" target=\\"_blank\\" rel=\\"noreferrer\\"><code>safe-to-evict: false</code></a>:</strong> Some pods (like single-replica stateful components for non-HA clusters) cannot be safely evicted by the Cluster Autoscaler. Mixing these with evictable pods on the same node can prevent the CA from scaling down that node, even if it's underutilized, negating cost savings. Placing these non-evictable pods in a dedicated pool (where scale-down might be disabled or carefully managed) isolates this behavior.</li>\\n<li><strong>Volume Concentration:</strong> Our &quot;etcd&quot; worker pools host primarily etcd pods (high volume count) and daemonsets, while &quot;standard&quot; pools host API servers, controllers, etc. (lower volume concentration). This difference influences the optimal machine type due to volume attachment limits.</li>\\n<li><strong>Preventing Scheduling Traps:</strong> Ensure critical, long-running pods (like Istio gateways) have node affinities/selectors to land only on their preferred, optimized node pools. Avoid them landing on temporary, large nodes spun up for short-lived bulky pods; if such a pod prevents the large node from scaling down (e.g., because the pool is at its minimum node count), the CA won't automatically replace the underutilized large node with a smaller one. That's a concept called &quot;workload consolidation&quot;, today only supported by <a href=\\"https://github.com/kubernetes-sigs/karpenter\\" target=\\"_blank\\" rel=\\"noreferrer\\">Karpenter</a>, which isn't supporting as many cloud providers as CA.</li>\\n</ul>\\n<h3 id=\\"analyzing-workload-profiles-finding-the-optimal-instance-size-and-family\\" tabindex=\\"-1\\">Analyzing Workload Profiles: Finding the Optimal Instance Size and Family <a class=\\"header-anchor\\" href=\\"#analyzing-workload-profiles-finding-the-optimal-instance-size-and-family\\" aria-label=\\"Permalink to &quot;Analyzing Workload Profiles: Finding the Optimal Instance Size and Family&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>Early on, we used a guide for operators to estimate a reasonable machine size for a seed cluster based on the number of hosted control planes, e.g.:</p>\\n<p>| Optimal<br/>Worker Pool (CPUxMem+Vols) | Very Low Seed Utilization<br/>0 &lt;= &vert;control planes&vert; &lt; 15 | Low Seed Utilization<br/>5 &lt;= &vert;control planes&vert; &lt; 30 | Medium Seed Utilization<br/>10 &lt;= &vert;control planes&vert; &lt; 70 | High Seed Utilization<br/>30 &lt;= &vert;control planes&vert; &lt; 180 | Very High Seed Utilization<br/>120 &lt;= &vert;control planes&vert; &lt; &infin; |\\n|</p>\\n","date":{"time":1744891200000,"string":"April 17, 2025"}},{"title":"Gardener at KubeCon + CloudNativeCon Europe, London 2025","url":"/blog/2025/03/03-18-Gardener-KubeCon-CloudNativeCon-Europe-2025-Announcement","excerpt":"","date":{"time":1742299200000,"string":"March 18, 2025"}},{"title":"Unleashing Potential: Highlights from the 6th Gardener Community Hackathon","url":"/blog/2024/12-08-Unleashing-Potential-Highlights-from-the-6th-Gardener-Community-Hackathon","excerpt":"","date":{"time":1733659200000,"string":"December 8, 2024"}},{"title":"Introducing the New Gardener Demo Environment: Your Hands-On Playground for Kubernetes Management","url":"/blog/2024/11-09-Demo","excerpt":"","date":{"time":1731153600000,"string":"November 9, 2024"}},{"title":"PromCon EU 2024 Highlights","url":"/blog/2024/11-06-PromCon-EU-2024","excerpt":"","date":{"time":1730462400000,"string":"November 1, 2024"}},{"title":"Gardener at KubeCon + CloudNativeCon North America 2024","url":"/blog/2024/10-24-Gardener-KubeCon-CloudNativeCon-NA-2024-Announcement","excerpt":"","date":{"time":1729771200000,"string":"October 24, 2024"}},{"title":"Innovation Unleashed: A Deep Dive into the 5th Gardener Community Hackathon","url":"/blog/2024/05-21-Innovation-Unleashed-A-Deep-Dive-into-the-5th-Gardener-Community-Hackathon","excerpt":"","date":{"time":1716292800000,"string":"May 21, 2024"}},{"title":"Gardener's Registry Cache Extension: Another Cost Saving Win and More","url":"/blog/2024/04-22-Gardener's-Registry-Cache-Extension-Another-Cost-Saving-Win-and-More","excerpt":"","date":{"time":1713787200000,"string":"April 22, 2024"}},{"title":"SpinKube on Gardener - Serverless WASM on Kubernetes","url":"/blog/2024/04-18-SpinKube-Gardener-Shoot-Cluster","excerpt":"","date":{"time":1713441600000,"string":"April 18, 2024"}},{"title":"KubeCon / CloudNativeCon Europe 2024 Highlights","url":"/blog/2024/04-05-KubeCon-CloudNativeCon-Europe-2024-Highlights","excerpt":"","date":{"time":1712318400000,"string":"April 5, 2024"}},{"title":"High Availability and Zone Outage Toleration","url":"/blog/2023/03-27-High-Availability-and-Zone-Outage-Toleration","excerpt":"","date":{"time":1679918400000,"string":"March 27, 2023"}},{"title":"Community Call - Get more computing power in Gardener by overcoming Kubelet limitations with CRI-resource-manager","url":"/blog/2022/10.20-Gardener-Community-Meeting-October-2","excerpt":"","date":{"time":1666267200000,"string":"October 20, 2022"}},{"title":"Community Call - Cilium / Isovalent Presentation","url":"/blog/2022/10.06-Gardener-Community-Meeting-October","excerpt":"","date":{"time":1665057600000,"string":"October 6, 2022"}},{"title":"Community Call - Gardener Extension Development","url":"/blog/2022/06.17-Gardener-Community-Meeting-June","excerpt":"","date":{"time":1655467200000,"string":"June 17, 2022"}},{"title":"Community Call - Deploying and Developing Gardener Locally","url":"/blog/2022/03.23-Gardener-Community-Meeting-March","excerpt":"","date":{"time":1648036800000,"string":"March 23, 2022"}},{"title":"Community Call - Gardenctl-v2","url":"/blog/2022/02.17-Gardener-Community-Meeting-February","excerpt":"","date":{"time":1645099200000,"string":"February 17, 2022"}},{"title":"Navigating Cloud-Native Security - Lessons from a Recent Container Service Vulnerability","url":"/blog/2021/09.12-Navigating-Cloud-Native-Security","excerpt":"","date":{"time":1631448000000,"string":"September 12, 2021"}},{"title":"Happy Anniversary, Gardener! Three Years of Open Source Kubernetes Management","url":"/blog/2021/02.01-Happy-anniversary-Gardener","excerpt":"","date":{"time":1612180800000,"string":"February 1, 2021"}},{"title":"Machine Controller Manager","url":"/blog/2021/01.25-Machine-Controller-Manager","excerpt":"","date":{"time":1611576000000,"string":"January 25, 2021"}},{"title":"STACKIT Kubernetes Engine with Gardener","url":"/blog/2020/12.03-STACKIT-Kubernetes-Engine-with-Gardener","excerpt":"","date":{"time":1606996800000,"string":"December 3, 2020"}},{"title":"Gardener v1.13 Released","url":"/blog/2020/11.23-Gardener-v1.13-Released","excerpt":"","date":{"time":1606132800000,"string":"November 23, 2020"}},{"title":"Case Study: Migrating ETCD Volumes in Production","url":"/blog/2020/11.20-Case-Study-Migrating-ETCD-Volumes-in-Production","excerpt":"","date":{"time":1605873600000,"string":"November 20, 2020"}},{"title":"Gardener v1.11 and v1.12 Released","url":"/blog/2020/11.04-Gardener-v1.11-and-v1.12-Released","excerpt":"","date":{"time":1604491200000,"string":"November 4, 2020"}},{"title":"Gardener Integrates with KubeVirt","url":"/blog/2020/10.19-Gardener-Integrates-with-KubeVirt","excerpt":"","date":{"time":1603108800000,"string":"October 19, 2020"}},{"title":"Shoot Reconciliation Details","url":"/blog/2020/10.19-Shoot-Reconciliation-Details","excerpt":"<h1 id=\\"shoot-reconciliation-details\\" tabindex=\\"-1\\">Shoot Reconciliation Details <a class=\\"header-anchor\\" href=\\"#shoot-reconciliation-details\\" aria-label=\\"Permalink to &quot;Shoot Reconciliation Details&quot;\\">&ZeroWidthSpace;</a></h1>\\n<p>Do you want to understand how Gardener creates and updates Kubernetes clusters (Shoots)?\\nWell, it's complicated, but if you are not afraid of large diagrams and are a visual learner like me, this might be useful to you.</p>\\n<h2 id=\\"introduction\\" tabindex=\\"-1\\">Introduction <a class=\\"header-anchor\\" href=\\"#introduction\\" aria-label=\\"Permalink to &quot;Introduction&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>In this blog post I will share a technical diagram which attempts to tie together the various components involved when Gardener creates a Kubernetes cluster.\\nI have created and curated the diagram, which visualizes the Shoot reconciliation flow since I started developing on Gardener.\\nAside from serving as a memory aid for myself, I created it in hopes that it may potentially help contributors to understand a core piece of the complex Gardener machinery.\\nPlease be advised that the diagram and components involved are large.\\nAlthough it can be easily divided into multiple diagrams, I want to show all the components and connections in a single diagram to create an overview of the reconciliation flow.</p>\\n<p>The goal is to visualize the interactions of the components involved in the Shoot creation.\\nIt is not intended to serve as a documentation of every component involved.</p>\\n<h2 id=\\"background\\" tabindex=\\"-1\\">Background <a class=\\"header-anchor\\" href=\\"#background\\" aria-label=\\"Permalink to &quot;Background&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>Taking a step back, the Gardener <a href=\\"https://github.com/gardener/gardener/blob/master/README.md\\" target=\\"_blank\\" rel=\\"noreferrer\\">README</a> states:</p>\\n<blockquote>\\n<p>In essence, Gardener is an <a href=\\"https://kubernetes.io/docs/tasks/access-kubernetes-api/setup-extension-api-server/\\" target=\\"_blank\\" rel=\\"noreferrer\\">extension API server</a>\\nthat comes along with a bundle of custom controllers.\\nIt introduces new API objects in an existing Kubernetes cluster (which is called a <strong>garden</strong> cluster) in order to use them for the\\nmanagement of end-user Kubernetes clusters (which are called <strong>shoot</strong> clusters).\\nThese shoot clusters are described via <a href=\\"https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml\\" target=\\"_blank\\" rel=\\"noreferrer\\">declarative cluster specifications</a> which are observed by the controllers.\\nThey will bring up the clusters, reconcile their state, perform automated updates and make sure they are always up and running.</p>\\n</blockquote>\\n<p>This means that Gardener, just like any Kubernetes controller, creates Kubernetes clusters (Shoots) using a reconciliation loop.</p>\\n<p>The <a href=\\"/docs/gardener/concepts/gardenlet/\\">Gardenlet</a> contains the controller and reconciliation loop responsible for the creation, update, deletion, and migration of Shoot clusters (there are more, but we spare them in this article).\\nIn addition, the <a href=\\"/docs/gardener/concepts/controller-manager/\\">Gardener Controller Manager</a> also reconciles Shoot resources, but only for seed-independent functionality such as Shoot hibernation, Shoot maintenance or quota control.</p>\\n<p>This blog post is about the reconciliation loop in the Gardenlet responsible for creating and updating Shoot clusters.\\nThe code can be found in the <a href=\\"https://github.com/gardener/gardener/blob/master/pkg/gardenlet/controller/shoot/shoot/reconciler_reconcile.go\\" target=\\"_blank\\" rel=\\"noreferrer\\">gardener/gardener repository</a>.\\nThe reconciliation loops of the extension controllers can be found in their individual repositories.</p>\\n<h2 id=\\"shoot-reconciliation-flow-diagram\\" tabindex=\\"-1\\">Shoot Reconciliation Flow Diagram <a class=\\"header-anchor\\" href=\\"#shoot-reconciliation-flow-diagram\\" aria-label=\\"Permalink to &quot;Shoot Reconciliation Flow Diagram&quot;\\">&ZeroWidthSpace;</a></h2>\\n<p>When Gardner creates a Shoot cluster, there are three conceptual layers involved: the Garden cluster, the Seed cluster and the Shoot cluster.\\nEach layer represents a top-level section in the diagram (similar to a lane in a BPMN diagram).</p>\\n<p>It might seem confusing that the Shoot cluster itself is a layer, because the whole flow in the first place is about creating the Shoot cluster.\\nI decided to introduce this separate layer to make a clear distinction between which resources exist in the Seed API server (managed by Gardener) and which in the Shoot API server (accessible by the Shoot owner).</p>\\n<p>Each section contains several components.\\nComponents are mostly Kubernetes resources in a Gardener installation (e.g. the gardenlet deployment in the Seed cluster).</p>\\n<p>This is the list of components:</p>\\n<p><strong>(Virtual) Garden Cluster</strong></p>\\n<ul>\\n<li>Gardener Extension API server</li>\\n<li>Validating Provider Webhooks</li>\\n<li>Project Namespace</li>\\n</ul>\\n<p><strong>Seed Cluster</strong></p>\\n<ul>\\n<li>Gardenlet</li>\\n<li>Seed API server\\n<ul>\\n<li>every Shoot Control Plane has a dedicated namespace in the Seed.</li>\\n</ul>\\n</li>\\n<li>Cloud Provider (owned by Stakeholder)\\n<ul>\\n<li>Arguably part of the Shoot cluster but used by components in the Seed cluster to create the infrastructure for the Shoot.</li>\\n</ul>\\n</li>\\n<li><a href=\\"https://github.com/gardener/external-dns-management\\" target=\\"_blank\\" rel=\\"noreferrer\\">Gardener DNS extension</a></li>\\n<li>Provider Extension (such as <a href=\\"https://github.com/gardener/gardener-extension-provider-aws\\" target=\\"_blank\\" rel=\\"noreferrer\\">gardener-extension-provider-aws</a>)</li>\\n<li><a href=\\"https://github.com/gardener/etcd-druid\\" target=\\"_blank\\" rel=\\"noreferrer\\">Gardener Extension ETCD Druid</a></li>\\n<li><a href=\\"https://github.com/gardener/gardener-resource-manager\\" target=\\"_blank\\" rel=\\"noreferrer\\">Gardener Resource Manager</a></li>\\n<li>Operating System Extension (such as <a href=\\"https://github.com/gardener/gardener-extension-os-gardenlinux\\" target=\\"_blank\\" rel=\\"noreferrer\\">gardener-extension-os-gardenlinux</a>)</li>\\n<li>Networking Extension (such as <a href=\\"https://github.com/gardener/gardener-extension-networking-cilium\\" target=\\"_blank\\" rel=\\"noreferrer\\">gardener-extension-networking-cilium</a>)</li>\\n<li><a href=\\"https://github.com/gardener/machine-controller-manager\\" target=\\"_blank\\" rel=\\"noreferrer\\">Machine Controller Manager</a></li>\\n<li>ContainerRuntime extension (such as <a href=\\"https://github.com/gardener/gardener-extension-runtime-gvisor\\" target=\\"_blank\\" rel=\\"noreferrer\\">gardener-extension-runtime-gvisor</a>)</li>\\n<li>Shoot API server (in the Shoot Namespace in the Seed cluster)</li>\\n</ul>\\n<p><strong>Shoot Cluster</strong></p>\\n<ul>\\n<li>Cloud Provider Compute API (owned by Stakeholder) - for VM/Node creation.</li>\\n<li>VM / Bare metal node hosted by Cloud Provider (in Stakeholder owned account).</li>\\n</ul>\\n<h3 id=\\"how-to-use-the-diagram\\" tabindex=\\"-1\\">How to Use the Diagram <a class=\\"header-anchor\\" href=\\"#how-to-use-the-diagram\\" aria-label=\\"Permalink to &quot;How to Use the Diagram&quot;\\">&ZeroWidthSpace;</a></h3>\\n<p>The diagram:</p>\\n<ul>\\n<li>should be read from top to bottom - starting in the top left corner with the creation of the Shoot resource via the Gardener Extension API server.</li>\\n<li>should not require an encompassing documentation / description.\\nMore detailed documentation on the components itself can usually be found in the respective repository.</li>\\n<li>does not show which activities execute in parallel (many) and also does not describe the exact dependencies between the steps.\\nThis can be found out by <a href=\\"https://github.com/gardener/gardener/blob/master/pkg/gardenlet/controller/shoot/shoot/reconciler_reconcile.go\\" target=\\"_blank\\" rel=\\"noreferrer\\">looking at the source code</a>.\\nIt however tries to put the activities in a logical order of execution during the reconciliation flow.</li>\\n</ul>\\n<p>Occasionally, there is an info box with additional information next to parts in the diagram that in my point of view require further explanation.\\nLarge example resource for the Gardener CRDs (e.g Worker CRD, Infrastructure CRD) are placed on the left side and are referenced by a dotted line (</p>\\n","date":{"time":1603108800000,"string":"October 19, 2020"}},{"title":"Gardener v1.9 and v1.10 Released","url":"/blog/2020/09.11-Gardener-v1.9-and-v1.10-Released","excerpt":"","date":{"time":1599825600000,"string":"September 11, 2020"}},{"title":"Gardener v1.8.0 Released","url":"/blog/2020/08.06-Gardener-v1.8.0-Released","excerpt":"","date":{"time":1596715200000,"string":"August 6, 2020"}},{"title":"New Website, Same Green Flower","url":"/blog/2020/05.11-New-Website-Same-Green-Flower","excerpt":"","date":{"time":1589198400000,"string":"May 11, 2020"}},{"title":"Feature Flags in Kubernetes Applications","url":"/blog/2019/06.11-Feature-Flags-in-Kubernetes-Applications","excerpt":"","date":{"time":1560254400000,"string":"June 11, 2019"}},{"title":"Organizing Access Using kubeconfig Files","url":"/blog/2019/06.11-Organizing-Access-Using-kubeconfig-Files","excerpt":"","date":{"time":1560254400000,"string":"June 11, 2019"}},{"title":"KubeCon Rewind: SIG Cluster API & Gardener  Managing Machines Automatically","url":"/blog/2019/05.24-Cluster-API-Machine-Abstractions-KubeCon-Talk","excerpt":"","date":{"time":1558699200000,"string":"May 24, 2019"}},{"title":"Gardener Cookies","url":"/blog/2018/12.25-Gardener_Cookies","excerpt":"","date":{"time":1545739200000,"string":"December 25, 2018"}},{"title":"Cookies Are Dangerous...","url":"/blog/2018/12.22-Cookies-are-dangerous","excerpt":"","date":{"time":1545480000000,"string":"December 22, 2018"}},{"title":"Hibernate a Cluster to Save Money","url":"/blog/2018/07.11-Hibernate-a-Cluster-to-save-money","excerpt":"","date":{"time":1531310400000,"string":"July 11, 2018"}},{"title":"Anti Patterns","url":"/blog/2018/06.11-Anti-Patterns","excerpt":"","date":{"time":1528718400000,"string":"June 11, 2018"}},{"title":"Auditing Kubernetes for Secure Setup","url":"/blog/2018/06.11-Auditing-Kubernetes-for-Secure-Setup","excerpt":"","date":{"time":1528718400000,"string":"June 11, 2018"}},{"title":"Big Things Come in Small Packages","url":"/blog/2018/06.11-Big-things-come-in-small-packages","excerpt":"","date":{"time":1528718400000,"string":"June 11, 2018"}},{"title":"Hardening the Gardener Community Setup","url":"/blog/2018/06.11-Hardening-the-Gardener-Community-Setup","excerpt":"","date":{"time":1528718400000,"string":"June 11, 2018"}},{"title":"Kubernetes is Available in Docker for Mac 17.12 CE","url":"/blog/2018/06.11-Kubernetes-is-available-in-Docker-for-Mac-17-12-CE","excerpt":"","date":{"time":1528718400000,"string":"June 11, 2018"}},{"title":"Namespace Isolation","url":"/blog/2018/06.11-Namespace-Isolation","excerpt":"","date":{"time":1528718400000,"string":"June 11, 2018"}},{"title":"Namespace Scope","url":"/blog/2018/06.11-Namespace-Scope","excerpt":"","date":{"time":1528718400000,"string":"June 11, 2018"}},{"title":"ReadWriteMany - Dynamically Provisioned Persistent Volumes Using Amazon EFS","url":"/blog/2018/06.11-ReadWriteMany-Dynamically-Provisioned-Persistent-Volumes-Using-Amazon-EFS","excerpt":"","date":{"time":1528718400000,"string":"June 11, 2018"}},{"title":"Shared Storage with S3 Backend","url":"/blog/2018/06.11-Shared-storage-with-S3-backend","excerpt":"","date":{"time":1528718400000,"string":"June 11, 2018"}},{"title":"Watching Logs of Several Pods","url":"/blog/2018/06.11-Watching-logs-of-several-pods","excerpt":"","date":{"time":1528718400000,"string":"June 11, 2018"}}]`),f={class:"blog-list"},b={class:"blog-entry"},E={key:0},v=["datetime"],F={class:"title"},w=["href"],C=h({__name:"BlogIndex",setup(l){function t(e){return new Date(e).toISOString()}return(e,S)=>(n(),s("ul",f,[(n(!0),s(d,null,p(o(y),i=>(n(),s("li",b,[i.title!=="Overview"?(n(),s("article",E,[a("time",{datetime:t(i.date.time)},r(i.date.string),9,v),a("h2",F,[a("a",{href:o(g)(i.url)},r(i.title),9,w)])])):c("",!0)]))),256))]))}}),A=u(C,[["__scopeId","data-v-b7268d0b"]]),x=JSON.parse('{"title":"Blogs","description":"","frontmatter":{"aside":false,"editLink":false,"github_repo":"https://github.com/gardener/documentation","github_subdir":"website/blog","outline":false,"params":{"github_branch":"master"},"path_base_for_github_subdir":{"from":"content/blog/index.md","to":"index.md"},"title":"Blogs"},"headers":[],"relativePath":"blog/index.md","filePath":"blog/index.md","lastUpdated":null}'),P={name:"blog/index.md"},I=Object.assign(P,{setup(l){return(t,e)=>(n(),s("div",null,[e[0]||(e[0]=k('<h1 id="blogs" tabindex="-1">Blogs <a class="header-anchor" href="#blogs" aria-label="Permalink to &quot;Blogs&quot;"></a></h1><h2 id="overview" tabindex="-1">Overview <a class="header-anchor" href="#overview" aria-label="Permalink to &quot;Overview&quot;"></a></h2><p>Here you can find a variety of articles related to Gardener and keep up to date with the latest community calls, features, and highlights!</p><h2 id="how-to-contribute" tabindex="-1">How to Contribute <a class="header-anchor" href="#how-to-contribute" aria-label="Permalink to &quot;How to Contribute&quot;"></a></h2><p>If you&#39;d like to create a new blog post, simply follow the steps outlined in the <a href="/docs/contribute/documentation/">Documentation Contribution Guide</a> and add the topic to the <a href="https://github.com/gardener/documentation/tree/master/website/blog" target="_blank" rel="noreferrer">corresponding folder</a>.</p><h2 id="posts" tabindex="-1">Posts <a class="header-anchor" href="#posts" aria-label="Permalink to &quot;Posts&quot;"></a></h2>',6)),m(A)]))}});export{x as __pageData,I as default};
