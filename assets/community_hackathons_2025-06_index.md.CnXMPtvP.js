import{_ as t,c as r,o,a2 as a}from"./chunks/framework.Bfq10Vlj.js";const n="/assets/2025-06.Bll2lPkr.png",s="/assets/otel-flow.BmzEehcj.png",i="/assets/prometheus-chart.D-gHNd9B.png",l="/assets/prometheus-traffic-chart.-Z2odiMc.png",b=JSON.parse('{"title":"June 2025","description":"","frontmatter":{"github_repo":"https://github.com/gardener/documentation","github_subdir":"website/community/hackathons","params":{"github_branch":"master"},"path_base_for_github_subdir":{"from":"content/community/hackathons/2025-06.md","to":"2025-06.md"},"title":"June 2025","weight":-202506,"prev":false,"next":false},"headers":[],"relativePath":"community/hackathons/2025-06/index.md","filePath":"community/hackathons/2025-06.md","lastUpdated":null}'),c={name:"community/hackathons/2025-06/index.md"};function d(h,e,p,g,u,m){return o(),r("div",null,e[0]||(e[0]=[a('<h1 id="hack-the-garden-06-2025-wrap-up" tabindex="-1">Hack The Garden 06/2025 Wrap Up <a class="header-anchor" href="#hack-the-garden-06-2025-wrap-up" aria-label="Permalink to &quot;Hack The Garden 06/2025 Wrap Up&quot;">â€‹</a></h1><ul><li>ğŸ—“ï¸ <strong>Date:</strong> 02.06.2025 â€“ 06.06.2025</li><li>ğŸ“ <strong>Location:</strong> <a href="https://www.schlosshof-info.de/" target="_blank" rel="noreferrer">Schlosshof Freizeitheim, Schelklingen</a></li><li>ğŸ‘¤ <strong>Organizer:</strong> <a href="https://www.x-cellent.com/" target="_blank" rel="noreferrer">x-cellent</a></li><li>ğŸ“˜ <strong>Topics:</strong> <a href="https://hackmd.io/ugrWYptbRAi9NPi78G-B9g" target="_blank" rel="noreferrer">https://hackmd.io/ugrWYptbRAi9NPi78G-B9g</a></li><li>ğŸ¤ <strong>Review Meeting Summary:</strong> <a href="https://gardener.cloud/community/review-meetings/2025-reviews/#_2025-06-11-hack-the-garden-wrap-up" target="_blank" rel="noreferrer">https://gardener.cloud/community/review-meetings/2025-reviews/#_2025-06-11-hack-the-garden-wrap-up</a></li></ul><p><img src="'+n+'" alt="Group picture"></p><h2 id="âš¡ï¸-replace-openvpn-with-wireguard" tabindex="-1">âš¡ï¸ Replace OpenVPN with Wireguard <a class="header-anchor" href="#âš¡ï¸-replace-openvpn-with-wireguard" aria-label="Permalink to &quot;âš¡ï¸ Replace OpenVPN with Wireguard&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> The Gardener VPN implementation between control and data plane currently uses OpenVPN, which is a well-established but somewhat old solution for VPNs. Wireguard is a relatively new, but well-liked contender in the VPN space. It could be possible to replace OpenVPN with Wireguard. As we do not want to spin up a load balancer per control plane (or use one port per control plane) a reverse proxy like <a href="https://github.com/apernet/mwgp" target="_blank" rel="noreferrer">mwgp</a> is required.</p><p><strong>Motivation/Benefits:</strong> ğŸš€ Modernize VPN stack, âš¡ï¸ improved performance and simplicity.</p><p><strong>Achievements:</strong> We have a POC for VPN via Wireguard connection for one shoot with local setup and the local setup for extensions. There is a <a href="https://github.com/axel7born/vpn2/blob/wireguard/docs/wireguard.md" target="_blank" rel="noreferrer">document</a> describing the approach and the deployment.</p><p><strong>Next Steps:</strong></p><ul><li>Test network connection: resilience, downtime during new deployment, throughput...</li><li>Aggregate the secrets from all shoots to generate a unified MWGP configuration.</li><li>Currently, MWPG needs to be restarted to get the new configuration. This would have to be done for each new shoot. Check if this is a problem and if it can be avoided.</li><li>Check if the functionality of MWGP can be implemented as Istio plugin.</li><li>Refactor and finalize the implementation to prepare PRs.</li></ul><p><strong>Code/Pull Requests:</strong></p><ul><li><a href="https://github.com/axel7born/gardener/tree/wireguard" target="_blank" rel="noreferrer">https://github.com/axel7born/gardener/tree/wireguard</a></li><li><a href="https://github.com/axel7born/vpn2/tree/wireguard" target="_blank" rel="noreferrer">https://github.com/axel7born/vpn2/tree/wireguard</a></li><li><a href="https://github.com/majst01/mwgp" target="_blank" rel="noreferrer">https://github.com/majst01/mwgp</a></li></ul><h2 id="â›³ï¸-make-gardener-operator-single-node-ready" tabindex="-1">â›³ï¸ Make <code>gardener-operator</code> Single-Node Ready <a class="header-anchor" href="#â›³ï¸-make-gardener-operator-single-node-ready" aria-label="Permalink to &quot;â›³ï¸ Make `gardener-operator` Single-Node Ready&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> By default, when Gardener is deployed, some components of it are deployed for high availability, assuming multiple nodes in the cluster. This is not necessary or hinders the deployment of Gardener in single-node clusters. For bare-metal scenarios, sometimes only a single node is available, meaning e.g. multiple replicas of some components are not needed.</p><p><strong>Motivation/Benefits:</strong> ğŸ§© Enable lightweight/single-node deployments, ğŸ› ï¸ reduce resource overhead.</p><p><strong>Achievements:</strong> In addition to several components being made single-node ready, the Prometheus deployments were made configurable via the component configuration.</p><p><strong>Next Steps:</strong> It needs to be evaluated if the steps taken are sufficient to be used in a real world scenario.</p><p><strong>Code/Pull Requests:</strong></p><ul><li><a href="https://github.com/gardener/gardener-extension-provider-gcp/pull/1052" target="_blank" rel="noreferrer">https://github.com/gardener/gardener-extension-provider-gcp/pull/1052</a></li><li><a href="https://github.com/gardener/gardener-extension-provider-openstack/pull/1042" target="_blank" rel="noreferrer">https://github.com/gardener/gardener-extension-provider-openstack/pull/1042</a></li><li><a href="https://github.com/fluent/fluent-operator/pull/1616" target="_blank" rel="noreferrer">https://github.com/fluent/fluent-operator/pull/1616</a></li><li><a href="https://github.com/afritzler/cortex/commit/4000c188086fe383d314efeb40a663f49aa8b35b" target="_blank" rel="noreferrer">https://github.com/afritzler/cortex/commit/4000c188086fe383d314efeb40a663f49aa8b35b</a></li><li><a href="https://github.com/afritzler/cortex/commit/616c0b80d90d036f4275636e1a3be9c5f2aac9e5" target="_blank" rel="noreferrer">https://github.com/afritzler/cortex/commit/616c0b80d90d036f4275636e1a3be9c5f2aac9e5</a></li><li><a href="https://github.com/afritzler/vali/commit/35fbce152f783f33fdc2066e09d60ec2ba56b562" target="_blank" rel="noreferrer">https://github.com/afritzler/vali/commit/35fbce152f783f33fdc2066e09d60ec2ba56b562</a></li><li><a href="https://github.com/afritzler?tab=packages&amp;repo_name=vali" target="_blank" rel="noreferrer">https://github.com/afritzler?tab=packages&amp;repo_name=vali</a></li><li><a href="https://github.com/gardener/gardener/pull/12248" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/pull/12248</a></li></ul><h2 id="ğŸ“¡-opentelemetry-transport-for-shoot-metrics" tabindex="-1">ğŸ“¡ OpenTelemetry Transport for <code>Shoot</code> Metrics <a class="header-anchor" href="#ğŸ“¡-opentelemetry-transport-for-shoot-metrics" aria-label="Permalink to &quot;ğŸ“¡ OpenTelemetry Transport for `Shoot` Metrics&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> Today the shoot metrics are collected by the control plane Prometheus using the Kubernetes API server <code>/proxy</code> endpoint, without any ability to fine tune the collected metrics sets. Since we introduce OpenTelemetry collector instance on the shoots as a replacement of valitail service and on seeds in the shoot control plane namespace, the goal is to try out collecting and filtering the shoot metrics via OpenTelemetry collector instances also giving the opportunity for filtering and fine-tuning of the metrics sets. This story is part of <a href="https://github.com/gardener/gardener/blob/master/docs/proposals/34-observability2.0-opentelemtry-operator-and-collectors.md" target="_blank" rel="noreferrer">Observability 2.0 initiative</a>.</p><p><strong>Motivation/Benefits:</strong> ğŸ“Š Flexible and modern metrics collection, ğŸ” improved observability.</p><p><strong>Achievements:</strong> Collecting shoot metrics by an OpenTelemetry collector instance running on the shoot and transporting those to the Prometheus <a href="https://opentelemetry.io/docs/specs/otlp/" target="_blank" rel="noreferrer">OTLP</a> ingestion endpoint on seeds has been proved to be a viable scenario. The OpenTelemetry collector instance can be configured to filter and fine tune the metrics sets, allowing for more flexibility in the metrics collection.</p><p><strong>Next Steps:</strong> Introducing OpenTelemetry collector on shoot nodes is now part of the implementation plan for Observability 2.0. The community is invited to participate in this or any future initiative part of Gardener Observability 2.0 concept.</p><p><img src="'+s+'" alt="OpenTelemetry transport flow"></p><p><img src="'+i+'" alt="Prometheus chart using OpenTelemtry metrics"></p><h2 id="ğŸ”¬-cluster-network-observability" tabindex="-1">ğŸ”¬ Cluster Network Observability <a class="header-anchor" href="#ğŸ”¬-cluster-network-observability" aria-label="Permalink to &quot;ğŸ”¬ Cluster Network Observability&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> It might be beneficial to be able to get deeper insights into the traffic of a Kubernetes cluster. For example, traffic across availability zone boundaries may have increased latency or monetary costs. There are tools, e.g. <a href="https://github.com/microsoft/retina" target="_blank" rel="noreferrer">https://github.com/microsoft/retina</a>, which allow to gain more detailed insights into the pod network, but may lack some features like availability zone tracking (see <a href="https://github.com/microsoft/retina/issues/1179" target="_blank" rel="noreferrer">https://github.com/microsoft/retina/issues/1179</a>).</p><p><strong>Motivation/Benefits:</strong> ğŸ‘ï¸â€ğŸ—¨ï¸ Enhanced network visibility, ğŸ“ˆ actionable insights for optimization.</p><p><strong>Achievements:</strong> The retina tool was successfully enhanced to also label source and destination zones of the traffic. This allows to get insights into the traffic across availability zones, to potentially reduce costs.</p><p><strong>Next Steps:</strong> The PR for retina is still open and needs to be merged. After that, it can be discussed how to proceed with the integration into Gardener, e.g. in the shape of an extension.</p><p><strong>Issue:</strong><a href="https://github.com/microsoft/retina/issues/1654" target="_blank" rel="noreferrer">https://github.com/microsoft/retina/issues/1654</a></p><p><strong>Code/Pull Requests:</strong><a href="https://github.com/microsoft/retina/pull/1657" target="_blank" rel="noreferrer">https://github.com/microsoft/retina/pull/1657</a></p><p><img src="'+l+'" alt="Cluster Network Observability â€“ Prometheus Traffic Chart"></p><h2 id="ğŸ“-signing-of-managedresource-secrets" tabindex="-1">ğŸ“ Signing of <code>ManagedResource</code> Secrets <a class="header-anchor" href="#ğŸ“-signing-of-managedresource-secrets" aria-label="Permalink to &quot;ğŸ“ Signing of `ManagedResource` Secrets&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> The secrets of <code>ManagedResource</code>s, are currently used as-is by the Gardener Resource Manager (GRM). This could lead to a bad-actor manipulating these secrets to deploy resources with the permissions of the GRM. To prevent one potential scenario of privilege escalation, we want to sign the secrets of <code>ManagedResource</code>s with a key that is only known to the Gardener Resource Manager. This way, the grm can verify that the secrets it receives are not manipulated by a bad actor.</p><p><strong>Motivation/Benefits:</strong> ğŸ”’ Improved security and integrity for managed resources.</p><p><strong>Achievements:</strong> A PoC was created that demonstrates that signing of <code>ManagedResource</code> secrets is possible.</p><p><strong>Next Steps:</strong> There are still some open questions, as well as testing which needs to be followed up on.</p><p><strong>Code/Pull Requests:</strong><a href="https://github.com/gardener/gardener/pull/12247" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/pull/12247</a></p><h2 id="ğŸ§°-migrate-control-plane-reconciliation-of-provider-extensions-to-managedresources" tabindex="-1">ğŸ§° Migrate Control Plane Reconciliation of Provider Extensions to <code>ManagedResource</code>s <a class="header-anchor" href="#ğŸ§°-migrate-control-plane-reconciliation-of-provider-extensions-to-managedresources" aria-label="Permalink to &quot;ğŸ§° Migrate Control Plane Reconciliation of Provider Extensions to `ManagedResource`s&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> Currently we deploy control-plane components using the chart applier instead of managed-resources. This creates some issues where for example if we want to scale a component, we have to do it &quot;manually&quot;, e.g. scaling a controller to 0 needs to be done imperatively.</p><p><strong>Motivation/Benefits:</strong> ğŸ”„ Simplified operations, âš™ï¸ improved scalability and automation.</p><p><strong>Achievements:</strong> A PR was created to merge this feature into Gardener.</p><p><strong>Next Steps:</strong> The PR needs to be reviewed and merged.</p><p><strong>Issue:</strong></p><ul><li><a href="https://github.com/gardener/gardener/issues/12250" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/issues/12250</a> (stretch goal)</li></ul><p><strong>Code/Pull Requests:</strong></p><ul><li><a href="https://github.com/gardener/gardener/pull/12251" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/pull/12251</a></li><li><a href="https://github.com/gardener/gardener/compare/master...metal-stack:gardener:controlplane-objects-provider-interface" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/compare/master...metal-stack:gardener:controlplane-objects-provider-interface</a> (stretch goal)</li></ul><h2 id="âœ¨-dashboard-usability-improvements" tabindex="-1">âœ¨ Dashboard Usability Improvements <a class="header-anchor" href="#âœ¨-dashboard-usability-improvements" aria-label="Permalink to &quot;âœ¨ Dashboard Usability Improvements&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> The Gardener Dashboard assumes static, non-configurable defaults for e.g. Shoot values, which may not be suitable for all deployment scenarios. Some points that could be improved: Value Defaulting (landscape scope): e.g. AutoScaler min/max replicas; Overrides (project scope): Optional labels for Shoots that can be used as a display name to overcome the project name length limit; Hide UI Elements (landscape scope): e.g. Control Plane HA; Add new UI Elements (stretch goal): would require extensibility concept for the dashboard</p><p><strong>Motivation/Benefits:</strong> ğŸ–¥ï¸ Improved user experience, ğŸ› ï¸ more flexible and customizable dashboard.</p><p><strong>Achievements:</strong></p><ul><li>Annotations for project resources can be added that are used by the gardener-dashboard to show a custom display name for projects. This allows to overcome the project name length limit.</li><li>The dashboard can now be configured to use default values for Shoots, e.g. AutoScaler min/max replicas.</li><li>The dashboard can now be configured to hide certain UI elements, e.g. Control Plane HA.</li></ul><p><strong>Next Steps:</strong> There are some open discussions on how to incorporate the new features into the dashboard.</p><p><strong>Issue:</strong><a href="https://github.com/gardener/dashboard/issues/2469" target="_blank" rel="noreferrer">https://github.com/gardener/dashboard/issues/2469</a></p><p><strong>Code/Pull Requests:</strong></p><ul><li>Project Title â€“ <a href="https://github.com/gardener/dashboard/pull/2470" target="_blank" rel="noreferrer">https://github.com/gardener/dashboard/pull/2470</a></li><li>Value Defaulting â€“ <a href="https://github.com/gardener/dashboard/pull/2476" target="_blank" rel="noreferrer">https://github.com/gardener/dashboard/pull/2476</a></li><li>Hide UI Elements â€“ <a href="https://github.com/gardener/dashboard/pull/2478" target="_blank" rel="noreferrer">https://github.com/gardener/dashboard/pull/2478</a></li></ul><h2 id="âš–ï¸-cluster-internal-l7-load-balancing-endpoints-for-kube-apiservers" tabindex="-1">âš–ï¸ Cluster-internal L7 Load-Balancing Endpoints for <code>kube-apiserver</code>s <a class="header-anchor" href="#âš–ï¸-cluster-internal-l7-load-balancing-endpoints-for-kube-apiservers" aria-label="Permalink to &quot;âš–ï¸ Cluster-internal L7 Load-Balancing Endpoints for `kube-apiserver`s&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> In the last hackathon we created an L7 load-balancing for the external endpoints of Gardener kube-apiservers (Shoots &amp; Virtual Garden). However, cluster internal traffic like from gardener-resource-manager and gardener-controller-manager accesses the Kubernetes internal services directly, skips Istio and so the L7 load-balancing. We noticed at least for gardener-controller-manager that it could generate some load to the gardener-apiserver. Thus, it would be nice to have a cluster internal load-balancing too. We don&#39;t want to use the external endpoint since depending on the infrastructure this could create additional external traffic.</p><p><strong>Motivation/Benefits:</strong> âš–ï¸ Better resource distribution, ğŸš¦ improved reliability for internal traffic.</p><p><strong>Achievements:</strong> Cluster internal L7 load balancing was implemented by leveraging generic token kubeconfig, creating a dedicated clusterIP service for Istio ingress gateway pods, and introducing a pod-kube-apiserver-load-balancing webhook to inject necessary configurations into control plane pods.</p><p><strong>Next Steps:</strong> Merge the PR.</p><p><strong>Issue:</strong><a href="https://github.com/gardener/gardener/issues/8810" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/issues/8810</a></p><p><strong>Code/Pull Requests:</strong><a href="https://github.com/gardener/gardener/pull/12260" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/pull/12260</a></p><h2 id="ğŸ“œ-documentation-revamp" tabindex="-1">ğŸ“œ Documentation Revamp <a class="header-anchor" href="#ğŸ“œ-documentation-revamp" aria-label="Permalink to &quot;ğŸ“œ Documentation Revamp&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> Usually, the individual content of our documentation is of high quality and helpful. However, we typically receive complaints about the structure and explorability of our documentation.</p><p><strong>Motivation/Benefits:</strong> ğŸ“š Easier onboarding, ğŸ” improved discoverability and structure.</p><p><strong>Achievements:</strong></p><ul><li>Metadata, e.g. page synonyms and categories for the documentation pages was added / enhanced to allow for better discoverability.</li><li>Glossary was enhanced to explain more terms and concepts used in the documentation.</li><li>A PoC for using VitePress as a documentation generator was created. This allows for a more modern and flexible documentation structure, as well as better developer experience when working on the documentation.</li></ul><p><strong>Next Steps:</strong></p><ul><li>Take more time to improve metadata as well as the search algorithm.</li><li>Potentially add analytics to the documentation to understand how it is used and where improvements can be made.</li><li>Look into the PoC for VitePress and decide if it should be used as a replacement for the current documentation.</li></ul><p><strong>Code/Pull Requests:</strong></p><ul><li><a href="https://github.com/gardener/documentation/pull/652" target="_blank" rel="noreferrer">https://github.com/gardener/documentation/pull/652</a></li><li><a href="https://github.com/gardener/documentation/pull/653" target="_blank" rel="noreferrer">https://github.com/gardener/documentation/pull/653</a></li></ul><h2 id="iï¸-expose-egresscidrs-in-shoot-info-configmap-ğŸï¸" tabindex="-1">â„¹ï¸ Expose EgressCIDRs in shoot-info <code>ConfigMap</code> ğŸï¸ <a class="header-anchor" href="#iï¸-expose-egresscidrs-in-shoot-info-configmap-ğŸï¸" aria-label="Permalink to &quot;â„¹ï¸ Expose EgressCIDRs in shoot-info `ConfigMap` ğŸï¸&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> Some stakeholders need to know the egress CIDRs of a shoot cluster. Helps expose meta-level information about the shoot to workloads of the shoot. This could be useful in case of controllers e.g. crossplane that run on the shoot and need access to some information of the existing infrastructure.</p><p><strong>Motivation/Benefits:</strong> ğŸ·ï¸ Enable better integration for shoot workloads, ğŸ“¤ improved transparency.</p><p><strong>Achievements:</strong> It was implemented and merged during the hackathon.</p><p><strong>Code/Pull Requests:</strong><a href="https://github.com/gardener/gardener/pull/12252" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/pull/12252</a></p><h2 id="ğŸ“ˆ-overcome-maximum-of-450-nodes-on-azure" tabindex="-1">ğŸ“ˆ Overcome Maximum of 450 <code>Node</code>s on Azure <a class="header-anchor" href="#ğŸ“ˆ-overcome-maximum-of-450-nodes-on-azure" aria-label="Permalink to &quot;ğŸ“ˆ Overcome Maximum of 450 `Node`s on Azure&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> Extensions that do not rely on overlay networking for their in-cluster networking usually rely on other mechanisms such as route tables to establish p2p traffic. Azure being one of them. We currently face scaling difficulties as clusters generally approach the maximum size of route tables set by the provider, and we need a new network architecture to overcome this limitation.</p><p><strong>Motivation/Benefits:</strong> ğŸš€ Enable larger clusters, ğŸ—ï¸ reference for other providers.</p><p><strong>Achievements:</strong> Analysis and hurdles of the problem were identified, and a potential solution was discussed. The solution involves using a combination of route tables and virtual networks to overcome the limitations.</p><p><strong>Next Steps:</strong> A preview feature of Azure is needed to be able to follow up on this. Until it is enabled, it takes some time.</p><h2 id="ğŸ¦œ-multiple-parallel-versions-in-a-gardener-landscape-fka-canary-deployments" tabindex="-1">ğŸ¦œ Multiple Parallel Versions in a Gardener Landscape (fka. Canary Deployments) <a class="header-anchor" href="#ğŸ¦œ-multiple-parallel-versions-in-a-gardener-landscape-fka-canary-deployments" aria-label="Permalink to &quot;ğŸ¦œ Multiple Parallel Versions in a Gardener Landscape (fka. Canary Deployments)&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> Gardener currently has very tight versioning constraints. For example, it is not possible to have multiple versions of the same extension type running on the same seed cluster. Therefore, previous ideas around canary deployments ended with the direction to make control plane migration as easy/fast/reliable as possible so that the control planes can be moved between seeds running with different versions of gardenlet/extensions. A different approach, favoured by this proposal, would be to make it possible to run multiple version in parallel on a single seed cluster with filters in place so that individual control plane components are still reconciled only by one set of version. This would allow multiple &quot;channel&quot; in the same landscape, e.g. alpha, beta, GA.</p><p><strong>Motivation/Benefits:</strong> ğŸ¦œ Enable canary/parallel versioning, ğŸ”„ safer rollouts.</p><p><strong>Achievements:</strong> It was discovered that the current implementation allows to roll out different versions across different seeds.</p><p><strong>Next Steps:</strong> There are still some caveats which require further discussion, which will be followed up on.</p><h2 id="â™»ï¸-gep-32-â€“-version-classification-lifecycles-ğŸï¸" tabindex="-1">â™»ï¸ GEP-32 â€“ Version Classification Lifecycles ğŸï¸ <a class="header-anchor" href="#â™»ï¸-gep-32-â€“-version-classification-lifecycles-ğŸï¸" aria-label="Permalink to &quot;â™»ï¸ GEP-32 â€“ Version Classification Lifecycles ğŸï¸&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> Last Gardener Hackathon we implemented Version Classification Lifecycles in the (Namespaced) Cloud Profile, proposed the GEP-32, but we couldn&#39;t find enough time to finish the required pull requests for the actual implementation.</p><p><strong>Motivation/Benefits:</strong> â™»ï¸ Automated version lifecycle management, â³ reduced manual effort.</p><p><strong>Achievements:</strong> The previous PR was rebased and confirmed to still be working. The implementation was cut into several smaller PRs to make it easier to review and merge.</p><p><strong>Next Steps:</strong> The individual PRs need to be worked on, reviewed and merged.</p><p><strong>Code/Pull Requests:</strong><a href="https://github.com/metal-stack/gardener/pull/9" target="_blank" rel="noreferrer">https://github.com/metal-stack/gardener/pull/9</a></p><h2 id="ğŸ§‘â€ğŸ”§-worker-group-node-roll-out-ğŸï¸" tabindex="-1">ğŸ§‘â€ğŸ”§ Worker Group Node Roll-out ğŸï¸ <a class="header-anchor" href="#ğŸ§‘â€ğŸ”§-worker-group-node-roll-out-ğŸï¸" aria-label="Permalink to &quot;ğŸ§‘â€ğŸ”§ Worker Group Node Roll-out ğŸï¸&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> Kubernetes Deployments allow simple rollout of pods with <code>kubectl rollout restart deployment &lt;name&gt;</code>. There are situations when something similar would be nice to have for worker groups in the Gardener context. For example, the dual-stack migration requires to roll nodes. It would be easier to have a simple way, e.g. by using an annotation on the shoot resource, to trigger node rollout for a worker group.</p><p><strong>Motivation/Benefits:</strong> ğŸ§‘â€ğŸ”§ Improved node management, ğŸš€ streamlined rollouts.</p><p><strong>Achievements:</strong> A PoC was created that allows to trigger a node roll-out for a worker group by using an annotation on the shoot resource. Now the Shoot resource can be annotated with <code>gardener.cloud/operation=rollout-workers=&lt;pool1&gt;,&lt;pool2&gt;,...,&lt;poolN&gt;.</code> This causes a new status to appear on the Shoot that shows the given worker pools as pending for roll-out.</p><p><strong>Next Steps:</strong> The PoC needs to be polished and tested, as well as reviewed and merged.</p><p><strong>Code/Pull Requests:</strong><a href="https://github.com/rrhubenov/gardener/tree/worker-pool-rollout" target="_blank" rel="noreferrer">https://github.com/rrhubenov/gardener/tree/worker-pool-rollout</a></p><h2 id="ğŸ‘€-instance-scheduled-events-watcher" tabindex="-1">ğŸ‘€ Instance Scheduled Events Watcher <a class="header-anchor" href="#ğŸ‘€-instance-scheduled-events-watcher" aria-label="Permalink to &quot;ğŸ‘€ Instance Scheduled Events Watcher&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> Cloud providers may need to move, stop, retire, reboot etc. The means of communication differ, but they are generally available either in the instance objects themselves or more commonly the metadata service. The general goal of this task is to create an agent that can browse and expose via node events and/or dashboard, shoot condition warnings or similar channels available to stakeholders the VM events. This will allow users to take action for their critical workloads inside their maintenance windows and not the arbitrary cloudprovider date.</p><p><strong>Motivation/Benefits:</strong> ğŸ•’ Proactive workload management, ğŸ”” timely notifications for critical events</p><p><strong>Achievements:</strong> For Azure, a PR was raised to enable this functionality. It allows to watch for scheduled events on Azure VMs and expose them as node conditions. This way, users can be notified of upcoming maintenance windows and take action accordingly.</p><p><strong>Next Steps:</strong> The PR needs to be reviewed and merged. After that, it can be discussed how to proceed with the integration of other providers.</p><p><strong>Code/Pull Requests:</strong><a href="https://github.com/kubernetes-sigs/cloud-provider-azure/pull/9170" target="_blank" rel="noreferrer">https://github.com/kubernetes-sigs/cloud-provider-azure/pull/9170</a></p><p><img src="https://apeirora.eu/assets/img/BMWK-EU.png" alt="ApeiroRA"></p>',107)]))}const v=t(c,[["render",d]]);export{b as __pageData,v as default};
