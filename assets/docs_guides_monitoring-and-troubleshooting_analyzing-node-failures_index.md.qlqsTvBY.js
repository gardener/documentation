import{_ as t,c as a,o,a2 as n}from"./chunks/framework.Bfq10Vlj.js";const p=JSON.parse(`{"title":"Analyzing Node Removal and Failures","description":"Utilize Gardener's Monitoring and Logging to analyze removal and failures of nodes","frontmatter":{"aliases":["/docs/guides/monitoring-and-troubleshooting/analysing-node-failures/"],"category":"Debugging","description":"Utilize Gardener's Monitoring and Logging to analyze removal and failures of nodes","github_repo":"https://github.com/gardener/documentation","github_subdir":"website/documentation/guides/monitoring-and-troubleshooting","level":"intermediate","params":{"github_branch":"master"},"path_base_for_github_subdir":{"from":"content/docs/guides/monitoring-and-troubleshooting/analyzing-node-failures.md","to":"analyzing-node-failures.md"},"scope":"operator","title":"Analyzing Node Removal and Failures","prev":false,"next":false},"headers":[],"relativePath":"docs/guides/monitoring-and-troubleshooting/analyzing-node-failures/index.md","filePath":"docs/guides/monitoring-and-troubleshooting/analyzing-node-failures.md","lastUpdated":null}`),i={name:"docs/guides/monitoring-and-troubleshooting/analyzing-node-failures/index.md"};function s(r,e,l,h,d,c){return o(),a("div",null,e[0]||(e[0]=[n('<h1 id="analyzing-node-removal-and-failures" tabindex="-1">Analyzing Node Removal and Failures <a class="header-anchor" href="#analyzing-node-removal-and-failures" aria-label="Permalink to &quot;Analyzing Node Removal and Failures&quot;">​</a></h1><h2 id="overview" tabindex="-1">Overview <a class="header-anchor" href="#overview" aria-label="Permalink to &quot;Overview&quot;">​</a></h2><p>Sometimes operators want to find out why a certain node got removed. This guide helps to identify possible causes. There are a few potential reasons why nodes can be removed:</p><ul><li><a href="/docs/guides/monitoring-and-troubleshooting/analyzing-node-failures/#find-out-whether-the-node-was-unhealthy">broken node</a>: a node becomes unhealthy and machine-controller-manager terminates it in an attempt to replace the unhealthy node with a new one</li><li><a href="/docs/guides/monitoring-and-troubleshooting/analyzing-node-failures/#scale-down">scale-down</a>: cluster-autoscaler sees that a node is under-utilized and therefore scales down a worker pool</li><li><a href="/docs/guides/monitoring-and-troubleshooting/analyzing-node-failures/#node-rolling">node rolling</a>: configuration changes to a worker pool (or cluster) require all nodes of one or all worker pools to be rolled and thus all nodes to be replaced. Some possible changes are: <ul><li>the K8s/OS version</li><li>changing machine types</li></ul></li></ul><p>Helpful information can be obtained by using the logging stack. See <a href="/docs/gardener/observability/logging/">Logging Stack</a> for how to utilize the logging information in Gardener.</p><h2 id="find-out-whether-the-node-was-unhealthy" tabindex="-1">Find Out Whether the Node Was <code>unhealthy</code> <a class="header-anchor" href="#find-out-whether-the-node-was-unhealthy" aria-label="Permalink to &quot;Find Out Whether the Node Was `unhealthy`&quot;">​</a></h2><h3 id="check-the-node-events" tabindex="-1">Check the Node Events <a class="header-anchor" href="#check-the-node-events" aria-label="Permalink to &quot;Check the Node Events&quot;">​</a></h3><p>A good first indication on what happened to a node can be obtained from the node&#39;s events. Events are scraped and ingested into the logging system, so they can be found in the explore tab of Grafana (make sure to select <code>loki</code> as datasource) with a query like <code>{job=&quot;event-logging&quot;} | unpack | object=&quot;Node/&lt;node-name&gt;&quot;</code> or find any event mentioning the node in question via a broader query like <code>{job=&quot;event-logging&quot;}|=&quot;&lt;node-name&gt;&quot;</code>.</p><p>A potential result might reveal:</p><div class="language-json vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">json</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">{</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">&quot;_entry&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Node ip-10-55-138-185.eu-central-1.compute.internal status is now: NodeNotReady&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">&quot;count&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">&quot;firstTimestamp&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;2023-04-05T12:02:08Z&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">&quot;lastTimestamp&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;2023-04-05T12:02:08Z&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">&quot;namespace&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;default&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">&quot;object&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Node/ip-10-55-138-185.eu-central-1.compute.internal&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">&quot;origin&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;shoot&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">&quot;reason&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;NodeNotReady&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">&quot;source&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;node-controller&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">&quot;type&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Normal&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div><h3 id="check-machine-controller-manager-logs" tabindex="-1">Check machine-controller-manager Logs <a class="header-anchor" href="#check-machine-controller-manager-logs" aria-label="Permalink to &quot;Check machine-controller-manager Logs&quot;">​</a></h3><p>If a node was getting unhealthy, the last conditions can be found in the logs of the <code>machine-controller-manager</code> by using a query like <code>{pod_name=~&quot;machine-controller-manager.*&quot;}|=&quot;&lt;node-name&gt;&quot;</code>.</p><p><strong>Caveat</strong>: every <code>node</code> resource is backed by a corresponding <code>machine</code> resource managed by machine-controller-manager. Usually two corresponding <code>node</code> and <code>machine</code> resources have the same name with the exception of AWS. Here you first need to find with the above query the corresponding <code>machine</code> name, typically via a log like this</p><div class="language-text vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">text</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>2023-04-05 12:02:08 {&quot;log&quot;:&quot;Conditions of Machine \\&quot;shoot--demo--cluster-pool-z1-6dffc-jh4z4\\&quot; with providerID \\&quot;aws:///eu-central-1/i-0a6ad1ca4c2e615dc\\&quot; and backing node \\&quot;ip-10-55-138-185.eu-central-1.compute.internal\\&quot; are changing&quot;,&quot;pid&quot;:&quot;1&quot;,&quot;severity&quot;:&quot;INFO&quot;,&quot;source&quot;:&quot;machine_util.go:629&quot;}</span></span></code></pre></div><p>This reveals that <code>node</code> <code>ip-10-55-138-185.eu-central-1.compute.internal</code> is backed by <code>machine</code> <code>shoot--demo--cluster-pool-z1-6dffc-jh4z4</code>. On infrastructures other than AWS you can omit this step.</p><p>With the machine name at hand, now search for log entries with <code>{pod_name=~&quot;machine-controller-manager.*&quot;}|=&quot;&lt;machine-name&gt;&quot;</code>. In case the node had failing conditions, you&#39;d find logs like this:</p><div class="language-text vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">text</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>2023-04-05 12:02:08 {&quot;log&quot;:&quot;Machine shoot--demo--cluster-pool-z1-6dffc-jh4z4 is unhealthy - changing MachineState to Unknown. Node conditions: [{Type:ClusterNetworkProblem Status:False LastHeartbeatTime:2023-04-05 11:58:39 +0000 UTC LastTransitionTime:2023-03-23 11:59:29 +0000 UTC Reason:NoNetworkProblems Message:no cluster network problems} ... {Type:Ready Status:Unknown LastHeartbeatTime:2023-04-05 11:55:27 +0000 UTC LastTransitionTime:2023-04-05 12:02:07 +0000 UTC Reason:NodeStatusUnknown Message:Kubelet stopped posting node status.}]&quot;,&quot;pid&quot;:&quot;1&quot;,&quot;severity&quot;:&quot;WARN&quot;,&quot;source&quot;:&quot;machine_util.go:637&quot;}</span></span></code></pre></div><p>In the example above, the reason for an unhealthy node was that <code>kubelet</code> failed to renew its heartbeat. Typical reasons would be either a broken VM (that couldn&#39;t execute <code>kubelet</code> anymore) or a broken network. Note that some VM terminations performed by the infrastructure provider are actually expected (e.g., <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-instances-status-check_sched.html" target="_blank" rel="noreferrer">scheduled events on AWS</a>).</p><p>In both cases, the infrastructure provider might be able to provide more information on particular VM or network failures.</p><p>Whatever the failure condition might have been, if a node gets unhealthy, it will be terminated by <code>machine-controller-manager</code> after the <code>machineHealthTimeout</code> has elapsed (this parameter can be configured in your <a href="https://github.com/gardener/gardener/blob/v1.68.0/example/90-shoot.yaml#L132" target="_blank" rel="noreferrer">shoot spec</a>).</p><h3 id="check-the-node-logs" tabindex="-1">Check the Node Logs <a class="header-anchor" href="#check-the-node-logs" aria-label="Permalink to &quot;Check the Node Logs&quot;">​</a></h3><p>For each <code>node</code> the kernel and <code>kubelet</code> logs, as well as a few others, are scraped and can be queried with this query <code>{nodename=&quot;&lt;node-name&gt;&quot;}</code> This might reveal OS specific issues or, in the absence of any logs (e.g., after the node went unhealthy), might indicate a network disruption or sudden VM termination. Note that some VM terminations performed by the infrastructure provider are actually expected (e.g., <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-instances-status-check_sched.html" target="_blank" rel="noreferrer">scheduled events on AWS</a>).</p><p>Infrastructure providers might be able to provide more information on particular VM failures in such cases.</p><h3 id="check-the-network-problem-detector-dashboard" tabindex="-1">Check the Network Problem Detector Dashboard <a class="header-anchor" href="#check-the-network-problem-detector-dashboard" aria-label="Permalink to &quot;Check the Network Problem Detector Dashboard&quot;">​</a></h3><p>If your Gardener installation utilizes <a href="https://github.com/gardener/gardener-extension-shoot-networking-problemdetector" target="_blank" rel="noreferrer">gardener-extension-shoot-networking-problemdetector</a>, you can check the dashboard named &quot;Network Problem Detector&quot; in Grafana for hints on network issues on the node of interest.</p><h2 id="scale-down" tabindex="-1">Scale-Down <a class="header-anchor" href="#scale-down" aria-label="Permalink to &quot;Scale-Down&quot;">​</a></h2><p>In general, scale-downs are managed by the <a href="https://github.com/gardener/autoscaler" target="_blank" rel="noreferrer">cluster-autoscaler</a>, its logs can be found with the query <code>{container_name=&quot;cluster-autoscaler&quot;}</code>. Attempts to remove a node can be found with the query <code>{container_name=&quot;cluster-autoscaler&quot;}|=&quot;Scale-down: removing empty node&quot;</code></p><p>If a scale-down has caused disruptions in your workload, consider protecting your workload by adding <code>PodDisruptionBudgets</code> (see the <a href="https://github.com/gardener/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-types-of-pods-can-prevent-ca-from-removing-a-node" target="_blank" rel="noreferrer">autoscaler FAQ</a> for more options).</p><h2 id="node-rolling" tabindex="-1">Node Rolling <a class="header-anchor" href="#node-rolling" aria-label="Permalink to &quot;Node Rolling&quot;">​</a></h2><p>Node rolling can be caused by, e.g.:</p><ul><li>change of the K8s minor version of the cluster or a worker pool</li><li>change of the OS version of the cluster or a worker pool</li><li>change of the disk size/type or machine size/type of a worker pool</li><li>change of node labels</li></ul><p>Changes like the above are done by altering the shoot specification and thus are recorded in the external auditlog system that is configured for the garden cluster.</p>',32)]))}const g=t(i,[["render",s]]);export{p as __pageData,g as default};
