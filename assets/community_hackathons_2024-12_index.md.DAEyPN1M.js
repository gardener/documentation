import{_ as t,c as r,o,a2 as n}from"./chunks/framework.Bfq10Vlj.js";const a="/assets/2024-12.QbVbQiKo.jpg",g=JSON.parse('{"title":"December 2024","description":"","frontmatter":{"github_repo":"https://github.com/gardener/documentation","github_subdir":"website/community/hackathons","params":{"github_branch":"master"},"path_base_for_github_subdir":{"from":"content/community/hackathons/2024-12.md","to":"2024-12.md"},"title":"December 2024","weight":-202412,"prev":false,"next":false},"headers":[],"relativePath":"community/hackathons/2024-12/index.md","filePath":"community/hackathons/2024-12.md","lastUpdated":null}'),s={name:"community/hackathons/2024-12/index.md"};function i(l,e,c,d,h,u){return o(),r("div",null,e[0]||(e[0]=[n('<h1 id="hack-the-garden-12-2024-wrap-up" tabindex="-1">Hack The Garden 12/2024 Wrap Up <a class="header-anchor" href="#hack-the-garden-12-2024-wrap-up" aria-label="Permalink to &quot;Hack The Garden 12/2024 Wrap Up&quot;">â€‹</a></h1><ul><li>ğŸ—“ï¸ <strong>Date:</strong> 02.12.2024 â€“ 06.12.2024</li><li>ğŸ“ <strong>Location:</strong> <a href="https://www.schlosshof-info.de/" target="_blank" rel="noreferrer">Schlosshof Freizeitheim, Schelklingen</a></li><li>ğŸ‘¤ <strong>Organizer:</strong> <a href="https://www.x-cellent.com/" target="_blank" rel="noreferrer">x-cellent</a></li><li>ğŸ“˜ <strong>Topics:</strong> <a href="https://hackmd.io/RSU9-ZteSDakBxDQAfI3rg" target="_blank" rel="noreferrer">https://hackmd.io/RSU9-ZteSDakBxDQAfI3rg</a></li><li>ğŸ¤ <strong>Review Meeting Summary:</strong> <a href="https://gardener.cloud/community/review-meetings/2024-reviews/#_2024-12-11-hack-the-garden-wrap-up" target="_blank" rel="noreferrer">https://gardener.cloud/community/review-meetings/2024-reviews/#_2024-12-11-hack-the-garden-wrap-up</a></li></ul><p><img src="'+a+'" alt="Group picture"></p><h2 id="ğŸŒ-ipv6-support-on-ironcore" tabindex="-1">ğŸŒ IPv6 Support On <a href="https://github.com/ironcore-dev" target="_blank" rel="noreferrer">IronCore</a> <a class="header-anchor" href="#ğŸŒ-ipv6-support-on-ironcore" aria-label="Permalink to &quot;ğŸŒ IPv6 Support On [IronCore](https://github.com/ironcore-dev)&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> IPv6-only shoot clusters are not yet supported on IronCore, yet it is clear that we eventually have to support it, given the limited scalability and phase-out of IPv4.</p><p><strong>Motivation/Benefits</strong>: âœ¨ Support more use cases/scenarios.</p><p><strong>Achievements:</strong> Due to incompatibilities of the recent Gardener version with the extension for IronCore, which had to be fixed first, the progress was delayed a bit. Eventually, we were able to create dual-stack shoot clusters. However, there are still quite some open issues, e.g., <code>Service</code> of type <code>LoadBalancer</code> don&#39;t not work for them, node-to-node communication does not work for IPv6 traffic, and a few more.</p><p><strong>Next Steps:</strong> Now that a shoot cluster can at least be created (reconciles to 100%), the issues can be investigated and hopefully fixed one-by-one.</p><p><strong>Code/Pull Requests:</strong> <a href="https://github.com/ironcore-dev/machine-controller-manager-provider-ironcore/pull/436" target="_blank" rel="noreferrer">https://github.com/ironcore-dev/machine-controller-manager-provider-ironcore/pull/436</a>, <a href="https://github.com/ironcore-dev/gardener-extension-provider-ironcore/pull/669" target="_blank" rel="noreferrer">https://github.com/ironcore-dev/gardener-extension-provider-ironcore/pull/669</a>, <a href="https://github.com/ironcore-dev/cloud-provider-ironcore/pull/473" target="_blank" rel="noreferrer">https://github.com/ironcore-dev/cloud-provider-ironcore/pull/473</a></p><h2 id="ğŸ”-version-classification-lifecycle-in-cloudprofiles" tabindex="-1">ğŸ” Version Classification Lifecycle In <code>CloudProfile</code>s <a class="header-anchor" href="#ğŸ”-version-classification-lifecycle-in-cloudprofiles" aria-label="Permalink to &quot;ğŸ” Version Classification Lifecycle In `CloudProfile`s&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> Kubernetes or machine image versions typically go through different classifications whilst being offered to users via <code>CloudProfile</code>s. Usually, they start with <code>preview</code>, transition to <code>supported</code>, and eventually to <code>deprecated</code> before they expire. Today, human operators have to manually facilitate these transitions and change the respective <code>CloudProfile</code>s at the times when they want to promote or demote a version. The goal of this task is to allow predefining the timestamps when a certain classification is reached. This does not only lift manual effort of the operators, but also makes the transitions more predictable/plannable for end-users.</p><p><strong>Motivation/Benefits</strong>: ğŸ”§ Reduced operational complexity, ğŸ¤© improved user experience.</p><p><strong>Achievements:</strong> After a lot of brainstorming, we decided that it makes sense to write a GEP to get more opinions on how the API should look like. The current tendency is to maintain the lifecycle times for the classifications in the <code>spec</code>, while a controller would then maintain the currently active classification in the <code>status</code> for clients to read.</p><p><strong>Next Steps:</strong> The GEP has been filed and is currently in review. There is a PoC implementation according to the description, so once the design has been approved, the work can be finalized and the pull requests can be prepared.</p><p><strong>Code/Pull Requests:</strong> <a href="https://github.com/gardener/gardener/pull/10982" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/pull/10982</a>, <a href="https://github.com/metal-stack/gardener/pull/9" target="_blank" rel="noreferrer">https://github.com/metal-stack/gardener/pull/9</a></p><h2 id="ğŸ’¡-gardener-slis-shoot-cluster-creation-deletion-times" tabindex="-1">ğŸ’¡ Gardener SLIs: Shoot Cluster Creation/Deletion Times <a class="header-anchor" href="#ğŸ’¡-gardener-slis-shoot-cluster-creation-deletion-times" aria-label="Permalink to &quot;ğŸ’¡ Gardener SLIs: Shoot Cluster Creation/Deletion Times&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> We lack observability in our end-to-end testing whether a change introduces a regression in terms of prolonged shoot cluster creation or deletion times. The goal is to (a) expose these metrics via <code>gardenlet</code>, and (b) collect and push them to a <a href="https://prometheus.io/" target="_blank" rel="noreferrer">Prometheus</a> instance running in our <a href="https://prow.gardener.cloud/" target="_blank" rel="noreferrer">Prow cluster</a>. From there, we can centrally observe the SLIs over time and potentially even define alerting rules.</p><p><strong>Motivation/Benefits</strong>: ğŸ‘¨ğŸ¼â€ğŸ’» Improved developer productivity, ğŸ§ª increased output qualification.</p><p><strong>Achievements:</strong> A new Prometheus instance in the Prow cluster now collects the metrics for cluster/creation time as well as times for individual tasks in the large <code>Shoot</code> reconciliation flow. Dashboards nicely show the data for service providers to evaluate and reason about. While the metrics are mainly collected in the CI system for e2e test executions, productive deployments of <code>gardenlet</code>s also expose them. Hence, we now also get some data for real systems.</p><p><strong>Code/Pull Requests:</strong> <a href="https://github.com/gardener/gardener/pull/10964" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/pull/10964</a>, <a href="https://github.com/gardener/gardener/pull/10967" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/pull/10967</a>, <a href="https://github.com/gardener/ci-infra/pull/2807" target="_blank" rel="noreferrer">https://github.com/gardener/ci-infra/pull/2807</a></p><h2 id="ğŸ›¡ï¸-enhanced-seed-authorizer-with-label-field-selectors" tabindex="-1">ğŸ›¡ï¸ Enhanced <code>Seed</code> Authorizer With Label/Field Selectors <a class="header-anchor" href="#ğŸ›¡ï¸-enhanced-seed-authorizer-with-label-field-selectors" aria-label="Permalink to &quot;ğŸ›¡ï¸ Enhanced `Seed` Authorizer With Label/Field Selectors&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> The <code>Seed</code> Authorizer is used to restrict <code>gardenlet</code> to only access resources which belong to the <code>Seed</code> it is responsible for. However, until Kubernetes <code>v1.31</code>, there was no technical possibility to limit its visibility for resources it watches (e.g., <code>Shoot</code>s). This means that all <code>gardenlet</code>s are able to see all <code>Shoot</code>s even if they don&#39;t belong to their <code>Seed</code>. With Kubernetes <code>v1.31</code>, a new feature allows to pass label/field selectors to authorizers, which finally allows us to enforce that the <code>gardenlet</code> uses respective selectors.</p><p><strong>Motivation/Benefits</strong>: ğŸ›¡ï¸ Increased cluster security.</p><p><strong>Achievements:</strong> We enabled the selector enforcement for the <code>Shoot</code>, <code>ControllerInstallation</code>, <code>Bastion</code>, and <code>Gardenlet</code> APIs. Unfortunately, it requires to enable the new <code>AuthorizeWithSelectors</code> feature gate on both <code>{kube,gardener}-apiserver</code>. We have also introduced a new feature gate in <code>gardener-admission-controller</code> (serving the <code>Seed</code> authorizer webhook) in order to be able to enforce the selector (this can only work with Kubernetes <code>v1.31</code> when the feature gates are enabled, or with <code>v1.32</code>+ where the feature gates got promoted to beta). On our way, we have discovered more opportunity to tune the authorizer, for example shrinking <code>gardenlet</code>&#39;s permissions for other <code>Seed</code> resources.</p><p><strong>Next Steps:</strong> A PoC implementation has been prepared, yet it has to be finalized and the mentioned feature gate has to be introduced. There are separate/independent work branches for the other improvements we discovered, and also they have to be finalized.</p><p><strong>Code/Pull Requests:</strong> <a href="https://github.com/rfranzke/gardener/tree/improve-seed-authz" target="_blank" rel="noreferrer">https://github.com/rfranzke/gardener/tree/improve-seed-authz</a></p><h2 id="ğŸ”‘-bring-your-own-etcd-encryption-key-via-key-management-systems" tabindex="-1">ğŸ”‘ Bring Your Own ETCD Encryption Key Via Key Management Systems <a class="header-anchor" href="#ğŸ”‘-bring-your-own-etcd-encryption-key-via-key-management-systems" aria-label="Permalink to &quot;ğŸ”‘ Bring Your Own ETCD Encryption Key Via Key Management Systems&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> Gardener uses the AESCBC algorithm to compute the encryption key for ETCD of shoot clusters. This key is stored as <code>Secret</code> in the seed cluster. Yet, some users prefer to manage the encryption key on their own via a key management system (e.g., Vault or AWS KMS). Kubernetes already supports <a href="https://kubernetes.io/docs/tasks/administer-cluster/kms-provider/" target="_blank" rel="noreferrer">various KMS providers</a>. The goal is to allow configuring an external KMS and key in the <code>Shoot</code> specification to support this feature.</p><p><strong>Motivation/Benefits</strong>: ğŸ›¡ï¸ Increased cluster security.</p><p><strong>Achievements:</strong> We came up with a new API called <code>ControlPlaneEncryptionConfig</code> in the <code>extensions.gardener.cloud</code> API group. This way, we can write new extensions for arbitrary external KMS plugins. In the <code>Shoot</code> API, the mentioned encryption config can be passed via <code>.spec.kubernetes.kubeAPIServer.encryptionConfig</code>. The respective extension is then responsible to create the <a href="https://kubernetes.io/docs/tasks/administer-cluster/kms-provider/#encrypting-your-data-with-the-kms-provider-kms-v1" target="_blank" rel="noreferrer"><code>EncryptionConfiguration</code> for <code>kube-apiserver</code></a> and inject the KMS plugin sidecar container into the <code>kube-apiserver</code> <code>Deployment</code>. We have also develop concepts for migration to and from the current AESCBC provider, and for KMS key rotation.</p><p><strong>Next Steps:</strong> Due to the size and criticality of the feature, we decided to write a GEP to align on the overall approach. The general concept has been validated in a PoC and looks promising.</p><p><strong>Code/Pull Requests:</strong> <a href="https://github.com/plkokanov/gardener/tree/hackathon-kms" target="_blank" rel="noreferrer">https://github.com/plkokanov/gardener/tree/hackathon-kms</a></p><h2 id="âš–ï¸-load-balancing-for-calls-to-kube-apiservers" tabindex="-1">âš–ï¸ Load Balancing For Calls To <code>kube-apiserver</code>s <a class="header-anchor" href="#âš–ï¸-load-balancing-for-calls-to-kube-apiservers" aria-label="Permalink to &quot;âš–ï¸ Load Balancing For Calls To `kube-apiserver`s&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> TLS connections are terminated by the individual shoot API server instances. After establishing a TLS connection, all requests sent over this connection end up on the same API server instance. Hence, the Istio ingress gateway performs L4 load balancing only, but it doesn&#39;t distribute individual requests across API server instances. This often overloads a single API server pod while others are much under-utilized.</p><p><strong>Motivation/Benefits</strong>: ğŸ“ˆ Better scalability, ğŸ¤© improved user experience.</p><p><strong>Achievements:</strong> We have switched to Istio terminating the TLS connections which allows to properly load balance the request to the <code>kube-apiserver</code> pods. A load generator nicely demonstrates that this indeed improves the situation compared to current implementation.</p><p><strong>Next Steps:</strong> The work has to finalized and the pull request has to be opened.</p><p><strong>Issue:</strong> <a href="https://github.com/gardener/gardener/issues/8810" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/issues/8810</a></p><p><strong>Code/Pull Requests:</strong> <a href="https://github.com/oliver-goetz/gardener/tree/hack/distribute-apiserver-requests" target="_blank" rel="noreferrer">https://github.com/oliver-goetz/gardener/tree/hack/distribute-apiserver-requests</a></p><h2 id="ğŸª´-validate-poc-for-in-place-node-updates-of-shoot-clusters" tabindex="-1">ğŸª´ Validate PoC For In-Place Node Updates Of Shoot Clusters <a class="header-anchor" href="#ğŸª´-validate-poc-for-in-place-node-updates-of-shoot-clusters" aria-label="Permalink to &quot;ğŸª´ Validate PoC For In-Place Node Updates Of Shoot Clusters&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> GEP-31 focuses on allowing updating Kubernetes minor versions and/or machine image versions without requiring the deletion and recreation of the nodes. This aims to minimize the overhead traditionally associated with node replacement, and it offers an alternative approach to updates, particularly important for physical machines or bare-metal nodes. There is already a proof-of-concept implementation, and the goal of this task was to validate it on a real infrastructure/system.</p><p><strong>Motivation/Benefits</strong>: âœ¨ Support more use cases/scenarios.</p><p><strong>Achievements:</strong> After fixing a few missing pieces in the PoC implementation that have to be incorporated into the work branch, we achieved performing an in-place update for a Kubernetes minor version change on a bare-metal infrastructure. It can be configured per worker pool with a new <code>updateStrategy</code> field.</p><p><strong>Next Steps:</strong> Unfortunately, we were lacking a functional version of Gardenlinux in order to test the in-place updates for OS versions. Hence, this part is still open and should be validated once an image is available.</p><p><strong>Issue:</strong> <a href="https://github.com/gardener/gardener/issues/10219" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/issues/10219</a></p><p><strong>Code/Pull Requests:</strong> <a href="https://github.com/gardener/gardener/pull/10828" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/pull/10828</a></p><h2 id="ğŸš€-prevent-pod-scheduling-issues-due-to-overscaling" tabindex="-1">ğŸš€ Prevent <code>Pod</code> Scheduling Issues Due To Overscaling <a class="header-anchor" href="#ğŸš€-prevent-pod-scheduling-issues-due-to-overscaling" aria-label="Permalink to &quot;ğŸš€ Prevent `Pod` Scheduling Issues Due To Overscaling&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> The Vertical Pod Autoscaler sometimes recommends resource requirements exceeding the allocatable resources of the largest nodes. This results in some pods becoming unschedulable (which eventually can result in downtimes). The goal is to prevent this from happening, thus ensuring properly running pods.</p><p><strong>Motivation/Benefits</strong>: ğŸ”§ Reduced operational complexity, âœ¨ support more use cases/scenarios.</p><p><strong>Achievements:</strong> We came up with three different approaches, but all of them have their drawbacks. Details about them can be read in the linked <code>hackmd.io</code> document. We decided to try augmenting the upstream VPA implementation with global &quot;max allowed&quot; flags for <code>vpa-recommender</code>. In addition, we prepared the needed changes in Gardener to consume it.</p><p><strong>Next Steps:</strong> Work with the autoscaling community to make sure the proposed pull requests gets accepted and merged.</p><p><strong>Issue:</strong> <a href="https://hackmd.io/GwTNubtZTg-D1mNhzV5VOw" target="_blank" rel="noreferrer">https://hackmd.io/GwTNubtZTg-D1mNhzV5VOw</a></p><p><strong>Code/Pull Requests:</strong> <a href="https://github.com/kubernetes/autoscaler/pull/7560" target="_blank" rel="noreferrer">https://github.com/kubernetes/autoscaler/pull/7560</a>, <a href="https://github.com/ialidzhikov/gardener/commits/enh/seed-and-shoot-vpa-max-allowed" target="_blank" rel="noreferrer">https://github.com/ialidzhikov/gardener/commits/enh/seed-and-shoot-vpa-max-allowed</a>, <a href="https://github.com/gardener/gardener/pull/10413" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/pull/10413</a></p><h2 id="ğŸ’ªğŸ»-prevent-multiple-systemd-unit-restarts-on-reconciliation-errors" tabindex="-1">ğŸ’ªğŸ» Prevent Multiple <code>systemd</code> Unit Restarts On Reconciliation Errors <a class="header-anchor" href="#ğŸ’ªğŸ»-prevent-multiple-systemd-unit-restarts-on-reconciliation-errors" aria-label="Permalink to &quot;ğŸ’ªğŸ» Prevent Multiple `systemd` Unit Restarts On Reconciliation Errors&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> Restarting <code>systemd</code> units, especially the <code>kubelet.service</code> unit, can be quite costly/counter-productive. When <code>gardener-node-agent</code> fails during its reconciliation of files and units on the worker nodes, it starts again from scratch and potentially restarts units multiple times until the entire reconciliation loop finally succeeds.</p><p><strong>Motivation/Benefits</strong>: ğŸ¤© Improved user experience.</p><p><strong>Achievements:</strong> Instead of only persisting its last applied <code>OperatingSystemConfig</code> specification at the end of a successful reconciliation, <code>gardener-node-agent</code> now updates its state immediately after the reconciliation of a file or unit. This way, it doesn&#39;t perform such changes again if the reconciliation fails in the meantime.</p><p><strong>Issue:</strong> <a href="https://github.com/gardener/gardener/issues/10972" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/issues/10972</a></p><p><strong>Code/Pull Requests:</strong> <a href="https://github.com/gardener/gardener/pull/10969" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/pull/10969</a></p><h2 id="ğŸ¤¹â€â™‚ï¸-trigger-nodes-rollout-individually-per-worker-pool-during-credentials-rotation" tabindex="-1">ğŸ¤¹â€â™‚ï¸ Trigger Nodes Rollout Individually Per Worker Pool During Credentials Rotation <a class="header-anchor" href="#ğŸ¤¹â€â™‚ï¸-trigger-nodes-rollout-individually-per-worker-pool-during-credentials-rotation" aria-label="Permalink to &quot;ğŸ¤¹â€â™‚ï¸ Trigger Nodes Rollout Individually Per Worker Pool During Credentials Rotation&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> Today, triggering a shoot cluster credentials rotation results in an immediate rollout of all worker nodes in the cluster. Some scenarios require to have more control over this rollout, particularly the option to trigger it individually per worker pool at different times.</p><p><strong>Motivation/Benefits</strong>: âœ¨ Support more use cases/scenarios, ğŸ—ï¸ lift restrictions.</p><p><strong>Achievements:</strong> A concept has been developed, proposed, and implemented in a prototype. The idea is to introduce new, special operation annotation for starting a credentials rotation without triggering worker poll rollouts. The user can then roll the nodes (via another operation annotation) at their convenience. Once all pools have been rolled out, the rotation can then be completed as usual.</p><p><strong>Next Steps:</strong> The PoC looks promising, yet the work has to be finished (tests, feature gate, documentation still missing) before a pull request has been opened.</p><p><strong>Issue:</strong> <a href="https://github.com/gardener/gardener/issues/10121#issuecomment-2515479242" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/issues/10121</a></p><p><strong>Code/Pull Requests:</strong> <a href="https://github.com/rfranzke/gardener/tree/individual-node-rollout-rotation" target="_blank" rel="noreferrer">https://github.com/rfranzke/gardener/tree/individual-node-rollout-rotation</a></p><h2 id="â›“ï¸â€ğŸ’¥-e2e-test-skeleton-for-autonomous-shoot-clusters" tabindex="-1">â›“ï¸â€ğŸ’¥ E2E Test Skeleton For Autonomous Shoot Clusters <a class="header-anchor" href="#â›“ï¸â€ğŸ’¥-e2e-test-skeleton-for-autonomous-shoot-clusters" aria-label="Permalink to &quot;â›“ï¸â€ğŸ’¥ E2E Test Skeleton For Autonomous Shoot Clusters&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> GEP-28 proposes to augment Gardener functionality with support for managing autonomous shoot clusters. The GEP has been merged a while ago, and we also crafted a skeleton for the to-be-developed <code>gardenadm</code> binary already. However, the needed e2e test infrastructure is missing and should be established before the implementation of the business logic is started.</p><p><strong>Motivation/Benefits</strong>: ğŸ‘¨ğŸ¼â€ğŸ’» Improved developer productivity.</p><p><strong>Achievements:</strong> We introduced new <code>make</code> targets to create a local development setup based on KinD and machine pods (like for regular shoot clusters). The <code>gardenadm</code> binary is build and transported to these machine pods. New e2e tests in <code>gardener/gardener</code> execute the various commands of <code>gardenadm</code> and assert that it behaves as expected.</p><p><strong>Issue:</strong> <a href="https://github.com/gardener/gardener/issues/2906" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/issues/2906</a></p><p><strong>Code/Pull Requests:</strong> <a href="https://github.com/gardener/gardener/pull/10977" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/pull/10977</a>, <a href="https://github.com/gardener/ci-infra/pull/2827" target="_blank" rel="noreferrer">https://github.com/gardener/ci-infra/pull/2827</a></p><h2 id="â¬†ï¸-deploy-prow-via-flux" tabindex="-1">â¬†ï¸ Deploy Prow Via Flux <a class="header-anchor" href="#â¬†ï¸-deploy-prow-via-flux" aria-label="Permalink to &quot;â¬†ï¸ Deploy Prow Via Flux&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> Prow is a system for Gardener&#39;s CI and automation. So far, it was deployed using manually crafted deployment scripts. Switching to Flux, which is a cloud-native solution for continuous delivery based on GitOps, allows to eliminate these scripts and implement industry best-practises.</p><p><strong>Motivation/Benefits</strong>: ğŸ‘¨ğŸ¼â€ğŸ’» Improved developer productivity.</p><p><strong>Achievements:</strong> The custom deployment script has been almost eliminated and replaced with Kustomizations and Helm releases that are managed by Flux.</p><p><strong>Next Steps:</strong> The monitoring configurations for the Prow cluster still need to be migrated. However, we prefer to create a new deployment from scratch (since we are not too happy with the current solution, i.e., we don&#39;t want to just &quot;migrate&quot; it over to Flux but use the opportunity to renovate it).</p><p><strong>Code/Pull Requests:</strong> <a href="https://github.com/gardener/ci-infra/pull/2812" target="_blank" rel="noreferrer">https://github.com/gardener/ci-infra/pull/2812</a>, <a href="https://github.com/gardener/ci-infra/pull/2813" target="_blank" rel="noreferrer">https://github.com/gardener/ci-infra/pull/2813</a>, <a href="https://github.com/gardener/ci-infra/pull/2814" target="_blank" rel="noreferrer">https://github.com/gardener/ci-infra/pull/2814</a>, <a href="https://github.com/gardener/ci-infra/pull/2816" target="_blank" rel="noreferrer">https://github.com/gardener/ci-infra/pull/2816</a>, <a href="https://github.com/gardener/ci-infra/pull/2817" target="_blank" rel="noreferrer">https://github.com/gardener/ci-infra/pull/2817</a>, <a href="https://github.com/gardener/ci-infra/pull/2818" target="_blank" rel="noreferrer">https://github.com/gardener/ci-infra/pull/2818</a>, and many more... ğŸ˜‰</p><h2 id="ğŸš-replace-topologyawarehints-with-servicetrafficdistribution" tabindex="-1">ğŸš Replace <code>TopologyAwareHints</code> With <code>ServiceTrafficDistribution</code> <a class="header-anchor" href="#ğŸš-replace-topologyawarehints-with-servicetrafficdistribution" aria-label="Permalink to &quot;ğŸš Replace `TopologyAwareHints` With `ServiceTrafficDistribution`&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> The <code>TopologyAwareHints</code> uses allocatable CPUs for distributing the traffic according to the configured hints. This forced us to introduce custom code (webhook on <code>EndpointSlice</code>s) in Gardener to mitigate this limitation. The new <code>ServiceTrafficDistribution</code> is just using the availability zone without taking into account the CPUs, hence it would eventually allow us to get rid of our custom code.</p><p><strong>Motivation/Benefits</strong>: ğŸ‘¨ğŸ¼â€ğŸ’» Improved developer productivity.</p><p><strong>Achievements:</strong> Kubernetes clusters of version <code>v1.31</code> and higher now use the new <code>ServiceTrafficDistribution</code> feature. Yet, until this is our lowest supported Kubernetes version, we have to keep our custom code (to also support older versions).</p><p><strong>Issue:</strong> <a href="https://github.com/gardener/gardener/issues/10421" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/issues/10421</a></p><p><strong>Code/Pull Requests:</strong> <a href="https://github.com/gardener/gardener/pull/10973" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/pull/10973</a></p><h2 id="ğŸªª-support-more-use-cases-for-tokenrequestor" tabindex="-1">ğŸªª Support More Use-Cases For <code>TokenRequestor</code> <a class="header-anchor" href="#ğŸªª-support-more-use-cases-for-tokenrequestor" aria-label="Permalink to &quot;ğŸªª Support More Use-Cases For `TokenRequestor`&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> Currently, <code>gardener-resource-manager</code>&#39;s <code>TokenRequestor</code> controller is not capable of injecting the current CA bundle into access secrets. This requires operators to use the generic token kubeconfig, however, this is not possible in all use cases. For example, Flux needs to read a <code>Secret</code> with a self-contained kubeconfig and cannot mount it. Hence, operators had to work around this and manually fetch the CA bundle first before crafting the kubeconfig for the controller. Another use-case is to make it watch additional namespaces.</p><p><strong>Motivation/Benefits</strong>: ğŸ‘¨ğŸ¼â€ğŸ’» Improved developer productivity, âœ¨ support more use cases/scenarios.</p><p><strong>Achievements:</strong> Access secrets annotated with <code>serviceaccount.resources.gardener.cloud/inject-ca-bundle</code> now get either a new <code>.data.&quot;bundle.crt&quot;</code> field with the CA bundle. When there is a kubeconfig, then the CA bundle will be injected directly into the server configuration. This enables new features and allows operators to get rid of workarounds and technical debt.</p><p><strong>Code/Pull Requests:</strong> <a href="https://github.com/gardener/gardener/pull/10988" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/pull/10988</a></p><h2 id="ğŸ«„-cluster-autoscaler-s-provisioningrequest-api" tabindex="-1">ğŸ«„ <code>cluster-autoscaler</code>&#39;s <code>ProvisioningRequest</code> API <a class="header-anchor" href="#ğŸ«„-cluster-autoscaler-s-provisioningrequest-api" aria-label="Permalink to &quot;ğŸ«„ `cluster-autoscaler`&#39;s `ProvisioningRequest` API&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> The <code>cluster-autoscaler</code> community introduced the new <code>ProvisioningRequest</code> API. This enables users to provide a pod template and ask the autoscaler to either provision a new node or to get to know whether the pod would fit in the existing cluster without doing any scale-up. The new API can help users to ensure all-or-nothing semantics. Without this API, users have to create the pod which might remain stuck in the <code>Pending</code> state.</p><p><strong>Motivation/Benefits</strong>: âœ¨ Support more use cases/scenarios.</p><p><strong>Achievements:</strong> A new field in the <code>Shoot</code> specification has been added to enable users to activate this API.</p><p><strong>Next Steps:</strong> Finalize unit/integration tests and open the pull request for review.</p><p><strong>Issue:</strong> <a href="https://github.com/gardener/gardener/issues/10962" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/issues/10962</a></p><p><strong>Code/Pull Requests:</strong> <a href="https://github.com/tobschli/gardener/tree/ca-provisioning-api" target="_blank" rel="noreferrer">https://github.com/tobschli/gardener/tree/ca-provisioning-api</a></p><h2 id="ğŸ‘€-watch-managedresources-in-shoot-care-controller" tabindex="-1">ğŸ‘€ Watch <code>ManagedResource</code>s In <code>Shoot</code> Care Controller <a class="header-anchor" href="#ğŸ‘€-watch-managedresources-in-shoot-care-controller" aria-label="Permalink to &quot;ğŸ‘€ Watch `ManagedResource`s In `Shoot` Care Controller&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> The <code>Shoot</code> care controller performs regular health checks for various aspects of the shoot cluster. These are ran periodically based on configuration. This approach has the downside that failing conditions will not be reported in the meantime, i.e., they have to wait for the next sync period before getting visible in the <code>Shoot</code> status.</p><p><strong>Motivation/Benefits</strong>: ğŸ”§ Reduced operational complexity, ğŸ¤© improved user experience.</p><p><strong>Achievements:</strong> Similar to <a href="https://github.com/gardener-community/hackathon/blob/main/2022-09_Hirschegg/README.md#resource-manager-health-check-watches" target="_blank" rel="noreferrer">this previous effort in the 2022/09 Hackathon</a>, we introduced a WATCH for <code>ManagedResource</code> in the controller which causes a re-evaluation of the health checks once a relevant condition has changed. This way, the <code>Shoot</code> status reflects health changes immediately.</p><p><strong>Code/Pull Requests:</strong> <a href="https://github.com/gardener/gardener/pull/10987" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/pull/10987</a></p><h2 id="ğŸ¢-cluster-api-provider-for-gardener" tabindex="-1">ğŸ¢ Cluster API Provider For Gardener <a class="header-anchor" href="#ğŸ¢-cluster-api-provider-for-gardener" aria-label="Permalink to &quot;ğŸ¢ Cluster API Provider For Gardener&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> It is a common/regular ask of users to use the community-standard APIs for cluster management. While this is generally questionable (see &lt;/docs/gardener/concepts/cluster-api/&gt;), this request is reoccurring every now and then. It could help adoption if Gardener supported the cluster API, even if it is just about forwarding the data to the actual Gardener API server.</p><p><strong>Motivation/Benefits</strong>: âœ¨ Support more use cases/scenarios.</p><p><strong>Achievements:</strong> It is possible to deploy or delete a shoot cluster via the cluster API (the manifest is &quot;just forwarded&quot; to the Gardener API). There are still some rough edges, but the general approach seems to work (i.e., we haven&#39;t discovered technical dead-ends). Yet, it is unclear what the benefit of using this is since it&#39;s just proxying it down.</p><p><strong>Next Steps:</strong> There are still quite a few open points before this should be used productively. We have to decide how to continue this effort.</p><p><strong>Issue:</strong> /docs/gardener/concepts/cluster-api/</p><p><strong>Code/Pull Requests:</strong> <a href="https://github.com/metal-stack/cluster-api-provider-gardener" target="_blank" rel="noreferrer">https://github.com/metal-stack/cluster-api-provider-gardener</a></p><h2 id="ğŸ‘¨ğŸ¼â€ğŸ’»-make-cluster-autoscaler-work-in-local-setup" tabindex="-1">ğŸ‘¨ğŸ¼â€ğŸ’» Make <code>cluster-autoscaler</code> Work In Local Setup <a class="header-anchor" href="#ğŸ‘¨ğŸ¼â€ğŸ’»-make-cluster-autoscaler-work-in-local-setup" aria-label="Permalink to &quot;ğŸ‘¨ğŸ¼â€ğŸ’» Make `cluster-autoscaler` Work In Local Setup&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> Currently, the <code>cluster-autoscaler</code> is not working in the local setup because we were not setting the <code>nodeTemplate</code> in the <code>MachineClass</code> for the <code>cluster-autoscaler</code> to get to know about the resource capacity of the nodes. In addition, the <code>Node</code> object reported false data for allocatable resources. It also missed a provider ID which prevented <code>cluster-autoscaler</code> to associate the <code>Node</code> with a <code>Machine</code> resource.</p><p><strong>Motivation/Benefits</strong>: ğŸ‘¨ğŸ¼â€ğŸ’» Improved developer productivity.</p><p><strong>Achievements:</strong> We crafted a custom <code>cloud-controller-manager</code> for <code>provider-local</code> such that the provider ID can be populated into the <code>Node</code> object. In addition, we properly populate the <code>nodeTemplate</code> in the <code>MachineClass</code> now.</p><p><strong>Next Steps:</strong> We still have to figure out how to make the <code>Node</code> report its allocatable resources properly.</p><p><strong>Code/Pull Requests:</strong> <a href="https://github.com/ialidzhikov/cloud-provider-local" target="_blank" rel="noreferrer">https://github.com/ialidzhikov/cloud-provider-local</a>, <a href="https://github.com/ialidzhikov/gardener/tree/fix/cluster-autoscaler-provider-local" target="_blank" rel="noreferrer">https://github.com/ialidzhikov/gardener/tree/fix/cluster-autoscaler-provider-local</a></p><h2 id="ğŸ§¹-use-structured-authorization-in-local-kind-cluster" tabindex="-1">ğŸ§¹ Use Structured Authorization In Local KinD Cluster <a class="header-anchor" href="#ğŸ§¹-use-structured-authorization-in-local-kind-cluster" aria-label="Permalink to &quot;ğŸ§¹ Use Structured Authorization In Local KinD Cluster&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> Due to changes in <code>kubeadm</code>, we had to introduce a workaround for enabling the seed authorizer in the local KinD clusters. This slows down the creation of the cluster and hence all e2e tests.</p><p><strong>Motivation/Benefits</strong>: ğŸ‘¨ğŸ¼â€ğŸ’» Improved developer productivity.</p><p><strong>Achievements:</strong> With the new <a href="https://kubernetes.io/docs/reference/access-authn-authz/authorization/#using-configuration-file-for-authorization" target="_blank" rel="noreferrer">Structured Authorization</a> feature of Kubernetes, we were able to get rid of the workaround and speed up the KinD cluster creation dramatically.</p><p><strong>Issue:</strong> <a href="https://github.com/gardener/gardener/issues/10421" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/issues/10421</a></p><p><strong>Code/Pull Requests:</strong> <a href="https://github.com/gardener/gardener/pull/10984" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/pull/10984</a></p><h2 id="ğŸ§¹-drop-internal-versions-from-component-configuration-apis" tabindex="-1">ğŸ§¹ Drop Internal Versions From Component Configuration APIs <a class="header-anchor" href="#ğŸ§¹-drop-internal-versions-from-component-configuration-apis" aria-label="Permalink to &quot;ğŸ§¹ Drop Internal Versions From Component Configuration APIs&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> In order to follow the same approaches like for regular Gardener APIs, we currently maintain an internal and an external version of the component configurations. However, we concluded that the internal version actually has no benefit and just causes more maintenance effort during development.</p><p><strong>Motivation/Benefits</strong>: ğŸ‘¨ğŸ¼â€ğŸ’» Improved developer productivity.</p><p><strong>Achievements:</strong> We started working on removing the internal version, but due to its wide-spread use in the code base, we weren&#39;t able to finish it yet.</p><p><strong>Next Steps:</strong> The effort has to be continued after the Hackathon such that a pull requests can be opened.</p><p><strong>Issue:</strong> <a href="https://github.com/gardener/gardener/issues/11043" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/issues/11043</a></p><p><strong>Code/Pull Requests:</strong> <a href="https://github.com/timebertt/gardener/tree/config-apis" target="_blank" rel="noreferrer">https://github.com/timebertt/gardener/tree/config-apis</a></p><h2 id="ğŸ›-fix-non-functional-shoot-node-logging-in-local-setup" tabindex="-1">ğŸ› Fix Non-Functional Shoot Node Logging In Local Setup <a class="header-anchor" href="#ğŸ›-fix-non-functional-shoot-node-logging-in-local-setup" aria-label="Permalink to &quot;ğŸ› Fix Non-Functional Shoot Node Logging In Local Setup&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> We discovered that the shoot node logging was not working in the local development setup for quite a while.</p><p><strong>Motivation/Benefits</strong>: ğŸ‘¨ğŸ¼â€ğŸ’» Improved developer productivity.</p><p><strong>Achievements:</strong> We have identified the reason for the problem (it probably broke when we introduced <code>NetworkPolicy</code>s in the <code>garden</code> namespace). In the meantime, <code>Ingress</code>es (like the Vali which gets the shoot node logs) are exposed via Istio. Yet, in the local setup the traffic was still sent to the <code>nginx-ingress-controller</code> for which the network path was blocked. By switching it to Istio, we were able to fix the problem.</p><p><strong>Issue:</strong> <a href="https://github.com/gardener/gardener/issues/10916" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/issues/10916</a></p><p><strong>Code/Pull Requests:</strong> <a href="https://github.com/gardener/gardener/pull/10991" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/pull/10991</a></p><h2 id="ğŸ§¹-no-longer-generate-empty-secret-for-reconcile-operatingsystemconfigs" tabindex="-1">ğŸ§¹ No Longer Generate Empty <code>Secret</code> For <code>reconcile</code> <code>OperatingSystemConfig</code>s <a class="header-anchor" href="#ğŸ§¹-no-longer-generate-empty-secret-for-reconcile-operatingsystemconfigs" aria-label="Permalink to &quot;ğŸ§¹ No Longer Generate Empty `Secret` For `reconcile` `OperatingSystemConfig`s&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> After the introduction of <code>gardener-node-agent</code>, the <code>OperatingSystemConfig</code> controller no longer needs to generate a <code>Secret</code> when the <code>purpose</code> is <code>reconcile</code>. Yet, it was still doing this, effectively creating an empty <code>Secret</code> which was not used at all (and just &quot;polluted&quot; the system).</p><p><strong>Motivation/Benefits</strong>: ğŸ‘¨ğŸ¼â€ğŸ’» Improved developer productivity.</p><p><strong>Achievements:</strong> <code>gardenlet</code> has been adapted to no longer read the <code>Secret</code>, and the respective creation in the <code>OperatingSystemConfig</code> has been deprecated. It can be removed after a few releases to ensure backwards-compatibility.</p><p><strong>Next Steps:</strong> Wait until the deprecation period has expired, and then finally cleanup the <code>Secret</code> generation from the <code>OperatingSystemConfig</code> controller.</p><p><strong>Code/Pull Requests:</strong> <a href="https://github.com/rfranzke/gardener/tree/osc-controller-cloudconfig-secret" target="_blank" rel="noreferrer">https://github.com/rfranzke/gardener/tree/osc-controller-cloudconfig-secret</a></p><h2 id="ğŸ–¥ï¸-generic-monitoring-extension" tabindex="-1">ğŸ–¥ï¸ Generic Monitoring Extension <a class="header-anchor" href="#ğŸ–¥ï¸-generic-monitoring-extension" aria-label="Permalink to &quot;ğŸ–¥ï¸ Generic Monitoring Extension&quot;">â€‹</a></h2><p><strong>Problem Statement:</strong> Since quite a while, we know that different parties/users of Gardener have different requirements w.r.t. monitoring (or observability in general). While a few things have been made configurable in the past, many others have not. This required users to write custom extensions that manipulate configuration. This approach has some limitations and cannot properly realize all use-cases, i.e., we need a better way of making them work without introducing more technical debt.</p><p><strong>Motivation/Benefits</strong>: ğŸ‘¨ğŸ¼â€ğŸ’» Improved developer productivity.</p><p><strong>Achievements:</strong> We talked about the requirements and brain-stormed how/whether the monitoring aspect of Gardener can be externalized. We decided that it&#39;s best to write them down in a clear way so that they can be used as input in follow-up discussions.</p><p><strong>Next Steps:</strong> Discuss the requirements with the monitoring experts of all affected parties and align on an approach that works for most use-cases.</p><p><strong>Issue:</strong> <a href="https://github.com/gardener/gardener/issues/10985" target="_blank" rel="noreferrer">https://github.com/gardener/gardener/issues/10985</a></p><p><img src="https://apeirora.eu/assets/img/BMWK-EU.png" alt="ApeiroRA"></p>',146)]))}const m=t(s,[["render",i]]);export{g as __pageData,m as default};
