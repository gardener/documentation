import{_ as i,c as s,o as t,a2 as a}from"./chunks/framework.Bfq10Vlj.js";const r="/assets/shoot-cluster-without-registry-cache.BZRGYzGQ.png",o="/assets/shoot-cluster-with-registry-cache.CTRlYJaQ.png",k=JSON.parse('{"title":"Configuring the Registry Cache Extension","description":"Learn what is the use-case for a pull-through cache, how to enable it and configure it","frontmatter":{"description":"Learn what is the use-case for a pull-through cache, how to enable it and configure it","github_repo":"https://github.com/gardener/gardener-extension-registry-cache","github_subdir":"docs/usage/registry-cache","params":{"github_branch":"main"},"path_base_for_github_subdir":{"from":"content/docs/extensions/others/gardener-extension-registry-cache/registry-cache/configuration.md","to":"configuration.md"},"persona":"Users","title":"Configuring the Registry Cache Extension","prev":false,"next":false},"headers":[],"relativePath":"docs/extensions/others/gardener-extension-registry-cache/registry-cache/configuration/index.md","filePath":"docs/extensions/others/gardener-extension-registry-cache/registry-cache/configuration.md","lastUpdated":null}'),n={name:"docs/extensions/others/gardener-extension-registry-cache/registry-cache/configuration/index.md"};function h(l,e,c,d,p,g){return t(),s("div",null,e[0]||(e[0]=[a('<h1 id="configuring-the-registry-cache-extension" tabindex="-1">Configuring the Registry Cache Extension <a class="header-anchor" href="#configuring-the-registry-cache-extension" aria-label="Permalink to &quot;Configuring the Registry Cache Extension&quot;">​</a></h1><h2 id="introduction" tabindex="-1">Introduction <a class="header-anchor" href="#introduction" aria-label="Permalink to &quot;Introduction&quot;">​</a></h2><h3 id="use-case" tabindex="-1">Use Case <a class="header-anchor" href="#use-case" aria-label="Permalink to &quot;Use Case&quot;">​</a></h3><p>For a Shoot cluster, the containerd daemon of every Node goes to the internet and fetches an image that it doesn&#39;t have locally in the Node&#39;s image cache. New Nodes are often created due to events such as auto-scaling (scale up), rolling update, or replacement of unhealthy Node. Such a new Node would need to pull all of the images of the Pods running on it from the internet because the Node&#39;s cache is initially empty. Pulling an image from a registry produces network traffic and registry costs. To avoid these network traffic and registry costs, you can use the registry-cache extension to run a registry as pull-through cache.</p><p>The following diagram shows a rough outline of how an image pull looks like for a Shoot cluster <strong>without registry cache</strong>: <img src="'+r+'" alt="shoot-cluster-without-registry-cache"></p><h3 id="solution" tabindex="-1">Solution <a class="header-anchor" href="#solution" aria-label="Permalink to &quot;Solution&quot;">​</a></h3><p>The registry-cache extension deploys and manages a registry in the Shoot cluster that runs as pull-through cache. The used registry implementation is <a href="https://github.com/distribution/distribution" target="_blank" rel="noreferrer">distribution/distribution</a>.</p><h3 id="how-does-it-work" tabindex="-1">How does it work? <a class="header-anchor" href="#how-does-it-work" aria-label="Permalink to &quot;How does it work?&quot;">​</a></h3><p>When the extension is enabled, a registry cache for each configured upstream is deployed to the Shoot cluster. Along with this, the containerd daemon on the Shoot cluster Nodes gets configured to use as a mirror the Service IP address of the deployed registry cache. For example, if a registry cache for upstream <code>docker.io</code> is requested via the Shoot spec, then containerd gets configured to first pull the image from the deployed cache in the Shoot cluster. If this image pull operation fails, containerd falls back to the upstream itself (<code>docker.io</code> in that case).</p><p>The first time an image is requested from the pull-through cache, it pulls the image from the configured upstream registry and stores it locally, before handing it back to the client. On subsequent requests, the pull-through cache is able to serve the image from its own storage.</p><div class="note custom-block github-alert"><p class="custom-block-title">NOTE</p><p>The used registry implementation (<a href="https://github.com/distribution/distribution" target="_blank" rel="noreferrer">distribution/distribution</a>) supports mirroring of only one upstream registry.</p></div><p>The following diagram shows a rough outline of how an image pull looks like for a Shoot cluster <strong>with registry cache</strong>: <img src="'+o+`" alt="shoot-cluster-with-registry-cache"></p><h2 id="shoot-configuration" tabindex="-1">Shoot Configuration <a class="header-anchor" href="#shoot-configuration" aria-label="Permalink to &quot;Shoot Configuration&quot;">​</a></h2><p>The extension is not globally enabled and must be configured per Shoot cluster. The Shoot specification has to be adapted to include the <code>registry-cache</code> extension configuration.</p><p>Below is an example of <code>registry-cache</code> extension configuration as part of the Shoot spec:</p><div class="language-yaml vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">yaml</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">apiVersion</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">core.gardener.cloud/v1beta1</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">kind</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Shoot</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">metadata</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  name</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">crazy-botany</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  namespace</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">garden-dev</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">spec</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  extensions</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  - </span><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">registry-cache</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    providerConfig</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">      apiVersion</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">registry.extensions.gardener.cloud/v1alpha3</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">      kind</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">RegistryConfig</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">      caches</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">      - </span><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">upstream</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">docker.io</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        volume</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">          size</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">100Gi</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">          # storageClassName: premium</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">      - </span><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">upstream</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">ghcr.io</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">      - </span><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">upstream</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">quay.io</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        garbageCollection</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">          ttl</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">0s</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        secretReferenceName</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">quay-credentials</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">      - </span><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">upstream</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">my-registry.io:5000</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        remoteURL</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">http://my-registry.io:5000</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        serviceNameSuffix</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">static-name</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # ...</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  resources</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  - </span><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">name</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">quay-credentials</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    resourceRef</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">      apiVersion</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">v1</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">      kind</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Secret</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">      name</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">quay-credentials-v1</span></span></code></pre></div><p>The <code>providerConfig</code> field is required.</p><p>The <code>providerConfig.caches</code> field contains information about the registry caches to deploy. It is a required field. At least one cache has to be specified.</p><p>The <code>providerConfig.caches[].upstream</code> field is the remote registry host to cache. It is a required field. The value must be a valid DNS subdomain (RFC 1123) and optionally a port (i.e. <code>&lt;host&gt;[:&lt;port&gt;]</code>). It must not include a scheme.</p><p>The <code>providerConfig.caches[].remoteURL</code> optional field is the remote registry URL. If configured, it must include an <code>https://</code> or <code>http://</code> scheme. If the field is not configured, the remote registry URL defaults to <code>https://&lt;upstream&gt;</code>. In case the upstream is <code>docker.io</code>, it defaults to <code>https://registry-1.docker.io</code>.</p><p>The <code>providerConfig.caches[].volume</code> field contains settings for the registry cache volume. The registry-cache extension deploys a StatefulSet with a volume claim template. A PersistentVolumeClaim is created with the configured size and StorageClass name.</p><p>The <code>providerConfig.caches[].volume.size</code> field is the size of the registry cache volume. Defaults to <code>10Gi</code>. The size must be a positive quantity (greater than 0). This field is immutable. See <a href="/docs/extensions/others/gardener-extension-registry-cache/registry-cache/configuration/#increase-the-cache-disk-size">Increase the cache disk size</a> on how to resize the disk. The extension defines alerts for the volume. More information about the registry cache alerts and how to enable notifications for them can be found in the <a href="/docs/extensions/others/gardener-extension-registry-cache/registry-cache/observability/#alerts">alerts documentation</a>.</p><p>The <code>providerConfig.caches[].volume.storageClassName</code> field is the name of the StorageClass used by the registry cache volume. This field is immutable. If the field is not specified, then the <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#default-storageclass" target="_blank" rel="noreferrer">default StorageClass</a> will be used.</p><p>The <code>providerConfig.caches[].garbageCollection.ttl</code> field is the time to live of a blob in the cache. If the field is set to <code>0s</code>, the garbage collection is disabled. Defaults to <code>168h</code> (7 days). See the <a href="/docs/extensions/others/gardener-extension-registry-cache/registry-cache/configuration/#garbage-collection">Garbage Collection section</a> for more details.</p><p>The <code>providerConfig.caches[].secretReferenceName</code> is the name of the reference for the Secret containing the upstream registry credentials. To cache images from a private registry, credentials to the upstream registry should be supplied. For more details, see <a href="/docs/extensions/others/gardener-extension-registry-cache/registry-cache/upstream-credentials/#how-to-provide-credentials-for-upstream-registry">How to provide credentials for upstream registry</a>.</p><p>The <code>providerConfig.caches[].serviceNameSuffix</code> field allows to customize the naming of the deployed service after the <code>registry-</code> prefix. This is useful in scenarios where environment specific registries should be available under the same name across shoots.</p><div class="note custom-block github-alert"><p class="custom-block-title">NOTE</p><p>It is only possible to provide one set of credentials for one private upstream registry.</p></div><p>The <code>providerConfig.caches[].proxy.httpProxy</code> field represents the proxy server for HTTP connections which is used by the registry cache. It must include an <code>https://</code> or <code>http://</code> scheme.</p><p>The <code>providerConfig.caches[].proxy.httpsProxy</code> field represents the proxy server for HTTPS connections which is used by the registry cache. It must include an <code>https://</code> or <code>http://</code> scheme.</p><p>The <code>providerConfig.caches[].http.tls</code> field indicates whether TLS is enabled for the HTTP server of the registry cache. Defaults to <code>true</code>.</p><p>The <code>providerConfig.caches[].highAvailability.enabled</code> defines if the registry cache is scaled with the <a href="/docs/gardener/high-availability-of-components/#system-components">high availability feature</a>. See the <a href="/docs/extensions/others/gardener-extension-registry-cache/registry-cache/configuration/#high-availability">High Availability section</a> for more details.</p><h2 id="garbage-collection" tabindex="-1">Garbage Collection <a class="header-anchor" href="#garbage-collection" aria-label="Permalink to &quot;Garbage Collection&quot;">​</a></h2><p>When the registry cache receives a request for an image that is not present in its local store, it fetches the image from the upstream, returns it to the client and stores the image in the local store. The registry cache runs a scheduler that deletes images when their time to live (ttl) expires. When adding an image to the local store, the registry cache also adds a time to live for the image. The ttl defaults to <code>168h</code> (7 days) and is configurable. The garbage collection can be disabled by setting the ttl to <code>0s</code>. Requesting an image from the registry cache does not extend the time to live of the image. Hence, an image is always garbage collected from the registry cache store when its ttl expires. At the time of writing this document, there is no functionality for garbage collection based on disk size - e.g., garbage collecting images when a certain disk usage threshold is passed. The garbage collection cannot be enabled once it is disabled. This constraint is added to mitigate <a href="https://github.com/distribution/distribution/issues/4249" target="_blank" rel="noreferrer">distribution/distribution#4249</a>.</p><h2 id="increase-the-cache-disk-size" tabindex="-1">Increase the Cache Disk Size <a class="header-anchor" href="#increase-the-cache-disk-size" aria-label="Permalink to &quot;Increase the Cache Disk Size&quot;">​</a></h2><p>When there is no available disk space, the registry cache continues to respond to requests. However, it cannot store the remotely fetched images locally because it has no free disk space. In such case, it is simply acting as a proxy without being able to cache the images in its local store. The disk has to be resized to ensure that the registry cache continues to cache images.</p><p>There are two alternatives to enlarge the cache&#39;s disk size:</p><h3 id="alternative-1-resize-the-pvc" tabindex="-1">[Alternative 1] Resize the PVC <a class="header-anchor" href="#alternative-1-resize-the-pvc" aria-label="Permalink to &quot;[Alternative 1] Resize the PVC&quot;">​</a></h3><p>To enlarge the PVC&#39;s size, perform the following steps:</p><ol><li><p>Make sure that the <code>KUBECONFIG</code> environment variable is targeting the correct Shoot cluster.</p></li><li><p>Find the PVC name to resize for the desired upstream. The below example fetches the PVC for the <code>docker.io</code> upstream:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">kubectl</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> kube-system</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> get</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> pvc</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -l</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> upstream-host=docker.io</span></span></code></pre></div></li><li><p>Patch the PVC&#39;s size to the desired size. The below example patches the size of a PVC to <code>10Gi</code>:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">kubectl</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> kube-system</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> patch</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> pvc</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> $PVC_NAME </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">--type</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> merge</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -p</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;{&quot;spec&quot;:{&quot;resources&quot;:{&quot;requests&quot;: {&quot;storage&quot;: &quot;10Gi&quot;}}}}&#39;</span></span></code></pre></div></li><li><p>Make sure that the PVC gets resized. Describe the PVC to check the resize operation result:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">kubectl</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> kube-system</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> describe</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> pvc</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -l</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> upstream-host=docker.io</span></span></code></pre></div></li></ol><blockquote><p>Drawback of this approach: The cache&#39;s size in the Shoot spec (<code>providerConfig.caches[].size</code>) diverges from the PVC&#39;s size.</p></blockquote><h3 id="alternative-2-remove-and-readd-the-cache" tabindex="-1">[Alternative 2] Remove and Readd the Cache <a class="header-anchor" href="#alternative-2-remove-and-readd-the-cache" aria-label="Permalink to &quot;[Alternative 2] Remove and Readd the Cache&quot;">​</a></h3><p>There is always the option to remove the cache from the Shoot spec and to readd it again with the updated size.</p><blockquote><p>Drawback of this approach: The already cached images get lost and the cache starts with an empty disk.</p></blockquote><h2 id="high-availability" tabindex="-1">High Availability <a class="header-anchor" href="#high-availability" aria-label="Permalink to &quot;High Availability&quot;">​</a></h2><p>By default the registry cache runs with a single replica. This fact may lead to concerns for the high availability such as &quot;What happens when the registry cache is down? Does containerd fail to pull the image?&quot;. As outlined in the <a href="/docs/extensions/others/gardener-extension-registry-cache/registry-cache/configuration/#how-does-it-work">How does it work? section</a>, containerd is configured to fall back to the upstream registry if it fails to pull the image from the registry cache. Hence, when the registry cache is unavailable, the containerd&#39;s image pull operations are not affected because containerd falls back to image pull from the upstream registry.</p><p>In special cases where this is not enough it is possible to set <code>providerConfig.caches[].highAvailability.enabled</code> to <code>true</code>. This will add the label <code>high-availability-config.resources.gardener.cloud/type=server</code> to the StatefulSet and it will be scaled to 2 replicas. Appropriate <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/" target="_blank" rel="noreferrer">Pod Topology Spread Constraints</a> will be added to the registry cache Pods according to the Shoot cluster configuration. See also <a href="/docs/gardener/high-availability-of-components/#system-components">High Availability of Deployed Components</a>. Pay attention that each registry cache replica uses its own volume, so each registry cache pulls the image from the upstream and stores it in its volume.</p><h2 id="possible-pitfalls" tabindex="-1">Possible Pitfalls <a class="header-anchor" href="#possible-pitfalls" aria-label="Permalink to &quot;Possible Pitfalls&quot;">​</a></h2><ul><li>The used registry implementation (the <a href="https://github.com/distribution/distribution" target="_blank" rel="noreferrer">Distribution project</a>) supports mirroring of only one upstream registry. The extension deploys a pull-through cache for each configured upstream.</li><li><code>us-docker.pkg.dev</code>, <code>europe-docker.pkg.dev</code>, and <code>asia-docker.pkg.dev</code> are different upstreams. Hence, configuring <code>pkg.dev</code> as upstream won&#39;t cache images from <code>us-docker.pkg.dev</code>, <code>europe-docker.pkg.dev</code>, or <code>asia-docker.pkg.dev</code>.</li></ul><h2 id="limitations" tabindex="-1">Limitations <a class="header-anchor" href="#limitations" aria-label="Permalink to &quot;Limitations&quot;">​</a></h2><ol><li><p>Images that are pulled before a registry cache Pod is running or before a registry cache Service is reachable from the corresponding Node won&#39;t be cached - containerd will pull these images directly from the upstream.</p><p>The reasoning behind this limitation is that a registry cache Pod is running in the Shoot cluster. To have a registry cache&#39;s Service cluster IP reachable from containerd running on the Node, the registry cache Pod has to be running and kube-proxy has to configure iptables/IPVS rules for the registry cache Service. If kube-proxy hasn&#39;t configured iptables/IPVS rules for the registry cache Service, then the image pull times (and new Node bootstrap times) will be increased significantly. For more detailed explanations, see point 2. and <a href="https://github.com/gardener/gardener-extension-registry-cache/pull/68" target="_blank" rel="noreferrer">gardener/gardener-extension-registry-cache#68</a>.</p><p>That&#39;s why the registry configuration on a Node is applied only after the registry cache Service is reachable from the Node. The <code>gardener-node-agent.service</code> systemd unit sends requests to the registry cache&#39;s Service. Once the registry cache responds with <code>HTTP 200</code>, the unit creates the needed registry configuration file (<code>hosts.toml</code>).</p><p>As a result, for images from Shoot system components:</p><ul><li>On Shoot creation with the registry cache extension enabled, a registry cache is unable to cache all of the images from the Shoot system components. Usually, until the registry cache Pod is running, containerd pulls from upstream the images from Shoot system components (before the registry configuration gets applied).</li><li>On new Node creation for existing Shoot with the registry cache extension enabled, a registry cache is unable to cache most of the images from Shoot system components. The reachability of the registry cache Service requires the Service network to be set up, i.e., the kube-proxy for that new Node to be running and to have set up iptables/IPVS configuration for the registry cache Service.</li></ul></li><li><p>containerd requests will time out in 30s in case kube-proxy hasn&#39;t configured iptables/IPVS rules for the registry cache Service - the image pull times will increase significantly.</p><p>containerd is configured to fall back to the upstream itself if a request against the cache fails. However, if the cluster IP of the registry cache Service does not exist or if kube-proxy hasn&#39;t configured iptables/IPVS rules for the registry cache Service, then containerd requests against the registry cache time out in 30 seconds. This significantly increases the image pull times because containerd does multiple requests as part of the image pull (HEAD request to resolve the manifest by tag, GET request for the manifest by SHA, GET requests for blobs)</p><p>Example: If the Service of a registry cache is deleted, then a new Service will be created. containerd&#39;s registry config will still contain the old Service&#39;s cluster IP. containerd requests against the old Service&#39;s cluster IP will time out and containerd will fall back to upstream.</p><ul><li>Image pull of <code>docker.io/library/alpine:3.13.2</code> from the upstream takes ~2s while image pull of the same image with invalid registry cache cluster IP takes ~2m.2s.</li><li>Image pull of <code>eu.gcr.io/gardener-project/gardener/ops-toolbelt:0.18.0</code> from the upstream takes ~10s while image pull of the same image with invalid registry cache cluster IP takes ~3m.10s.</li></ul></li><li><p>Amazon Elastic Container Registry is currently not supported. For details see <a href="https://github.com/distribution/distribution/issues/4383" target="_blank" rel="noreferrer">distribution/distribution#4383</a>.</p></li></ol>`,50)]))}const y=i(n,[["render",h]]);export{k as __pageData,y as default};
