import{_ as o,c as a,o as t,a2 as n}from"./chunks/framework.Bfq10Vlj.js";const i="/assets/external-traffic-policy-cluster.BTfVkoFb.png",s="/assets/external-traffic-policy-local.CQM5BCGw.png",f=JSON.parse('{"title":"Seed Settings","description":"","frontmatter":{"github_repo":"https://github.com/gardener/gardener","github_subdir":"docs/operations","params":{"github_branch":"master"},"path_base_for_github_subdir":{"from":"content/docs/gardener/seed_settings.md","to":"seed_settings.md"},"persona":"Operators","title":"Seed Settings","prev":false,"next":false},"headers":[],"relativePath":"docs/gardener/seed_settings/index.md","filePath":"docs/gardener/seed_settings.md","lastUpdated":null}'),r={name:"docs/gardener/seed_settings/index.md"};function c(l,e,d,h,p,u){return t(),a("div",null,e[0]||(e[0]=[n('<h1 id="settings-for-seeds" tabindex="-1">Settings for <code>Seed</code>s <a class="header-anchor" href="#settings-for-seeds" aria-label="Permalink to &quot;Settings for `Seed`s&quot;">​</a></h1><p>The <code>Seed</code> resource offers a few settings that are used to control the behaviour of certain Gardener components. This document provides an overview over the available settings:</p><h2 id="dependency-watchdog" tabindex="-1">Dependency Watchdog <a class="header-anchor" href="#dependency-watchdog" aria-label="Permalink to &quot;Dependency Watchdog&quot;">​</a></h2><p>Gardenlet can deploy two instances of the <a href="https://github.com/gardener/dependency-watchdog" target="_blank" rel="noreferrer">dependency-watchdog</a> into the <code>garden</code> namespace of the seed cluster. One instance only activates the weeder while the second instance only activates the prober.</p><h3 id="weeder" tabindex="-1">Weeder <a class="header-anchor" href="#weeder" aria-label="Permalink to &quot;Weeder&quot;">​</a></h3><p>The weeder helps to alleviate the delay where control plane components remain unavailable by finding the respective pods in <code>CrashLoopBackoff</code> status and restarting them once their dependents become ready and available again. For example, if <code>etcd</code> goes down then also <code>kube-apiserver</code> goes down (and into a <code>CrashLoopBackoff</code> state). If <code>etcd</code> comes up again then (without the <code>endpoint</code> controller) it might take some time until <code>kube-apiserver</code> gets restarted as well.</p><p>⚠️ <code>.spec.settings.dependencyWatchdog.endpoint.enabled</code> is deprecated and will be removed in a future version of Gardener. Use <code>.spec.settings.dependencyWatchdog.weeder.enabled</code> instead.</p><p>It can be enabled/disabled via the <code>.spec.settings.dependencyWatchdog.endpoint.enabled</code> field. It defaults to <code>true</code>.</p><h3 id="prober" tabindex="-1">Prober <a class="header-anchor" href="#prober" aria-label="Permalink to &quot;Prober&quot;">​</a></h3><p>The <code>probe</code> controller scales down the <code>kube-controller-manager</code> of shoot clusters in case their respective <code>kube-apiserver</code> is not reachable via its external ingress. This is in order to avoid melt-down situations, since the <code>kube-controller-manager</code> uses in-cluster communication when talking to the <code>kube-apiserver</code>, i.e., it wouldn&#39;t be affected if the external access to the <code>kube-apiserver</code> is interrupted for whatever reason. The <code>kubelet</code>s on the shoot worker nodes, however, would indeed be affected since they typically run in different networks and use the external ingress when talking to the <code>kube-apiserver</code>. Hence, without scaling down <code>kube-controller-manager</code>, the nodes might be marked as <code>NotReady</code> and eventually replaced (since the <code>kubelet</code>s cannot report their status anymore). To prevent such unnecessary turbulence, <code>kube-controller-manager</code> is being scaled down until the external ingress becomes available again. In addition, as a precautionary measure, <code>machine-controller-manager</code> is also scaled down, along with <code>cluster-autoscaler</code> which depends on <code>machine-controller-manager</code>.</p><p>⚠️ <code>.spec.settings.dependencyWatchdog.probe.enabled</code> is deprecated and will be removed in a future version of Gardener. Use <code>.spec.settings.dependencyWatchdog.prober.enabled</code> instead.</p><p>It can be enabled/disabled via the <code>.spec.settings.dependencyWatchdog.probe.enabled</code> field. It defaults to <code>true</code>.</p><h2 id="reserve-excess-capacity" tabindex="-1">Reserve Excess Capacity <a class="header-anchor" href="#reserve-excess-capacity" aria-label="Permalink to &quot;Reserve Excess Capacity&quot;">​</a></h2><p>If the excess capacity reservation is enabled, then the gardenlet will deploy a special <code>Deployment</code> into the <code>garden</code> namespace of the seed cluster. This <code>Deployment</code>&#39;s pod template has only one container, the <code>pause</code> container, which simply runs in an infinite loop. The priority of the deployment is very low, so any other pod will preempt these <code>pause</code> pods. This is especially useful if new shoot control planes are created in the seed. In case the seed cluster runs at its capacity, then there is no waiting time required during the scale-up. Instead, the low-priority <code>pause</code> pods will be preempted and allow newly created shoot control plane pods to be scheduled fast. In the meantime, the cluster-autoscaler will trigger the scale-up because the preempted <code>pause</code> pods want to run again. However, this delay doesn&#39;t affect the important shoot control plane pods, which will improve the user experience.</p><p>Use <code>.spec.settings.excessCapacityReservation.configs</code> to create excess capacity reservation deployments which allow to specify custom values for <code>resources</code>, <code>nodeSelector</code> and <code>tolerations</code>. Each config creates a deployment with a minimum number of 2 replicas and a maximum equal to the number of zones configured for this seed. It defaults to a config reserving 2 CPUs and 6Gi of memory for each pod with no <code>nodeSelector</code> and no <code>tolerations</code>.</p><p>Excess capacity reservation is enabled when <code>.spec.settings.excessCapacityReservation.enabled</code> is <code>true</code> or not specified while <code>configs</code> are present. It can be disabled by setting the field to <code>false</code>.</p><h2 id="scheduling" tabindex="-1">Scheduling <a class="header-anchor" href="#scheduling" aria-label="Permalink to &quot;Scheduling&quot;">​</a></h2><p>By default, the Gardener Scheduler will consider all seed clusters when a new shoot cluster shall be created. However, administrators/operators might want to exclude some of them from being considered by the scheduler. Therefore, seed clusters can be marked as &quot;invisible&quot;. In this case, the scheduler simply ignores them as if they wouldn&#39;t exist. Shoots can still use the invisible seed but only by explicitly specifying the name in their <code>.spec.seedName</code> field.</p><p>Seed clusters can be marked visible/invisible via the <code>.spec.settings.scheduling.visible</code> field. It defaults to <code>true</code>.</p><p>ℹ️ In previous Gardener versions (&lt; 1.5) these settings were controlled via taint keys (<code>seed.gardener.cloud/{disable-capacity-reservation,invisible}</code>). The taint keys are no longer supported and removed in version 1.12. The rationale behind it is the implementation of tolerations similar to Kubernetes tolerations. More information about it can be found in <a href="https://github.com/gardener/gardener/issues/2193" target="_blank" rel="noreferrer">#2193</a>.</p><h2 id="load-balancer-services" tabindex="-1">Load Balancer Services <a class="header-anchor" href="#load-balancer-services" aria-label="Permalink to &quot;Load Balancer Services&quot;">​</a></h2><p>Gardener creates certain Kubernetes <code>Service</code> objects of type <code>LoadBalancer</code> in the seed cluster. Most prominently, they are used for exposing the shoot control planes, namely the kube-apiserver of the shoot clusters. In most cases, the cloud-controller-manager (responsible for managing these load balancers on the respective underlying infrastructure) supports certain customization and settings via annotations. <a href="https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer" target="_blank" rel="noreferrer">This document</a> provides a good overview and many examples.</p><p>By setting the <code>.spec.settings.loadBalancerServices.annotations</code> field the Gardener administrator can specify a list of annotations, which will be injected into the <code>Service</code>s of type <code>LoadBalancer</code>.</p><h3 id="load-balancer-class" tabindex="-1">Load Balancer Class <a class="header-anchor" href="#load-balancer-class" aria-label="Permalink to &quot;Load Balancer Class&quot;">​</a></h3><p>By default, Gardener creates <code>Services</code> without the <code>spec.loadBalancerClass</code> field set, meaning that the default load balancer implementation of the underlying cloud infrastructure is used (implemented by the <code>Service</code> controller of cloud-controller-manager). If a non-default load balancer implementation should be used for load balancer services in the seed cluster, the <code>spec.settings.loadBalancerServices.loadBalancerClass</code> field can be configured accordingly to set the <code>spec.loadBalancerClass</code> on the created <code>Service</code> objects. Note that changing the <code>loadBalancerClass</code> of existing load balancer services is denied by Kubernetes, i.e., this setting can only be applied automatically to newly created load balancer services. If an existing load balancer service should use a different load balancer class, the migration needs to be performed manually by the operator.</p><h3 id="external-traffic-policy" tabindex="-1">External Traffic Policy <a class="header-anchor" href="#external-traffic-policy" aria-label="Permalink to &quot;External Traffic Policy&quot;">​</a></h3><p>Setting the <a href="https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip" target="_blank" rel="noreferrer">external traffic policy</a> to <code>Local</code> can be beneficial as it preserves the source IP address of client requests. In addition to that, it removes one hop in the data path and hence reduces request latency. On some cloud infrastructures, it can furthermore be used in conjunction with <code>Service</code> annotations as described above to prevent cross-zonal traffic from the load balancer to the backend pod.</p><p>The default external traffic policy is <code>Cluster</code>, meaning that all traffic from the load balancer will be sent to any cluster node, which then itself will redirect the traffic to the actual receiving pod. This approach adds a node to the data path, may cross the zone boundaries twice, and replaces the source IP with one of the cluster nodes.</p><p><img src="'+i+'" alt="External Traffic Policy Cluster"></p><p>Using external traffic policy <code>Local</code> drops the additional node, i.e., only cluster nodes with corresponding backend pods will be in the list of backends of the load balancer. However, this has multiple implications. The health check port in this scenario is exposed by <code>kube-proxy</code> , i.e., if <code>kube-proxy</code> is not working on a node a corresponding pod on the node will not receive traffic from the load balancer as the load balancer will see a failing health check. (This is quite different from ordinary service routing where <code>kube-proxy</code> is only responsible for setup, but does not need to run for its operation.) Furthermore, load balancing may become imbalanced if multiple pods run on the same node because load balancers will split the load equally among the nodes and not among the pods. This is mitigated by corresponding node anti affinities.</p><p><img src="'+s+'" alt="External Traffic Policy Local"></p><p>Operators need to take these implications into account when considering switching external traffic policy to <code>Local</code>.</p><h3 id="proxy-protocol" tabindex="-1">Proxy Protocol <a class="header-anchor" href="#proxy-protocol" aria-label="Permalink to &quot;Proxy Protocol&quot;">​</a></h3><p>Traditionally, the client IP address can be used for security filtering measures, e.g. IP allow listing. However, for this to have any usefulness, the client IP address needs to be correctly transferred to the filtering entity.</p><p>Load balancers can either act transparently and simply pass the client IP on, or they terminate one connection and forward data on a new connection. The latter (intransparant) approach requires a separate way to propagate the client IP address. Common approaches are an HTTP header for TLS terminating load balancers or <a href="https://www.haproxy.org/download/3.0/doc/proxy-protocol.txt" target="_blank" rel="noreferrer">(HA) proxy protocol</a>.</p><p>For level 3 load balancers, <a href="https://www.haproxy.org/download/3.0/doc/proxy-protocol.txt" target="_blank" rel="noreferrer">(HA) proxy protocol</a> is the default way to preserve client IP addresses. As it prepends a small proxy protocol header before the actual workload data, the receiving server needs to be aware of it and handle it properly. This means that activating proxy protocol needs to happen on both load balancer and receiving server at/around the same time, as otherwise the receiving server will incorrectly interpret data as workload/proxy protocol header.</p><p>For disruption-free migration to proxy protocol, set <code>.spec.settings.loadBalancerServices.proxyProtocol.allow</code> to <code>true</code>. The migration path should be to enable the option and shortly thereafter also enable proxy protocol on the load balancer with infrastructure-specific means, e.g. a corresponding load balancer annotation.</p><p>When switching back from use of proxy protocol to no use of it, use the inverse order, i.e. disable proxy protocol first on the load balancer before disabling <code>.spec.settings.loadBalancerServices.proxyProtocol.allow</code>.</p><h3 id="zonal-ingress" tabindex="-1">Zonal Ingress <a class="header-anchor" href="#zonal-ingress" aria-label="Permalink to &quot;Zonal Ingress&quot;">​</a></h3><p>By default, Gardener deploys Istio ingress gateways in each availability zone of a seed. This reduces cross-zonal traffic for single-zone shoot control planes.<br> If cross-zonal traffic costs are not a concern and minimizing the number of load balancers is preferred to save resources, zonal Istio ingress gateways can be disabled.</p><p>The behavior is controlled by the <code>.spec.settings.loadBalancerServices.zonalIngress.enabled</code> field, which defaults to <code>true</code>.</p><h4 id="impact-of-disabling-zonal-ingress" tabindex="-1">Impact of Disabling Zonal Ingress <a class="header-anchor" href="#impact-of-disabling-zonal-ingress" aria-label="Permalink to &quot;Impact of Disabling Zonal Ingress&quot;">​</a></h4><p>When zonal ingress is disabled:</p><ul><li>A single default Istio ingress gateway is deployed, spanning all availability zones</li><li>All shoot control plane traffic (single-zone and multi-zone) is routed through the default gateway</li><li>Cross-zonal traffic may increase, since single-zone shoots no longer use zonal gateways</li><li>Fewer load balancers are created, potentially reducing infrastructure costs</li></ul><h4 id="migration-guide" tabindex="-1">Migration Guide <a class="header-anchor" href="#migration-guide" aria-label="Permalink to &quot;Migration Guide&quot;">​</a></h4><p>To disable zonal ingress gateways, follow these steps:</p><ol><li>Set <code>.spec.settings.loadBalancerServices.zonalIngress.enabled=false</code> in the <code>Seed</code> specification</li><li>Wait for all shoots to reconcile (during reconciliation, single-zone shoots will update their <code>Gateway</code> resources to use the default gateway)</li><li>Once all shoots have reconciled, zonal ingress gateways are automatically cleaned up</li></ol><p>During migration, zonal ingress gateways remain deployed as long as any shoot <code>Gateway</code> resource still selects them. This ensures minimal downtime for shoot control planes during the transition.</p><div class="caution custom-block github-alert"><p class="custom-block-title">CAUTION</p><p>When disabling zonal ingress gateways, expect temporary unavailability of shoot control planes due to DNS propagation delays. During migration, DNS entries are updated to point to the default gateway, but clients may continue using cached DNS entries pointing to the zonal gateways for up to the configured DNS TTL. Additionally, existing long-running connections will be interrupted when the Gateway resources are updated to no longer use the zonal ingress gateways. This downtime should be scheduled during a maintenance window.</p></div><p>For more about Istio&#39;s multi-zone behavior, see the <a href="/docs/gardener/istio/#handling-multiple-availability-zones-with-istio">Istio documentation</a>.</p><h3 id="zone-specific-settings" tabindex="-1">Zone-Specific Settings <a class="header-anchor" href="#zone-specific-settings" aria-label="Permalink to &quot;Zone-Specific Settings&quot;">​</a></h3><p>In case a seed cluster is configured to use multiple zones via <code>.spec.provider.zones</code>, it may be necessary to configure the load balancers in individual zones in different way, e.g., by utilizing different annotations. One reason may be to reduce cross-zonal traffic and have zone-specific load balancers in place. Zone-specific load balancers may then be bound to zone-specific subnets or availability zones in the cloud infrastructure.</p><p>Besides the load balancer annotations, it is also possible to set <a href="/docs/gardener/seed_settings/#proxy-protocol">proxy protocol termination</a> and the <a href="/docs/gardener/seed_settings/#external-traffic-policy">external traffic policy</a> for each zone-specific load balancer individually.</p><h2 id="vertical-pod-autoscaler" tabindex="-1">Vertical Pod Autoscaler <a class="header-anchor" href="#vertical-pod-autoscaler" aria-label="Permalink to &quot;Vertical Pod Autoscaler&quot;">​</a></h2><p>Gardener heavily relies on the Kubernetes <a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler" target="_blank" rel="noreferrer"><code>vertical-pod-autoscaler</code> component</a>. By default, the seed controller deploys the VPA components into the <code>garden</code> namespace of the respective seed clusters. In case you want to manage the VPA deployment on your own or have a custom one, then you might want to disable the automatic deployment of Gardener. Otherwise, you might end up with two VPAs, which will cause erratic behaviour. By setting the <code>.spec.settings.verticalPodAutoscaler.enabled=false</code>, you can disable the automatic deployment.</p><p>⚠️ In any case, there must be a VPA available for your seed cluster. Using a seed without VPA is not supported.</p><h3 id="vpa-pitfall-excessive-resource-requests-making-pod-unschedulable" tabindex="-1">VPA Pitfall: Excessive Resource Requests Making Pod Unschedulable <a class="header-anchor" href="#vpa-pitfall-excessive-resource-requests-making-pod-unschedulable" aria-label="Permalink to &quot;VPA Pitfall: Excessive Resource Requests Making Pod Unschedulable&quot;">​</a></h3><p>VPA is unaware of node capacity, and can increase the resource requests of a pod beyond the capacity of any single node. Such pod is likely to become permanently unschedulable. That problem can be partly mitigated by using the <code>VerticalPodAutoscaler.Spec.ResourcePolicy.ContainerPolicies[].MaxAllowed</code> field to constrain pod resource requests to the level of nodes&#39; allocatable resources. The downside is that a pod constrained in such fashion would be using more resources than it has requested, and can starve for resources and/or negatively impact neighbour pods with which it is sharing a node.</p><p>As an alternative, in scenarios where MaxAllowed is not set, it is important to maintain a worker pool which can accommodate the highest level of resources that VPA would actually request for the pods it controls.</p><p>Finally, the optimal strategy typically is to both ensure large enough worker pools, and, as an insurance, use MaxAllowed aligned with the allocatable resources of the largest worker.</p><h2 id="topology-aware-traffic-routing" tabindex="-1">Topology-Aware Traffic Routing <a class="header-anchor" href="#topology-aware-traffic-routing" aria-label="Permalink to &quot;Topology-Aware Traffic Routing&quot;">​</a></h2><p>Refer to the <a href="/docs/gardener/topology_aware_routing/">Topology-Aware Traffic Routing documentation</a> as this document contains the documentation for the topology-aware routing Seed setting.</p><h2 id="temporarily-disabling-shoot-reconciliations" tabindex="-1">Temporarily Disabling Shoot Reconciliations <a class="header-anchor" href="#temporarily-disabling-shoot-reconciliations" aria-label="Permalink to &quot;Temporarily Disabling Shoot Reconciliations&quot;">​</a></h2><p>There may be emergency situations where you need to temporarily stop the reconciliation of <code>Shoot</code> clusters in a <code>Seed</code> cluster, for example, to prevent faulty updates from propagating further or to avoid gardenlet performing unintended actions.</p><div class="caution custom-block github-alert"><p class="custom-block-title">CAUTION</p><p>This mechanism should only be used in exceptional cases and not as part of regular operations, as it can have significant implications for cluster health and update consistency.</p></div><p>To temporarily disable reconciliations, add the annotation <code>shoot.gardener.cloud/emergency-stop-reconciliations=true</code> to the <code>Seed</code> resource.</p><p>While this annotation is present:</p><ul><li>The <code>Shoot</code> controller will not reconcile any <code>Shoot</code> clusters in the affected <code>Seed</code>.</li><li>New <code>Shoot</code> clusters will not be scheduled to this <code>Seed</code>.</li><li>The <code>Seed</code> will expose the <code>SeedDisabledShootReconciliations</code> condition.</li></ul><div class="note custom-block github-alert"><p class="custom-block-title">NOTE</p><p>When you remove this annotation, reconciliation will resume, but <code>Shoot</code> clusters will only be reconciled during their next scheduled reconciliation window. They are not reconciled immediately. This is important if you expect all clusters to be updated right away after re-enabling reconciliations.</p></div><p>To annotate all <code>Seed</code> resources at once, run:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">kubectl</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> annotate</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> seed</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --all</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> shoot.gardener.cloud/emergency-stop-reconciliations=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">true</span></span></code></pre></div>',71)]))}const b=o(r,[["render",c]]);export{f as __pageData,b as default};
