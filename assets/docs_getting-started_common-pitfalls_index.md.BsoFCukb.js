import{_ as o,c as t,o as a,a2 as i}from"./chunks/framework.Bfq10Vlj.js";const s="/assets/microservices.-mq6BxW5.png",n="/assets/hibernation-1.BC-Pe5G5.png",r="/assets/capacity.Cl7dfYpU.png",l="/assets/mask-size.CmP6zz6t.png",d="/assets/cidr-ranges.BqgLQpRa.png",c="/assets/expired-credentials.9weq32fs.png",h="/assets/node-draining.2qCaNUMS.gif",u="/assets/node-draining-pdb.APOUBeDB.gif",p="/assets/user-webhook.D8cWSf1C.gif",m="/assets/user-webhook-fail.Llb6ylr4.gif",b="/assets/timeout.B-R6Fmat.png",g="/assets/conversion-webhook-crd.DM4tj57v.png",C=JSON.parse('{"title":"Common Pitfalls","description":"","frontmatter":{"github_repo":"https://github.com/gardener/documentation","github_subdir":"website/documentation/getting-started","params":{"github_branch":"master"},"path_base_for_github_subdir":{"from":"content/docs/getting-started/common-pitfalls.md","to":"common-pitfalls.md"},"title":"Common Pitfalls","weight":9,"prev":false,"next":false},"headers":[],"relativePath":"docs/getting-started/common-pitfalls/index.md","filePath":"docs/getting-started/common-pitfalls.md","lastUpdated":null}'),f={name:"docs/getting-started/common-pitfalls/index.md"};function k(w,e,y,v,q,P){return a(),t("div",null,e[0]||(e[0]=[i('<h1 id="common-pitfalls" tabindex="-1">Common Pitfalls <a class="header-anchor" href="#common-pitfalls" aria-label="Permalink to &quot;Common Pitfalls&quot;">​</a></h1><h2 id="architecture" tabindex="-1">Architecture <a class="header-anchor" href="#architecture" aria-label="Permalink to &quot;Architecture&quot;">​</a></h2><h3 id="containers-will-not-fix-a-broken-architecture" tabindex="-1">Containers will NOT fix a broken architecture! <a class="header-anchor" href="#containers-will-not-fix-a-broken-architecture" aria-label="Permalink to &quot;Containers will NOT fix a broken architecture!&quot;">​</a></h3><p><img src="'+s+'" alt="microservices"></p><p>Running a highly distributed system has advantages, but of course, those come at a cost. In order to succeed, one would need:</p><ul><li>Logging</li><li>Tracing</li><li>No singleton</li><li>Tolerance to failure of individual instances</li><li>Automated config / change management</li><li>Kubernetes knowledge</li></ul><h2 id="scalability" tabindex="-1">Scalability <a class="header-anchor" href="#scalability" aria-label="Permalink to &quot;Scalability&quot;">​</a></h2><p>Most scalability dimensions are interconnected with others. If a cluster grows beyond reasonable defaults, it can still function very well. But tuning it comes at the cost of time and can influence stability negatively.</p><p>Take the number of nodes and pods, for example. Both are connected and you cannot grow both towards their individual limits, as you would face issues way before reaching any theoretical limits.</p><p>Reading the <a href="/docs/guides/administer-shoots/scalability/">Scalability of Gardener Managed Kubernetes Clusters</a> guide is strongly recommended in order to understand the topic of scalability within Kubernetes and Gardener.</p><h3 id="a-small-sample-of-things-that-can-grow-beyond-reasonable-limits" tabindex="-1">A Small Sample of Things That Can Grow Beyond Reasonable Limits <a class="header-anchor" href="#a-small-sample-of-things-that-can-grow-beyond-reasonable-limits" aria-label="Permalink to &quot;A Small Sample of Things That Can Grow Beyond Reasonable Limits&quot;">​</a></h3><p><img src="'+n+'" alt="hibernation-1"></p><p>When scaling a cluster, there are plenty of resources that can be exhausted or reach a limit:</p><ul><li>The API server will be scaled horizontally and vertically by Gardener. However, it can still consume too much resources to fit onto a single node on the seed. In this case, you can only reduce the load on the API server. This should not happen with regular usage patterns though.</li><li>ETCD disk space: 8GB is the limit. If you have too many resources or a high churn rate, a cluster can run out of ETCD capacity. In such a scenario it will stop working until defragmented, compacted, and cleaned up.</li><li>The number of nodes is limited by the network configuration (pod cidr range &amp; node cidr mask). Also, there is a reasonable number of nodes (300) that most workloads should not exceed. It is possible to go beyond but doing so requires careful tuning and consideration of connected scaling dimensions (like the number of pods per node).</li></ul><p><strong>The availability of your cluster is directly impacted by the way you use it.</strong></p><h3 id="infrastructure-capacity-and-quotas" tabindex="-1">Infrastructure Capacity and Quotas <a class="header-anchor" href="#infrastructure-capacity-and-quotas" aria-label="Permalink to &quot;Infrastructure Capacity and Quotas&quot;">​</a></h3><p><img src="'+r+'" alt="capacity"></p><p>Sometimes requests cannot be fulfilled due to shortages on the infrastructure side. For example, a certain instance type might not be available and new Kubernetes nodes of this type cannot be added. It is a good practice to use the <a href="https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/expander/priority/readme.md" target="_blank" rel="noreferrer">cluster-autoscaler&#39;s priority expander</a> and have a secondary node pool.</p><p>Sometimes, it is not the physical capacity but exhausted quotas within an infrastructure account that result in limits. Obviously, there should be sufficient quota to create as many VMs as needed. But there are also other resources that are created in the infrastructure that need proper quotas:</p><ul><li>Loadbalancers</li><li>VPC</li><li>Disks</li><li>Routes (often forgotten, but very important for clusters without overlay network; typically defaults to around 50 routes, meaning that 50 nodes is the maximum a cluster can have)</li><li>...</li></ul><h3 id="nodecidrmasksize" tabindex="-1">NodeCIDRMaskSize <a class="header-anchor" href="#nodecidrmasksize" aria-label="Permalink to &quot;NodeCIDRMaskSize&quot;">​</a></h3><p><img src="'+l+'" alt="mask-size"></p><p>Upon cluster creation, there are several settings that are network related. For example, the address space for Pods has to be defined. In this case, it is a <code>/16</code> subnet that includes a total of 65.536 hosts. However, that does not imply that you can easily use all addresses at the same point in time.</p><p>As part of the Kubernetes network setup, the <code>/16</code> network is divided into smaller subnets and each node gets a distinct subnet. The size of this subnet defaults to <code>/24</code>. It can also be specified (but not changed later).</p><p>Now, as you create more nodes, you have a total of 256 subnets that can be assigned to nodes, thus limiting the total number of nodes of this cluster to 256.</p><p>For more information, see <a href="/docs/gardener/networking/shoot_networking/">Shoot Networking</a>.</p><h2 id="overlapping-vpcs" tabindex="-1">Overlapping VPCs <a class="header-anchor" href="#overlapping-vpcs" aria-label="Permalink to &quot;Overlapping VPCs&quot;">​</a></h2><h3 id="avoid-overlapping-cidr-ranges-in-vpcs" tabindex="-1">Avoid Overlapping CIDR Ranges in VPCs <a class="header-anchor" href="#avoid-overlapping-cidr-ranges-in-vpcs" aria-label="Permalink to &quot;Avoid Overlapping CIDR Ranges in VPCs&quot;">​</a></h3><p><img src="'+d+'" alt="cidr-ranges"></p><p>Gardener can create shoot cluster resources in an existing / user-created VPC. However, you have to make sure that the CIDR ranges used by the shoots nodes or subnets for zones do not overlap with other shoots deployed to the same VPC.</p><p>In case of an overlap, there might be strange routing effects, and packets ending up at a wrong location.</p><h2 id="expired-credentials" tabindex="-1">Expired Credentials <a class="header-anchor" href="#expired-credentials" aria-label="Permalink to &quot;Expired Credentials&quot;">​</a></h2><p><img src="'+c+'" alt="expired-credentials"></p><p>Credentials expire or get revoked. When this happens to the actively used infrastructure credentials of a shoot, the cluster will stop working after a while. New nodes cannot be added, LoadBalancers cannot be created, and so on.</p><p>You can update the credentials stored in the project namespace and reconcile the cluster to replicate the new keys to all relevant controllers. Similarly, when doing a planned rotation one should wait until the shoot reconciled successfully before invalidating the old credentials.</p><h2 id="autoupdate-breaking-clusters" tabindex="-1">AutoUpdate Breaking Clusters <a class="header-anchor" href="#autoupdate-breaking-clusters" aria-label="Permalink to &quot;AutoUpdate Breaking Clusters&quot;">​</a></h2><p>Gardener can automatically update a shoot&#39;s Kubernetes patch version, when a new patch version is labeled as &quot;supported&quot;. Automatically updating of the OS images works in a similar way. Both are triggered by the &quot;supported&quot; classification in the respective cloud profile and can be enabled / disabled as part a shoot&#39;s spec.</p><p>Additionally, when a minor Kubernetes / OS version expires, Gardener will force-update the shoot to the next supported version.</p><p>Turning on AutoUpdate for a shoot may be convenient but comes at the risk of potentially unwanted changes. While it is possible to switch to another OS version, updates to the Kubernetes version are a one way operation and cannot be reverted.</p><div class="note custom-block github-alert"><p class="custom-block-title">Recommendation</p><p>Control the version lifecycle separately for any cluster that hosts important workload.</p></div><h2 id="node-draining" tabindex="-1">Node Draining <a class="header-anchor" href="#node-draining" aria-label="Permalink to &quot;Node Draining&quot;">​</a></h2><h3 id="node-draining-and-pod-disruption-budget" tabindex="-1">Node Draining and Pod Disruption Budget <a class="header-anchor" href="#node-draining-and-pod-disruption-budget" aria-label="Permalink to &quot;Node Draining and Pod Disruption Budget&quot;">​</a></h3><p><img src="'+h+'" alt="node-draining"></p><p>Typically, nodes are drained when:</p><ul><li>There is a update of the OS / Kubernetes minor version</li><li>An Operator cordons &amp; drains a node</li><li>The cluster-autoscaler wants to scale down</li></ul><p>Without a PodDistruptionBudget, pods will be terminated as fast as possible. If an application has 2 out of 2 replicas running on the drained node, this will probably cause availability issues.</p><h3 id="node-draining-with-pdb" tabindex="-1">Node Draining with PDB <a class="header-anchor" href="#node-draining-with-pdb" aria-label="Permalink to &quot;Node Draining with PDB&quot;">​</a></h3><p><img src="'+u+'" alt="node-draining-pdb"></p><p>PodDisruptionBudgets can help to manage a graceful node drain. However, if no disruptions are allowed there, the node drain will be blocked until it reaches a timeout. Only then will the nodes be terminated but without respecting PDB thresholds.</p><div class="note custom-block github-alert"><p class="custom-block-title">Recommendation</p><p>Configure PDBs and allow disruptions.</p></div><h2 id="pod-resource-requests-and-limits" tabindex="-1">Pod Resource Requests and Limits <a class="header-anchor" href="#pod-resource-requests-and-limits" aria-label="Permalink to &quot;Pod Resource Requests and Limits&quot;">​</a></h2><h3 id="resource-consumption" tabindex="-1">Resource Consumption <a class="header-anchor" href="#resource-consumption" aria-label="Permalink to &quot;Resource Consumption&quot;">​</a></h3><p>Pods consume resources and, of course, there are only so many resources available on a single node. Setting requests will make the scheduling much better, as the scheduler has more information available.</p><p>Specifying limits can help, but can also limit an application in unintended ways. A recommendation to start with:</p><ul><li>Do not set CPU limits (CPU is compressible and throttling is really hard to detect)</li><li>Set memory limits and monitor OOM kills / restarts of workload (typically detectable by container status exit code 137 and corresponding events). This will decrease the likelihood of OOM situations on the node itself. However, for critical workloads it might be better to have uncapped growth and rather risk a node going OOM.</li></ul><p>Next, consider if assigning the workload to quality of service class <code>guaranteed</code> is needed. Again - this can help or be counterproductive. It is important to be aware of its implications. For more information, see <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/" target="_blank" rel="noreferrer">Pod Quality of Service Classes</a>.</p><p>Tune <code>shoot.spec.Kubernetes.kubeReserved</code> to protect the node (kubelet) in case of a workload pod consuming too much resources. It is very helpful to ensure a high level of stability.</p><p>If the usage profile changes over time, the VPA can help a lot to adapt the resource requests / limits automatically.</p><h2 id="webhooks" tabindex="-1">Webhooks <a class="header-anchor" href="#webhooks" aria-label="Permalink to &quot;Webhooks&quot;">​</a></h2><h3 id="user-deployed-webhooks-in-kubernetes" tabindex="-1">User-Deployed Webhooks in Kubernetes <a class="header-anchor" href="#user-deployed-webhooks-in-kubernetes" aria-label="Permalink to &quot;User-Deployed Webhooks in Kubernetes&quot;">​</a></h3><p><img src="'+p+'" alt="user-webhook"></p><p>By default, any request to the API server will go through a chain of checks. Let&#39;s take the example of creating a pod.</p><p>When the resource is submitted to the API server, it will be checked against the following validations:</p><ul><li>Is the user authorized to perform this action?</li><li>Is the pod definitionactually valid?</li><li>Are the specified values allowed?</li></ul><p>Additionally, there is the defaulting - like the injection of the <code>default</code> service account&#39;s name, if nothing else is specified.</p><p>This chain of admission control and mutation can be enhanced by the user. Read about <a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/" target="_blank" rel="noreferrer">dynamic admission control</a> for more details.</p><p><code>ValidatingWebhookConfiguration</code>: allow or deny requests based on custom rules</p><p><code>MutatingWebhookConfiguration</code>: change а resource before it is actually stored in etcd (that is, before any other controller acts upon)</p><p>Both <code>ValidatingWebhookConfiguration</code> as well as <code>MutatingWebhookConfiguration</code> resources:</p><ul><li>specify for which resources and operations these checks should be executed.</li><li>specify how to reach the webhook server (typically a service running on the data plane of a cluster)</li><li>rely on a webhook server performing a review and reply to the <code>admissionReview</code> request</li></ul><p><img src="'+m+'" alt="user-webhook-fail"></p><p>What could possibly go wrong? Due to the separation of control plane and data plane in Gardener&#39;s architecture, webhooks have the potential to break a cluster. If the webhook server is not responding in time with a valid answer, the request should timeout and the failure policy is invoked. Depending on the scope of the webhook, frequent failures may cause downtime for applications. Common causes for failure are:</p><ul><li>The call to the webhook is made through the VPN tunnel. VPN / connection issues can happen both on the side of the seed as well as the shoot and would render the webhook unavailable from the perspective of the control plane.</li><li>The traffic cannot reach the pod (network issue, pod not available)</li><li>The pod is processing too slow (e.g., because there are too many requests)</li></ul><h3 id="timeout" tabindex="-1">Timeout <a class="header-anchor" href="#timeout" aria-label="Permalink to &quot;Timeout&quot;">​</a></h3><p><img src="'+b+'" alt="timeout"></p><p>Webhooks are a very helpful feature of Kubernetes. However, they can easily be configured to break a shoot cluster. Take the timeout, for example. High timeouts (&gt;15s) can lead to blocking requests of control plane components. That&#39;s because most control-plane API calls are made with a client-side timeout of 30s, so if a webhook has <code>timeoutSeconds=30</code>, the overall request might still fail as there is overhead in communication with the API server and other potential webhooks.</p><div class="note custom-block github-alert"><p class="custom-block-title">Recommendation</p><p>Webhooks (esp. mutating) may be called sequentially and thus adding up their individual timeouts. Even with a <code>faliurePolicy=ignore</code> the timeout will stop the request.</p></div><h3 id="recommendations" tabindex="-1">Recommendations <a class="header-anchor" href="#recommendations" aria-label="Permalink to &quot;Recommendations&quot;">​</a></h3><p>Problematic webhooks are reported as part of a shoot&#39;s status. In addition to timeouts, it is crucial to exclude the <code>kube-system</code> namespace and (potentially non-namespaced) resources that are necessary for the cluster to function properly. Those should not be subject to a user-defined webhook.</p><p>In particular, a webhook should not operate on:</p><ul><li>the <code>kube-system</code> namespace</li><li><code>Endpoints</code> or <code>EndpointSlices</code></li><li><code>Nodes</code></li><li><code>PodSecurityPolicies</code></li><li><code>ClusterRoles</code></li><li><code>ClusterRoleBindings</code></li><li><code>CustomResourceDefinitions</code></li><li><code>ApiServices</code></li><li><code>CertificateSigningRequests</code></li><li><code>PriorityClasses</code></li></ul><p><strong>Example:</strong></p><p>A webhook checks node objects upon creation and has a <code>failurePolicy: fail</code>. If the webhook does not answer in time (either due to latency or because there is no pod serving it), new nodes cannot join the cluster.</p><p>For more information, see <a href="/docs/gardener/shoot/shoot_status/#constraints">Shoot Status</a>.</p><h2 id="conversion-webhooks" tabindex="-1">Conversion Webhooks <a class="header-anchor" href="#conversion-webhooks" aria-label="Permalink to &quot;Conversion Webhooks&quot;">​</a></h2><h3 id="who-installs-a-conversion-webhook" tabindex="-1">Who installs a conversion webhook? <a class="header-anchor" href="#who-installs-a-conversion-webhook" aria-label="Permalink to &quot;Who installs a conversion webhook?&quot;">​</a></h3><p>If you have written your own <code>CustomResourceDefinition</code> (CRD) and made a version upgrade, you will also have consciously written &amp; deployed the conversion webhook.</p><p>However, sometimes, you simply use helm or kustomize to install a (third-party) dependency that contains CRDs. Of course, those can contain conversion webhooks as well. As a user of a cluster, please make sure to be aware what you deploy.</p><h3 id="crd-with-a-conversion-webhook" tabindex="-1">CRD with a Conversion Webhook <a class="header-anchor" href="#crd-with-a-conversion-webhook" aria-label="Permalink to &quot;CRD with a Conversion Webhook&quot;">​</a></h3><p><img src="'+g+'" alt="conversion-webhook-crd"></p><p>Conversion webhooks are tricky. Similarly to regular webhooks, they should have a low timeout. However, they cannot be remediated automatically and can cause errors in the control plane. For example, if a webhook is invoked but not available, it can block the garbage collection run by the kube-controller-manager.</p><p>In turn, when deleting something like a <code>deployment</code>, dependent resources like <code>pods</code> will not be deleted automatically.</p><div class="note custom-block github-alert"><p class="custom-block-title">Recommendation</p><p>Try to avoid conversion webhooks. They are valid and can be used, but should not stay in place forever. Complete the upgrade to a new version of the CRD as soon as possible.</p></div><p>For more information, see the <a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#webhook-conversion" target="_blank" rel="noreferrer">Webhook Conversion</a>, <a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#upgrade-existing-objects-to-a-new-stored-version" target="_blank" rel="noreferrer">Upgrade Existing Objects to a New Stored Version</a>, and <a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#version-priority" target="_blank" rel="noreferrer">Version Priority</a> topics in the Kubernetes documentation.</p>',94)]))}const _=o(f,[["render",k]]);export{C as __pageData,_ as default};
