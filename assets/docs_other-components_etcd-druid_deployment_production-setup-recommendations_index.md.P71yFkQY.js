import{_ as t,c as a,o as s,a2 as r}from"./chunks/framework.Bfq10Vlj.js";const u=JSON.parse('{"title":"Production Setup Recommendations","description":"","frontmatter":{"github_repo":"https://github.com/gardener/etcd-druid","github_subdir":"docs/deployment","params":{"github_branch":"master"},"path_base_for_github_subdir":{"from":"content/docs/other-components/etcd-druid/deployment/production-setup-recommendations.md","to":"production-setup-recommendations.md"},"title":"Production Setup Recommendations","prev":false,"next":false},"headers":[],"relativePath":"docs/other-components/etcd-druid/deployment/production-setup-recommendations/index.md","filePath":"docs/other-components/etcd-druid/deployment/production-setup-recommendations.md","lastUpdated":null}'),o={name:"docs/other-components/etcd-druid/deployment/production-setup-recommendations/index.md"};function i(n,e,l,c,d,h){return s(),a("div",null,e[0]||(e[0]=[r(`<h1 id="setting-up-etcd-druid-in-production" tabindex="-1">Setting up etcd-druid in Production <a class="header-anchor" href="#setting-up-etcd-druid-in-production" aria-label="Permalink to &quot;Setting up etcd-druid in Production&quot;">​</a></h1><p>You can get familiar with <code>etcd-druid</code> and all the resources that it creates by setting up etcd-druid locally by following the <a href="/docs/other-components/etcd-druid/deployment/getting-started-locally/getting-started-locally/">detailed guide</a>. This document lists down recommendations for a productive setup of etcd-druid.</p><h2 id="helm-charts" tabindex="-1">Helm Charts <a class="header-anchor" href="#helm-charts" aria-label="Permalink to &quot;Helm Charts&quot;">​</a></h2><p>You can use <a href="https://helm.sh/" target="_blank" rel="noreferrer">helm</a> charts at <a href="https://github.com/gardener/etcd-druid/tree/55efca1c8f6c852b0a4e97f08488ffec2eed0e68/charts/druid" target="_blank" rel="noreferrer">this</a> location to deploy druid. Values for charts are present <a href="https://github.com/gardener/etcd-druid/blob/55efca1c8f6c852b0a4e97f08488ffec2eed0e68/charts/druid/values.yaml" target="_blank" rel="noreferrer">here</a> and can be configured as per your requirement. Following charts are present:</p><ul><li><p><code>deployment.yaml</code> - defines a kubernetes <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/" target="_blank" rel="noreferrer">Deployment</a> for etcd-druid. To configure the CLI flags for druid you can refer to <a href="/docs/other-components/etcd-druid/deployment/configure-etcd-druid/">this</a> document which explains these flags in detail.</p></li><li><p><code>serviceaccount.yaml</code> - defines a kubernetes <a href="https://kubernetes.io/docs/concepts/security/service-accounts/" target="_blank" rel="noreferrer">ServiceAccount</a> which will serve as a technical user to which role/clusterroles can be bound.</p></li><li><p><code>clusterrole.yaml</code> - etcd-druid can manage multiple etcd clusters. In a <code>hosted control plane</code> setup (e.g. <a href="https://github.com/gardener/gardener" target="_blank" rel="noreferrer">Gardener</a>), one would typically create separate <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/" target="_blank" rel="noreferrer">namespace</a> per control-plane. This would require a <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#role-and-clusterrole" target="_blank" rel="noreferrer">ClusterRole</a> to be defined which gives etcd-druid permissions to operate across namespaces. Packing control-planes via namespaces provides you better resource utilisation while providing you isolation from the data-plane (where the actual workload is scheduled).</p></li><li><p><code>rolebinding.yaml</code> - binds the <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#role-and-clusterrole" target="_blank" rel="noreferrer">ClusterRole</a> defined in <code>druid-clusterrole.yaml</code> to the <a href="https://kubernetes.io/docs/concepts/security/service-accounts/" target="_blank" rel="noreferrer">ServiceAccount</a> defined in <code>service-account.yaml</code>.</p></li><li><p><code>service.yaml</code> - defines a <code>Cluster IP</code> <a href="https://kubernetes.io/docs/concepts/services-networking/service/" target="_blank" rel="noreferrer">Service</a> allowing other control-plane components to communicate to <code>http</code> endpoints exposed out of etcd-druid (e.g. enables <a href="https://prometheus.io/" target="_blank" rel="noreferrer">prometheus</a> to scrap metrics, validating webhook to be invoked upon change to <code>Etcd</code> CR etc.)</p></li><li><p><code>secret-ca-crt.yaml</code> - Contains the base64 encoded CA certificate used for the etcd-druid webhook server.</p></li><li><p><code>secret-server-tls-crt.yaml</code> - Contains the base64 encoded server certificate used for the etcd-druid webhook server.</p></li><li><p><code>validating-webhook-config.yaml</code> - Configuration for all webhooks that etcd-druid registers to the webhook server. At the time of writing this document <a href="/docs/other-components/etcd-druid/concepts/etcd-cluster-resource-protection/">EtcdComponents</a> webhook gets registered.</p></li></ul><h2 id="etcd-cluster-size" tabindex="-1">Etcd cluster size <a class="header-anchor" href="#etcd-cluster-size" aria-label="Permalink to &quot;Etcd cluster size&quot;">​</a></h2><p><a href="https://etcd.io/docs/v3.3/faq/#why-an-odd-number-of-cluster-members" target="_blank" rel="noreferrer">Recommendation</a> from upstream etcd is to always have an odd number of members in an <code>Etcd</code> cluster.</p><h2 id="mounted-volume" tabindex="-1">Mounted Volume <a class="header-anchor" href="#mounted-volume" aria-label="Permalink to &quot;Mounted Volume&quot;">​</a></h2><p>All <code>Etcd</code> cluster member <a href="https://kubernetes.io/docs/concepts/workloads/pods/" target="_blank" rel="noreferrer">Pods</a> provisioned by etcd-druid mount a <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/" target="_blank" rel="noreferrer">Persistent Volume</a>. A mounted persistent storage helps in faster recovery in case of single-member transient failures. <code>etcd</code> is I/O intensive and its performance is heavily dependent on the <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/" target="_blank" rel="noreferrer">Storage Class</a>. It is therefore recommended that high performance SSD drives be used.</p><p>At the time of writing this document etcd-druid provisions the following volume types:</p><table tabindex="0"><thead><tr><th>Cloud Provider</th><th>Type</th><th>Size</th></tr></thead><tbody><tr><td>AWS</td><td>GP3</td><td>25Gi</td></tr><tr><td>Azure</td><td>Premium SSD</td><td>33Gi</td></tr><tr><td>GCP</td><td>Performance (SSD) Persistent Disks (pd-ssd)</td><td>25Gi</td></tr></tbody></table><blockquote><p>Also refer: <a href="https://etcd.io/docs/v3.4/op-guide/hardware/#disks" target="_blank" rel="noreferrer">Etcd Disk recommendation</a>.</p><p>Additionally, each cloud provider offers redundancy for managed disks. You should choose redundancy as per your availability requirement.</p></blockquote><h2 id="backup-restore" tabindex="-1">Backup &amp; Restore <a class="header-anchor" href="#backup-restore" aria-label="Permalink to &quot;Backup &amp; Restore&quot;">​</a></h2><p>A permanent quorum loss or data-volume corruption is a reality in production clusters and one must ensure that data loss is minimized. <code>Etcd</code> clusters provisioned via etcd-druid offer two levels of data-protection</p><p>Via <a href="https://github.com/gardener/etcd-backup-restore" target="_blank" rel="noreferrer">etcd-backup-restore</a> all clusters started via etcd-druid get the capability to regularly take delta &amp; full snapshots. These snapshots are stored in an object store. Additionally, a <code>snapshot-compaction</code> job is run to compact and defragment the latest snapshot, thereby reducing the time it takes to restore a cluster in case of a permanent quorum loss. You can read the <a href="/docs/other-components/etcd-druid/recovering-etcd-clusters/">detailed guide</a> on how to restore from permanent quorum loss.</p><p>It is therefore recommended that you configure an <code>Object store</code> in the cloud/infra provider of your choice, enabled backup &amp; restore functionality by filling in <a href="https://github.com/gardener/etcd-druid/blob/55efca1c8f6c852b0a4e97f08488ffec2eed0e68/api/v1alpha1/etcd.go#L143" target="_blank" rel="noreferrer">store</a> configuration of an <code>Etcd</code> custom CR.</p><h3 id="ransomware-protection" tabindex="-1">Ransomware protection <a class="header-anchor" href="#ransomware-protection" aria-label="Permalink to &quot;Ransomware protection&quot;">​</a></h3><p>Ransomware is a form of malware designed to encrypt files on a device, rendering any files and the systems that rely on them unusable. All cloud providers (<a href="https://aws.amazon.com/s3/features/object-lock/" target="_blank" rel="noreferrer">aws</a>, <a href="https://cloud.google.com/storage/docs/bucket-lock" target="_blank" rel="noreferrer">gcp</a>, <a href="https://learn.microsoft.com/en-us/azure/storage/blobs/immutable-storage-overview" target="_blank" rel="noreferrer">azure</a>) provide a feature of immutability that can be set at the bucket/object level which provides <code>WORM</code> access to objects as long as the bucket/lock retention duration.</p><p>All delta &amp; full snapshots that are periodically taken by <code>etcd-backup-restore</code> are stored in Object store provided by a cloud provider. It is recommended that these backups be protected from ransomware protection by turning locking at the bucket/object level.</p><h2 id="security" tabindex="-1">Security <a class="header-anchor" href="#security" aria-label="Permalink to &quot;Security&quot;">​</a></h2><h3 id="use-distroless-container-images" tabindex="-1">Use Distroless Container Images <a class="header-anchor" href="#use-distroless-container-images" aria-label="Permalink to &quot;Use Distroless Container Images&quot;">​</a></h3><p>It is generally recommended to use a minimal base image which additionally reduces the attack surface. Google&#39;s <a href="https://github.com/GoogleContainerTools/distroless" target="_blank" rel="noreferrer">Distroless</a> is one way to reduce the attack surface and also minimize the size of the base image. It provides the following benefits:</p><ul><li>Reduces the attack surface</li><li>Minimizes vulnerabilities</li><li>No shell</li><li>Reduced size - only includes what is necessary</li></ul><p>For every <code>Etcd</code> cluster provisioned by etcd-druid, <code>distroless</code> images are used as base images.</p><h3 id="enable-tls-for-peer-and-client-communication" tabindex="-1">Enable TLS for Peer and Client communication <a class="header-anchor" href="#enable-tls-for-peer-and-client-communication" aria-label="Permalink to &quot;Enable TLS for Peer and Client communication&quot;">​</a></h3><p>Generally you should enable TLS for peer and client communication for an <code>Etcd</code> cluster. To enable TLS CA certificate, server and client certificates needs to be generated. You can refer to the list of TLS artifacts that are generated for an <code>Etcd</code> cluster provisioned by etcd-druid <a href="/docs/other-components/etcd-druid/securing-etcd-clusters/">here</a>.</p><h3 id="enable-tls-for-druid-webhooks" tabindex="-1">Enable TLS for Druid Webhooks <a class="header-anchor" href="#enable-tls-for-druid-webhooks" aria-label="Permalink to &quot;Enable TLS for Druid Webhooks&quot;">​</a></h3><p>If you choose to enable webhooks in <code>etcd-druid</code> then it is necessary to create a separate CA and server certificate to be used by the webhooks.</p><h3 id="rotate-tls-artifacts" tabindex="-1">Rotate TLS artifacts <a class="header-anchor" href="#rotate-tls-artifacts" aria-label="Permalink to &quot;Rotate TLS artifacts&quot;">​</a></h3><p>It is generally recommended to rotate all TLS certificates to reduce the chances of it getting leaked or have expired. Kubernetes does not support revocation of certificates (see <a href="https://github.com/kubernetes/kubernetes/issues/18982" target="_blank" rel="noreferrer">issue#18982</a>). One possible way to revoke certificates is to also revoke the entire chain including CA certificates.</p><h2 id="scaling-etcd-pods" tabindex="-1">Scaling etcd pods <a class="header-anchor" href="#scaling-etcd-pods" aria-label="Permalink to &quot;Scaling etcd pods&quot;">​</a></h2><p><code>etcd</code> clusters cannot be scaled-out horizontly to meet the increased traffic/storage demand for the following reasons:</p><ul><li>There is a soft limit of 8GB and a hard limit of 10GB for the etcd DB beyond which perfomance and stability of etcd is not guaranteed.</li><li>All members of etcd maintain the entire replica of the entire DB, thus scaling-out will not really help if the storage demand grows.</li><li>Increasing the number of cluster members beyond 5 also increases the cost of consensus amongst now a larger quorum, increases load on the single leader as it needs to also participate in bringing up <a href="https://etcd.io/docs/v3.3/learning/learner/" target="_blank" rel="noreferrer">etcd learner</a>.</li></ul><p>Therefore the following is recommended:</p><ul><li>To meet the increased demand, configure a <a href="https://github.com/kubernetes/autoscaler/tree/cecb34cb863fb015264098b5379bdba40a9113cf/vertical-pod-autoscaler" target="_blank" rel="noreferrer">VPA</a>. You have to be careful on selection of <code>containerPolicies</code>, <code>targetRef</code>.</li><li>To meet the increased demand in storage etcd-druid already configures each etcd member to <a href="https://etcd.io/docs/v3.4/op-guide/maintenance/#auto-compaction" target="_blank" rel="noreferrer">auto-compact</a> and it also configures periodic <a href="https://etcd.io/docs/v3.4/op-guide/maintenance/#defragmentation" target="_blank" rel="noreferrer">defragmentation</a> of the etcd DB. The only case this will not help is when you only have unique writes all the time.</li></ul><p>!!! note Care should be taken with usage of VPA. While it helps to vertically scale up etcd-member pods, it also can cause transient quorum loss. This is a direct consequence of the design of VPA - where recommendation is done by <a href="https://github.com/kubernetes/autoscaler/blob/2800c70d425b89e88cb6e608df494a0cd21f242d/vertical-pod-autoscaler/pkg/recommender/README.md" target="_blank" rel="noreferrer">Recommender</a> component, <a href="https://github.com/kubernetes/autoscaler/blob/2800c70d425b89e88cb6e608df494a0cd21f242d/vertical-pod-autoscaler/pkg/updater/README.md" target="_blank" rel="noreferrer">Updater</a> evicts the pods that do not have the resources recommended by the <code>Recommender</code> and <a href="https://github.com/kubernetes/autoscaler/blob/2800c70d425b89e88cb6e608df494a0cd21f242d/vertical-pod-autoscaler/pkg/admission-controller/README.md" target="_blank" rel="noreferrer">Admission Controller</a> which updates the resources on the Pods. All these three components act asynchronously and can fail independently, so while VPA respects PDB&#39;s it can easily enter into a state where updater evicts a pod while respecting PDB but the admission controller fails to apply the recommendation. The pod comes with a default resources which still differ from the recommended values, thus causing a repeat eviction. There are other race conditions that can also occur and one needs to be careful of using VPA for quorum based workloads.</p><h2 id="high-availability" tabindex="-1">High Availability <a class="header-anchor" href="#high-availability" aria-label="Permalink to &quot;High Availability&quot;">​</a></h2><p>To ensure that an <code>Etcd</code> cluster is highly available, following is recommended:</p><h3 id="ensure-that-the-etcd-cluster-members-are-spread" tabindex="-1">Ensure that the <code>Etcd</code> cluster members are spread <a class="header-anchor" href="#ensure-that-the-etcd-cluster-members-are-spread" aria-label="Permalink to &quot;Ensure that the \`Etcd\` cluster members are spread&quot;">​</a></h3><p><code>Etcd</code> cluster members should always be spread across nodes. This provides you failure tolerance at the node level. For failure tolerance of a zone, it is recommended that you spread the <code>Etcd</code> cluster members across zones. We recommend that you use a combination of <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/" target="_blank" rel="noreferrer">TopologySpreadConstraints</a> and <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity" target="_blank" rel="noreferrer">Pod Anti-Affinity</a>. To set the scheduling constraints you can either specify these constraints using <a href="https://github.com/gardener/etcd-druid/blob/55efca1c8f6c852b0a4e97f08488ffec2eed0e68/api/v1alpha1/etcd.go#L257-L265" target="_blank" rel="noreferrer">SchedulingConstraints</a> in the <code>Etcd</code> custom resource or use a <a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/" target="_blank" rel="noreferrer">MutatingWebhook</a> to dynamically inject these into pods.</p><p>An example of scheduling constraints for a multi-node cluster with zone failure tolerance will be:</p><div class="language-yaml vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">yaml</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  topologySpreadConstraints</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  - </span><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">labelSelector</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">      matchLabels</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        app.kubernetes.io/component</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">etcd-statefulset</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        app.kubernetes.io/managed-by</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">etcd-druid</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        app.kubernetes.io/name</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">etcd-main</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        app.kubernetes.io/part-of</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">etcd-main</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    maxSkew</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    minDomains</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    topologyKey</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">kubernetes.io/hostname</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    whenUnsatisfiable</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">DoNotSchedule</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  - </span><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">labelSelector</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">      matchLabels</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        app.kubernetes.io/component</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">etcd-statefulset</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        app.kubernetes.io/managed-by</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">etcd-druid</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        app.kubernetes.io/name</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">etcd-main</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        app.kubernetes.io/part-of</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">etcd-main</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    maxSkew</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    minDomains</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    topologyKey</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">topology.kubernetes.io/zone</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    whenUnsatisfiable</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">DoNotSchedule</span></span></code></pre></div><p>For a 3 member etcd-cluster, the above TopologySpreadConstraints will ensure that the members will be spread across zones (assuming there are 3 zones -&gt; minDomains=3) and no two members will be on the same node.</p><h3 id="optimize-network-cost" tabindex="-1">Optimize Network Cost <a class="header-anchor" href="#optimize-network-cost" aria-label="Permalink to &quot;Optimize Network Cost&quot;">​</a></h3><p>In most cloud providers there is no network cost (ingress/egress) for any traffic that is confined within a single zone. For <code>Zonal</code> failure tolerance, it will become imperative to spread the <code>Etcd</code> cluster across zones within a region. Knowing that an <code>Etcd</code> cluster members are quite chatty (leader election, consensus building for writes and linearizable reads etc.), this can add to the network cost.</p><p>One could evaluate using <a href="https://kubernetes.io/docs/concepts/services-networking/topology-aware-routing/" target="_blank" rel="noreferrer">TopologyAwareRouting</a> which reduces cross-zonal traffic thus saving costs and latencies.</p><p>!!! tip You can read about how it is done in Gardener <a href="/docs/gardener/topology_aware_routing/">here</a>.</p><h2 id="metrics-alerts" tabindex="-1">Metrics &amp; Alerts <a class="header-anchor" href="#metrics-alerts" aria-label="Permalink to &quot;Metrics &amp; Alerts&quot;">​</a></h2><p>Monitoring <code>etcd</code> metrics is essential for fine tuning <code>Etcd</code> clusters. etcd already exports a lot of <a href="https://etcd.io/docs/v3.4/metrics/" target="_blank" rel="noreferrer">metrics</a>. You can see the complete list of metrics that are exposed out of an <code>Etcd</code> cluster provisioned by etcd-druid <a href="/docs/other-components/etcd-druid/monitoring/metrics/">here</a>. It is also recommended that you configure an alert for <a href="https://etcd.io/docs/v3.2/op-guide/maintenance/#space-quota" target="_blank" rel="noreferrer">etcd space quota alarms</a>.</p><h2 id="hibernation" tabindex="-1">Hibernation <a class="header-anchor" href="#hibernation" aria-label="Permalink to &quot;Hibernation&quot;">​</a></h2><p>If you have a concept of <code>hibernating</code> kubernetes clusters, then following should be kept in mind:</p><ul><li>Before you bring down the <code>Etcd</code> cluster, leverage the capability to take a <code>full snapshot</code> which captures the state of the etcd DB and stores it in the configured Object store. This ensures that when the cluster is woken up from hibernation it can restore from the last state with no data loss.</li><li>To save costs you should consider deleting the <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims" target="_blank" rel="noreferrer">PersistentVolumeClaims</a> associated to the StatefulSet pods. However, it must be ensured that you take a full snapshot as highlighted in the previous point.</li><li>When the cluster is woken up from hibernation then you should do the following (assuming prior to hibernation the cluster had a size of 3 members): <ul><li>Start the <code>Etcd</code> cluster with 1 replica. Let it restore from the last full snapshot.</li><li>Once the cluster reports that it is ready, only then increase the replicas to its original value (e.g. 3). The other two members will start up each as learners and post learning they will join as voting members (<code>Followers</code>).</li></ul></li></ul><h2 id="reference" tabindex="-1">Reference <a class="header-anchor" href="#reference" aria-label="Permalink to &quot;Reference&quot;">​</a></h2><ul><li>A nicely written <a href="/blog/2023/03-27-high-availability-and-zone-outage-toleration/">blog post</a> on <code>High Availability and Zone Outage Toleration</code> has a lot of recommendations that one can borrow from.</li></ul>`,54)]))}const m=t(o,[["render",i]]);export{u as __pageData,m as default};
