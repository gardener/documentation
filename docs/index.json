[
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/howto/access_pod_from_local/",
	"title": "Access a port of a pod locally",
	"tags": [],
	"description": "",
	"content": " Question You deployed an application with a web UI or an internal endpoint in your Kubernetes (K8s) cluster. How could I access this endpoint without an external load balancer (e.g. Ingress)? This tutorial presents two options:\n Using Kubernetes port forward Using Kubernetes apiserver proxy  Please note that the options described here are mostly for quick testing or troublshoot your application. For enabling access to your application for productive environment, please refer to Access my service\nSolution 1: Using Kubernetes port forward You could use the port forwarding functionality of kubectl to access the pods from your local host without involving a service.\nTo access any pod follow these steps: - Run kubectl get pods - Note down the name of the pod in question as \u0026lt;your-pod-name\u0026gt; - Run kubectl port-forward \u0026lt;your-pod-name\u0026gt; \u0026lt;local-port\u0026gt;:\u0026lt;your-app-port\u0026gt; - Run a web browser or curl locally and enter the URL http(s)://localhost:\u0026lt;local-port\u0026gt;\nIn addition, kubectl port-forward allows using resource name, such as deployment name, service name, to select a matching pod to port forward. More details in the Kubernetes documentation.\nThe main drawback of this approach is that the pods name changes as soon as it is restarted. Moreover, you need to have a web browser on your client and you need to make sure that the local port is not already used by an application running on your system. Finally, sometimes the port forwarding is canceled due to non obvious reasons. This leads to a kind of shaky approach. A more stable possibility is based on accessing the app via the kube-proxy, which accesses the corresponding service.\nSolution 2: Using the apiserver proxy of your K8S cluster There are several different proxies when using Kubernetes, the official documentation provides a good overview.\nIn this tutorial we are using apiserver proxy to enable accessing services in your cluster without Ingress. Different from the first solution, a service is required for the second solution .\nUse following format to compose URL accessing your service through existing proxy on K8S cluster. For a detailed discussion of the format please refer to official documentation\nhttps://\u0026lt;your-cluster-master\u0026gt;/api/v1/namespace/\u0026lt;your-namespace\u0026gt;/services/\u0026lt;your-service\u0026gt;:\u0026lt;your-service-port\u0026gt;/proxy/\u0026lt;service-endpoint\u0026gt;\nExample:\n   your-cluster-master your-namespace your-service your-service-port your-service-endpoint url to access service     api.testclstr.cpet.k8s.sapcloud.io default nginx-svc 80 / url   api.testclstr.cpet.k8s.sapcloud.io default docker-nodejs-svc 4500 /cpu?baseNumber=4 url    There are applications, which do not yet support relative URLs like Prometheus (as of end of November, 2017). This typically leads to missing JavaScript objects as it could be investigated with your browsers development tools. In this case please use the port-forward approach described above.\n"
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/howto/service-access/",
	"title": "Access my service from outside Kubernetes cluster",
	"tags": [],
	"description": "Is there an ingress deployed and how is it configured",
	"content": " TL;DR To expose your application / service for access from outside the cluster, following options exist: - Kubernetes Service of type LoadBalancer - Kubernetes Service of type \u0026lsquo;NodePort\u0026rsquo; + Ingress\nThis tutorial discusses how to enable access to your application from outside Kubernetes cluster (sometimes called North-South traffic). For internal communication amongst pods and services (sometimes called East-West traffic) there are many literatures, here is one brief example.\nService Types A Service in Kubernetes is an abstraction which defines a logical set of Pods and a policy by which to access them.\nServices can be exposed in different ways by specifying a type in the service spec, and different types determine accessibility from inside and outside of cluster. - ClusterIP - NodePort - LoadBalancer\nType ExternalName is a special case of service and not discussed here.\nType ClusterIP A service of type ClusterIP exposes service on an internal IP in the cluster, which makes the service only reachable from within the cluster. This is the default value if no type is specified.\napiVersion: apps/v1beta2 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx-app replicas: 1 template: metadata: labels: app: nginx-app spec: containers: - name: nginx image: nginx:1.13.12 ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: labels: app: nginx-app name: nginx-svc namespace: default spec: type: ClusterIP # use ClusterIP as type here ports: - port: 80 selector: app: nginx-app  Execute following commands to create deployement and service\nkubectl create -f \u0026lt;Your yaml file name\u0026gt;  Checking the service status\n$ kubectl get svc nginx-svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-svc ClusterIP 100.66.125.61 \u0026lt;none\u0026gt; 80/TCP 45m  As shown above, the service is assigned with a cluster ip address and port 80 as defined in yaml file. You can test the service as below:\n# list all existing pods in cluster $ kubectl get po NAME READY STATUS RESTARTS AGE docker-nodejs-app-76b77494-vwv4d 1/1 Running 0 11d nginx-deployment-74d949bf69-nvdzs 1/1 Running 0 1h privileged-pod 1/1 Running 0 11d # test service from within the cluster on the same pod $ kubectl exec -it nginx-deployment-74d949bf69-nvdzs curl 100.66.125.61:80 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 612 100 612 0 0 1006k 0 --:--:-- --:--:-- --:--:-- 597k \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; ...    Tip - The service is also accessible from any container (even on different pod) within the same cluster, e.g. kubectl -it exec \u0026lt;another POD_NAME\u0026gt; curl \u0026lt;YourServiceClusterIP:YourPort\u0026gt;. You need to make sure command curl is installed in the container. - You can also find out the dns name of the ClusterIP by command kubectl exec -it \u0026lt;POD_NAME\u0026gt; nslookup \u0026lt;ClusterIP\u0026gt;, then replace the IP address with the resolved name in your test. The resolved name typically looks like nginx-svc.default.svc.cluster.local where nginx-svc is the name of your service defined in yaml.\n Type NodePort Following previous example, just replace the type with NodePort\n... spec: type: NodePort ports: - port: 80 ...  A service of type NodePort is a ClusterIP service with an additional capability: it is reachable at the IP address of the node as well as at the assigned cluster IP on the services network. The way this is accomplished is pretty straightforward: when Kubernetes creates a NodePort service kube-proxy allocates a port in the range 30000–32767 and opens this port on every node (thus the name “NodePort”). Connections to this port are forwarded to the service’s cluster IP. If we create the service above and run kubectl get svc \u0026lt;your-service\u0026gt;, we can see the NodePort that has been allocated for it.\nNotice in following example, in addition to port 80, port 32521 has been opened as well on node, in contrast to the output of \u0026ldquo;ClusterIP\u0026rdquo; case where only port 80 is opened.\n$ kubectl get svc nginx-svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-svc NodePort 100.70.105.182 \u0026lt;none\u0026gt; 80:32521/TCP 16m  Therefore you can access the service from within the cluster with two options:\n Access via ClusterIP:port  #via ClusterIP kubectl exec -it nginx-deployment-74d949bf69-7n6bs curl 100.70.105.182:80 #via internal name of ClusterIP kubectl exec -it nginx-deployment-74d949bf69-7n6bs curl nginx-svc.default.svc.cluster.local:80   Access via NodeIP:NodePort  # First find out the Node IP address $ kubectl describe node Name: ip-10-250-20-203.eu-central-1.compute.internal Roles: node Addresses: InternalIP: 10.250.20.203 InternalDNS: ip-10-250-20-203.eu-central-1.compute.internal Hostname: ip-10-250-20-203.eu-central-1.compute.internal ... #via NodeIP:NodePort kubectl exec -it nginx-deployment-74d949bf69-7n6bs curl 10.250.20.203:32521 #via internal name of NodeIP kubectl exec -it nginx-deployment-74d949bf69-7n6bs curl ip-10-250-20-203.eu-central-1.compute.internal:32521  Type LoadBalancer LoadBalancer type is the simplest approach, which is created by specifying type as LoadBalancer.\napiVersion: apps/v1beta2 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx-app replicas: 1 template: metadata: labels: app: nginx-app spec: containers: - name: nginx image: nginx:1.13.12 ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: labels: app: nginx-app name: nginx-svc namespace: default spec: type: LoadBalancer # use LoadBalancer as type here ports: - port: 80 selector: app: nginx-app  Once the service is created, it contains an external-ip as following:\n$ kubectl get services -l app=nginx-app -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR nginx-svc LoadBalancer 100.67.182.148 a54a62300696611e88ba00af02406931-1787163476.eu-central-1.elb.amazonaws.com 80:31196/TCP 9m app=nginx-app  A service of type LoadBalancer has all the capabilities of a NodePort service plus the ability to build out a complete ingress path. Hence the service can be accessible from outside the cluster without additional components(e.g. Ingress). To test the External-IP from outside cluster, try following (note that curl command is triggered locally):\n$ curl http://a54a62300696611e88ba00af02406931-1787163476.eu-central-1.elb.amazonaws.com StatusCode : 200 StatusDescription : OK Content : \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;... RawContent : HTTP/1.1 200 OK ...  Obviously the service can also be accessed from within the cluster. You can tests the same way as described in section NodePort.\nLoadBalancer vs. Ingress As presented in previous section, only LoadBalancer type of service can enable access from outside the cluster. However this approach has its own limitation. You cannot configure load balancer to terminate HTTPS traffic, virtual hosts or path-based routing. In Kubernetes 1.2 a separate resource called Ingress is introduced.\nWhy an Ingress LoadBalancer services are all about extending a single service to support external clients. By contrast an Ingress is a separate resource that configures a load balancer much more flexibly. The Ingress API supports TLS termination, virtual hosts, and path-based routing. It can easily set up a load balancer to handle multiple backend services. In addition, there is also difference in how the traffic routing is realized . In the case of the LoadBalancer service, the traffic that enters through the external load balancer is forwarded to the kube-proxy that in turn forwards the traffic to the selected pods. In contrast, the Ingress load balancer forwards the traffic straight to the selected pods which is more efficient.\nEvery service of the \u0026ldquo;LoadBalancer\u0026rdquo; type will be charged by the cloud provider for at least 40$/month. Ten Services ends up in 400$/month just for the load balancing.\nHow to use the ingress? In the cluster, a nginx-ingress controller has been deployed for you as an LB service and also registered the DNS record. Depending on how your cluster is defined, the DNS registration is performed under following conventions:\n k8s-hana.ondemand.com  \u0026lt;gardener_cluster_name\u0026gt;.\u0026lt;gardener_project_name\u0026gt;.shoot.canary.k8s-hana.ondemand.com.\nBoth \u0026lt;gardener_cluster_name\u0026gt; and \u0026lt;gardener_project_name\u0026gt; are defined in Gardener which can be determined on Gardener dashboard.\nThis results in the following default DNS endpoints: * api.\u0026lt;cluster_domain\u0026gt; Kubernetes API * *.ingress.\u0026lt;cluster_domain\u0026gt; Internal nginx ingress\nExample: Configure an Ingress resource with Service type: NodePort With the configuration below you can reach your service nginx-svc with:\nhttp://test.ingress.\u0026lt;GARDENER-CLUSTER-NAME\u0026gt;.\u0026lt;GARDENER-PROJECT-NAME\u0026gt;.shoot.canary.k8s-hana.ondemand.com\napiVersion: apps/v1beta2 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx-app replicas: 1 template: metadata: labels: app: nginx-app spec: containers: - name: nginx image: nginx:1.13.12 ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: labels: app: nginx-app name: nginx-svc namespace: default spec: type: NodePort ports: - port: 80 selector: app: nginx-app --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginxsvc-ingress spec: rules: - host: nginxsvc.ingress.eecluster.cpet.shoot.canary.k8s-hana.ondemand.com http: paths: - backend: serviceName: nginx-svc servicePort: 80  You can show the created ingress entry and test it.\n$ kubectl get ing NAME HOSTS ADDRESS PORTS AGE nginxsvc-ingress nginxsvc.ingress.eecluster.cpet.shoot.canary.k8s-hana.ondemand.com 10.250.20.203 80 29s $ curl nginxsvc.ingress.eecluster.cpet.shoot.canary.k8s-hana.ondemand.com StatusCode : 200 StatusDescription : OK Content : \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; ...  Reference:  Concepts: Kubernetes Service Concepts: Connecting Applications with Services Tutorial: Using a Service to Expose Your App Tutorial: Using Source IP Kubernetes Networking Accessing Kubernetes Pods from Outside of the Cluster  "
},
{
	"uri": "https://gardener.github.io/website/using-gardener/administrator/",
	"title": "Administrator",
	"tags": [],
	"description": "",
	"content": "Learning Material Everything you need to know to operate gardener.    "
},
{
	"uri": "https://gardener.github.io/website/010-blog/2018_week_22/",
	"title": "Anti Patterns",
	"tags": [],
	"description": "",
	"content": " Running as root user Whenever possible, do not run containers as root users. One could be tempted to say that Kubernetes Pods and Node are well separated. The host and the container share the same kernel. If the container is compromised, a root user can damage the underlying node. Use RUN groupadd -r anygroup \u0026amp;\u0026amp; useradd -r -g anygroup myuser to create a group and a user in it. Use the USER command to switch to this user. \nStoring data or logs in containers Containers are ideal for stateless applications and should be transient. This means that no data or logs should be stored in the container, as they are lost when the container is closed. If absolutely necessary, you can use persistence volumes instead to persist them outside the containers. However, an ELK stack is preferred for storing and processing log files.\n..read some more on Common Kubernetes Antipattern.\n"
},
{
	"uri": "https://gardener.github.io/website/using-gardener/developer/topic/",
	"title": "App Developer",
	"tags": [],
	"description": "",
	"content": "Learning Material Everything you need to know about running your software.      by Topic     by Experience Level  \n\n"
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/app/",
	"title": "Apps",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.github.io/website/030-architecture/",
	"title": "Architecture",
	"tags": [],
	"description": "",
	"content": " Official Definition - What is Kubernetes?  \u0026ldquo;Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.\u0026rdquo;\n Introduction - Basic Principle The foundation of the Gardener (providing Kubernetes Clusters as a Service) is Kubernetes itself, because Kubernetes is the go-to solution to manage software in the Cloud, even when it\u0026rsquo;s Kubernetes itself (see also OpenStack which is provisioned more and more on top of Kubernetes as well).\nWhile self-hosting, meaning to run Kubernetes components inside Kubernetes, is a popular topic in the community, we apply a special pattern catering to the needs of our cloud platform to provision hundreds or even thousands of clusters. We take a so-called \u0026ldquo;seed\u0026rdquo; cluster and seed the control plane (such as the API server, scheduler, controllers, etcd persistence and others) of an end-user cluster, which we call \u0026ldquo;shoot\u0026rdquo; cluster, as pods into the \u0026ldquo;seed\u0026rdquo; cluster. That means one \u0026ldquo;seed\u0026rdquo; cluster, of which we will have one per IaaS and region, hosts the control planes of multiple \u0026ldquo;shoot\u0026rdquo; clusters. That allows us to avoid dedicated hardware/virtual machines for the \u0026ldquo;shoot\u0026rdquo; cluster control planes. We simply put the control plane into pods/containers and since the \u0026ldquo;seed\u0026rdquo; cluster watches them, they can be deployed with a replica count of 1 and only need to be scaled out when the control plane gets under pressure, but no longer for HA reasons. At the same time, the deployments get simpler (standard Kubernetes deployment) and easier to update (standard Kubernetes rolling update). The actual \u0026ldquo;shoot\u0026rdquo; cluster consists only out of the worker nodes (no control plane) and therefore the users may get full administrative access to their clusters.\nSetting The Scene - Components and Procedure We provide a central operator UI, which we call the \u0026ldquo;Gardener Dashboard\u0026rdquo;. It talks to a dedicated cluster, which we call the \u0026ldquo;Garden\u0026rdquo; cluster and uses custom resources managed by an aggregated API server, one of the general extension concepts of Kubernetes) to represent \u0026ldquo;shoot\u0026rdquo; clusters. In this \u0026ldquo;Garden\u0026rdquo; cluster runs the \u0026ldquo;Gardener\u0026rdquo;, which is basically a Kubernetes controller that watches the custom resources and acts upon them, i.e. creates, updates/modifies, or deletes \u0026ldquo;shoot\u0026rdquo; clusters. The creation follows basically these steps: * Create a namespace in the \u0026ldquo;seed\u0026rdquo; cluster for the \u0026ldquo;shoot\u0026rdquo; cluster which will host the \u0026ldquo;shoot\u0026rdquo; cluster control plane * Generate secrets and credentials which the worker nodes will need to talk to the control plane * Create the infrastructure (using Terraform), which basically consists out of the network setup) * Deploy the \u0026ldquo;shoot\u0026rdquo; cluster control plane into the \u0026ldquo;shoot\u0026rdquo; namespace in the \u0026ldquo;seed\u0026rdquo; cluster, containing the \u0026ldquo;machine-controller-manager\u0026rdquo; pod * Create machine CRDs in the \u0026ldquo;seed\u0026rdquo; cluster, describing the configuration and the number of worker machines for the \u0026ldquo;shoot\u0026rdquo; (the machine-controller-manager watches the CRDs and creates virtual machines out of it) * Wait for the \u0026ldquo;shoot\u0026rdquo; cluster API server to become responsive (pods will be scheduled, persistent volumes and load balancers are created by Kubernetes via the respective cloud provider) * Finally we deploy kube-system daemons like kube-proxy and further add-ons like the dashboard into the \u0026ldquo;shoot\u0026rdquo; cluster and the cluster becomes active\nOverview Architecture Diagram Note: While the kubelet talks through the front-door (public Internet) to its \u0026ldquo;shoot\u0026rdquo; cluster API server running in the \u0026ldquo;seed\u0026rdquo; cluster, the pods inside the \u0026ldquo;shoot\u0026rdquo; cluster use the Kubernetes service and its load balancer IP to reach the API server. This communication and the reverse communication from the API server to the pod and service IPs happens through a VPN connection that we deploy into \u0026ldquo;seed\u0026rdquo; and \u0026ldquo;shoot\u0026rdquo; clusters.\n"
},
{
	"uri": "https://gardener.github.io/website/045_contribute/20_documentation/25_markup/attachments/",
	"title": "Attachments",
	"tags": [],
	"description": "The Attachments shortcode displays a list of files attached to a page.",
	"content": " The Attachments shortcode displays a list of files attached to a page.\n  Attachments   BachGavotteShort.mp3  (0 ko)   Carroll_AliceAuPaysDesMerveilles.pdf  (0 ko)   adivorciarsetoca00cape.pdf  (0 ko)   hugo.png  (17 ko)   movieselectricsheep-flock-244-32500-2.mp4  (0 ko)    Usage The shortcurt lists files found in a specific folder. Currently, it support two implementations for pages\n If your page is a markdown file, attachements must be place in a folder named like your page and ending with .files.\n  content  _index.md page.files  attachment.pdf  page.md    If your page is a folder, attachements must be place in a nested \u0026lsquo;files\u0026rsquo; folder.\n  content  _index.md page  index.md files  attachment.pdf       Be aware that if you use a multilingual website, you will need to have as many folders as languages.\nThat\u0026rsquo;s all !\nParameters    Parameter Default Description     title \u0026ldquo;Attachments\u0026rdquo; List\u0026rsquo;s title   style \u0026rdquo;\u0026rdquo; Choose between \u0026ldquo;orange\u0026rdquo;, \u0026ldquo;grey\u0026rdquo;, \u0026ldquo;blue\u0026rdquo; and \u0026ldquo;green\u0026rdquo; for nice style   pattern \u0026rdquo;.*\u0026rdquo; A regular expressions, used to filter the attachments by file name. The pattern parameter value must be regular expressions.    For example:\n To match a file suffix of \u0026lsquo;jpg\u0026rsquo;, use .*jpg (not *.jpg). To match file names ending in \u0026lsquo;jpg\u0026rsquo; or \u0026lsquo;png\u0026rsquo;, use .*(jpg|png)  Examples List of attachments ending in pdf or mp4 {{%attachments title=\u0026quot;Related files\u0026quot; pattern=\u0026quot;.*(pdf|mp4)\u0026quot;/%}}  renders as\n  Related files   Carroll_AliceAuPaysDesMerveilles.pdf  (0 ko)   adivorciarsetoca00cape.pdf  (0 ko)   movieselectricsheep-flock-244-32500-2.mp4  (0 ko)    Colored styled box {{%attachments style=\u0026quot;orange\u0026quot; /%}}  renders as\n  Attachments   BachGavotteShort.mp3  (0 ko)   Carroll_AliceAuPaysDesMerveilles.pdf  (0 ko)   adivorciarsetoca00cape.pdf  (0 ko)   hugo.png  (17 ko)   movieselectricsheep-flock-244-32500-2.mp4  (0 ko)    {{%attachments style=\u0026quot;grey\u0026quot; /%}}  renders as\n  Attachments   BachGavotteShort.mp3  (0 ko)   Carroll_AliceAuPaysDesMerveilles.pdf  (0 ko)   adivorciarsetoca00cape.pdf  (0 ko)   hugo.png  (17 ko)   movieselectricsheep-flock-244-32500-2.mp4  (0 ko)    {{%attachments style=\u0026quot;blue\u0026quot; /%}}  renders as\n  Attachments   BachGavotteShort.mp3  (0 ko)   Carroll_AliceAuPaysDesMerveilles.pdf  (0 ko)   adivorciarsetoca00cape.pdf  (0 ko)   hugo.png  (17 ko)   movieselectricsheep-flock-244-32500-2.mp4  (0 ko)    {{%attachments style=\u0026quot;green\u0026quot; /%}}  renders as\n  Attachments   BachGavotteShort.mp3  (0 ko)   Carroll_AliceAuPaysDesMerveilles.pdf  (0 ko)   adivorciarsetoca00cape.pdf  (0 ko)   hugo.png  (17 ko)   movieselectricsheep-flock-244-32500-2.mp4  (0 ko)    "
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/howto/kubectl-apiserver/",
	"title": "Automated deployment",
	"tags": [],
	"description": "Automated deployment via kubectl",
	"content": " Introduction With kubectl you can easily deploy an image from your local environment.\nHowever, what if you want to use a automated deployment script on a CI server (e.g. Jenkins), but don\u0026rsquo;t want to store the KUBECONFIG on that server?\nYou can use kubectl and connect to the API-server of your cluster.\nPrerequisites  Create a service account user   kubectl create serviceaccount deploy-user -n default   Bind a role to the new created serviceuser \u0026gt;!!! warning !!! \u0026ldquo;In this example the preconfigured role \\\u0026ldquo;edit\\\u0026rdquo; and the namespace \\\u0026ldquo;default\\\u0026rdquo; is being used, please adjust the role to a more strict scope! see https://kubernetes.io/docs/admin/authorization/rbac/\u0026quot;\u0026lt;   kubectl create rolebinding deploy-default-role --clusterrole=edit --serviceaccount=default:deploy-user --namespace=default   Get the url of your API-server   APISERVER=$(kubectl config view | grep server | cut -f 2- -d \u0026quot;:\u0026quot; | tr -d \u0026quot; \u0026quot;)   get the SERVICEACCOUNT\nSERVICEACCOUNT=$(kubectl get serviceaccount deploy-user -n default -o json | jq -r .secrets[0].name)  Generate a TOKEN for the serviceaccount\nTOKEN=$(kubectl get secret -n default $SERVICEACCOUNT -o json | jq -r .data.token | base64 -D)   Usage You can deploy your app without having the KUBECONFIG set locally, you just need to pass the environment variables (e.g. store them in the Jenkins credentials store)\n kubectl --server=${APIServer} --token=${TOKEN} --insecure-skip-tls-verify=true apply --filename myapp.yaml  "
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/howto/multi_az/",
	"title": "Availibility Zones",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.github.io/website/010-blog/2018_week_07/",
	"title": "Big things come in small packages",
	"tags": [],
	"description": "",
	"content": " Microservices should tend to use smaller runtimes but you can use what you have today - and this can be a problem in kubernetes.\nSwitching your architecture to microservices instead of a single monolith has many advantages, both in the way you write software and the way it is used throughout its lifecycle. In this post, my attempt is to cover one problem which does not get as much attention and discussion - technology stack size.\nGeneral purpose technology stack There is a tendency to be more generalized in development and to apply this pattern to all services. One feels that a homogeneous image of the technology stack is good if it is the same for all services.\nOne forgets, however, that a large percentage of the integrated infrastructure is not used by all services in the same way, and is therefore only a burden. Thus, resources are wasted and the entire application becomes expensive in operation and scales very badly.\nLight technology stack Due to the lightweight nature of your service, you can run more containers on a physical server and virtual machines. The result is higher resource utilization.\nAdditional, microservices are developed and deployed as containers independently of one another. This means that a development team can be developing, optimizing and deploying a certain microservice without impacting other subsystems.\n"
},
{
	"uri": "https://gardener.github.io/website/components/bouquet/",
	"title": "Bouquet",
	"tags": [],
	"description": "",
	"content": " Bouquet Bouquet is a draft addon manager for the Gardener. It incorporates some of the requested features of the community but not yet all of them.\n Caution: This software is early alpha. It is not meant for production use and shall (currently) only serve as a possible outlook of what is possible with pre-deployed software on Gardener Kubernetes clusters.\n Installation If you want to deploy Bouquet on a target Gardener cluster, run the following:\nhelm install charts/bouquet \\ --name gardener-bouquet \\ --namespace garden  This will deploy Bouquet with the required permissions into your garden cluster.\nStructure As of now, Bouquet comes with two new custom resources: AddonManifest and AddonInstance.\nAn AddonManifest can be considered equivalent to a Helm template. The manifest itself only contains metadata (like the name, default values etc.). The actual content of a manifest is specified via its source attribute. Currently, the only available source is a ConfigMap.\nAn AddonInstance references an AddonManifest and a target Shoot. It may also contain value overrides in its spec. As soon as an AddonInstance is created, Bouquet will apply the values to the templates and then ensure that the objects exist in the target shoot. If an AddonInstance is deleted, Bouquet will also make sure that the created objects are deleted as well.\nExample use case Say you want your cluster to contain istio right from the start. How can you do that?\nFirst you need to get the .yaml files necessary to deploy istio into your cluster. Download an istio release as follows:\nwget -O istio.yaml https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/istio/noauth/istio.yaml  This will fetch a .yaml file containing all necessary kubernetes objects of istio. To make this data available in your garden cluster, create a configmap in your cluster via\nkubectl -n garden create configmap istio-files --from-file ./istio.yaml  Now you need to create an AddonManifest that references this file and push it to Kubernetes. The file could look like the following:\napiVersion: \u0026quot;garden.sapcloud.io/v1alpha1\u0026quot; kind: \u0026quot;AddonManifest\u0026quot; metadata: name: \u0026quot;istio-0.0.1\u0026quot; spec: configMap: \u0026quot;istio-files\u0026quot;  You can submit this manifest to Kubernetes via kubectl (given that you saved the file to addonmanifest.yaml:\nkubectl -n garden apply -f addonmanifest.yaml  Once this is done, the only thing left to do is to create an AddonInstance referencing both your target Shoot and your AddonManifest. This AddonInstance has to be in the same namespace as your target Shoot:\napiVersion: \u0026quot;garden.sapcloud.io/v1alpha1\u0026quot; kind: \u0026quot;AddonInstance\u0026quot; metadata: name: \u0026quot;example\u0026quot; finalizers: - \u0026quot;bouquet\u0026quot; spec: manifest: namespace: \u0026quot;garden\u0026quot; name: \u0026quot;istio\u0026quot; version: \u0026quot;0.0.1\u0026quot; target: shoot: \u0026quot;addon-test\u0026quot;  And apply it via kubectl (given that you saved the file to addoninstance.yaml):\nkubectl -n garden-addon-test apply -f addoninstance.yaml  Bouquet will then start deploying your objects to the target Shoot once it is ready.\nOutlook / Future Since this is just a tech-preview, features like value / chart updates, more efficient templating, company addon guidelines etc. are not yet implemented / yet to come / yet to be discussed. It is also not yet clear whether this should eventually move into the Gardener or remain as a stand-alone component.\nCore points that have to be tackled are: * Fire and forget mode (only deploy objects once, don\u0026rsquo;t monitor afterwards) * Reconciliation (currently, updating behavior is not correctly implemented) * Updates of an addon (-\u0026gt; Update strategies) * Dependent addons / dependency resolution / dependency lifecycle\nAs such, contributions and help on shaping this topic is highly appreciated.\n"
},
{
	"uri": "https://gardener.github.io/website/045_contribute/20_documentation/25_markup/button/",
	"title": "Button",
	"tags": [],
	"description": "Nice buttons on your page.",
	"content": "A button is a just a clickable button with optional icon.\n{{% button href=\u0026quot;https://getgrav.org/\u0026quot; %}}Get Grav{{% /button %}} {{% button href=\u0026quot;https://getgrav.org/\u0026quot; icon=\u0026quot;fa fa-download\u0026quot; %}}Get Grav with icon{{% /button %}} {{% button href=\u0026quot;https://getgrav.org/\u0026quot; icon=\u0026quot;fa fa-download\u0026quot; icon-position=\u0026quot;right\u0026quot; %}}Get Grav with icon right{{% /button %}}  Get Grav   Get Grav with icon  Get Grav with icon right   "
},
{
	"uri": "https://gardener.github.io/website/using-gardener/developer/experience/",
	"title": "By Skill",
	"tags": [],
	"description": "",
	"content": "Learning Material Everything you need to know about running your software.     by Topic      by Experience Level  \n\n"
},
{
	"uri": "https://gardener.github.io/website/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.github.io/website/contribute/code/",
	"title": "Code",
	"tags": [],
	"description": "",
	"content": " Contributing Code How to Contribute to the Open Source Project Gardener    You are welcome to contribute code to Gardener in order to fix a bug or to implement a new feature.\nThe following rules govern code contributions: * Contributions must be licensed under the Apache 2.0 License * You need to sign the Contributor License Agreement. We are using CLA assistant providing a click-through workflow for accepting the CLA. For company contributors additionally the company needs to sign a corporate license agreement. See the following sections for details.\n "
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/howto/commit_secret_fail/",
	"title": "Commit secrets in Github 💀",
	"tags": [],
	"description": "Never ever commit a kubeconfig.yaml into github",
	"content": " Problem If you commit sensitive data, such as a kubeconfig.yaml or SSH key into a Git repository, you can remove it from the history. To entirely remove unwanted files from a repository\u0026rsquo;s history you can use the git filter-branch command.\nThe git filter-branch command rewrite your repository\u0026rsquo;s history, which changes the SHAs for existing commits that you alter and any dependent commits. Changed commit SHAs may affect open pull requests in your repository. I recommend merging or closing all open pull requests before removing files from your repository.\n Warning: - if someone has already checked out the repository, then of course he has the secret on his computer. So ALWAYS revoke the OAuthToken/Password or whatever it was imediately.\n Purging a file from your repository\u0026rsquo;s history  Warning: If you run git filter-branch after stashing changes, you won\u0026rsquo;t be able to retrieve your changes with other stash commands. Before running git filter-branch, we recommend unstashing any changes you\u0026rsquo;ve made. To unstash the last set of changes you\u0026rsquo;ve stashed, run git stash show -p | git apply -R. For more information, see Git Tools Stashing.\n To illustrate how git filter-branch works, we\u0026rsquo;ll show you how to remove your file with sensitive data from the history of your repository and add it to .gitignore to ensure that it is not accidentally re-committed.\nNavigate into the repository\u0026rsquo;s working directory.\ncd YOUR-REPOSITORY  Run the following command, replacing PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA with the path to the file you want to remove, not just its filename.\nThese arguments will: * Force Git to process, but not check out, the entire history of every branch and tag * Remove the specified file, as well as any empty commits generated as a result * Overwrite your existing tags\ngit filter-branch --force --index-filter \\ 'git rm --cached --ignore-unmatch PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA' \\ --prune-empty --tag-name-filter cat -- --all  Add your file with sensitive data to .gitignore to ensure that you don\u0026rsquo;t accidentally commit it again.\necho \u0026quot;YOUR-FILE-WITH-SENSITIVE-DATA\u0026quot; \u0026gt;\u0026gt; .gitignore  Double-check that you\u0026rsquo;ve removed everything you wanted to from your repository\u0026rsquo;s history, and that all of your branches are checked out.\nOnce you\u0026rsquo;re happy with the state of your repository, force-push your local changes to overwrite your GitHub repository, as well as all the branches you\u0026rsquo;ve pushed up:\ngit push origin --force --all  In order to remove the sensitive file from your tagged releases, you\u0026rsquo;ll also need to force-push against your Git tags:\ngit push origin --force --tags   Warning: Tell your collaborators to rebase, not merge, any branches they created off of your old (tainted) repository history. One merge commit could reintroduce some or all of the tainted history that you just went to the trouble of purging.\n  blockquote { border:1px solid red; padding:10px; margin-top:40px; margin-bottom:40px; } blockquote p { font-size: 1.5rem; color: black; }  "
},
{
	"uri": "https://gardener.github.io/website/045_contribute/10_code/15_conf_secrets/",
	"title": "Configuration and Secrets",
	"tags": [],
	"description": "",
	"content": " Configuration In order to establish different configuration settings for the same cloud environment, one has to define CloudProfiles. These profiles define configuration and constraints for allowed values in the Shoot manifest as well.\nSeed clusters have their own resource as well. These resources contain metadata about the respective Seed cluster and a reference to a secret holding the credentials (see below).\nThe Gardener requires some secrets in order to work properly. These secrets are: * Seed cluster secrets, contain the credentials of the cloud provider account in which the Seed cluster is deployed, and a Kubeconfig which can be used to authenticate against the Seed cluster\u0026rsquo;s kube-apiserver, please see this for an example.\n Internal domain secrets (optional), contain the DNS provider credentials (with appropriate privileges) which will be used to create/delete internal DNS records for the Shoot clusters (e.g., example.com), please see this for an example.\n These secrets are used in order to establish a stable endpoint for Shoot clusters which is used internally by all control plane components. It is forbidden to change the internal domain secret if there are existing Shoot clusters.  Default domain secrets (optional), contain the DNS provider credentials (with appropriate privileges) which will be used to create/delete DNS records for the default domain (e.g., example.com), please see this for an example.\n These secrets are used in order to allow not specifying a hosted zone when creating a Shoot cluster in the .spec.dns.hostedZoneID field (useful when a user does not have an own domain/hosted zone but want us to manage it). In this case, based on the provided .spec.dns.domain value, the Gardener tries to find an appropriate secret holding the credentials for the hosted zone of this domain. It will use them to manage the relevant DNS records. Currently, we have implemented AWS Route53 and Google CloudDNS as DNS providers. For Google CloudDNS you need to provide a service account with the DNS Administrator role. For AWS you need to provide a user being assigned to this IAM policy document: bash { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;route53:GetChange\u0026quot;, \u0026quot;route53:GetHostedZone\u0026quot;, \u0026quot;route53:ListResourceRecordSets\u0026quot;, \u0026quot;route53:ChangeResourceRecordSets\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] }   Alerting SMTP secrets (optional), contain the SMTP credentials which will be used by the Alertmanager to send emails on alerts, please see this for an example.\n These secrets are used by the Alertmanager which is deployed next to the Kubernetes control plane of a Shoot cluster in Seed clusters. In case there have been alerting SMTP secrets configured, the Gardener will inject the credentials in the configuration of the Alertmanager. It will use them to send mails to the stated email address in case anything is wrong with the Shoot clusters.  Cloud provider secrets, contains the credentials of the cloud provider account in which Shoot clusters can be deployed, please see this for an example.\n For each Shoot cluster, the Gardener needs to create infrastructure (networks, security groups, technical users, \u0026hellip;) and worker nodes in the desired cloud provider account.   The described secrets are expected to be stored in the so-called Garden namespace. In case the Gardener runs inside a Kubernetes cluster, the Garden namespace is the namespace the Gardener is deployed in (default, can be overwritten). In case it runs outside (local development), the Garden namespace must be specified via a command line flag (see below). The secrets are determined based on labels with key garden.sapcloud.io/role. Please take a look on the above linked examples.\nThe Seed cluster which is used to deploy the control plane of a Shoot cluster can be specified by the user in the Shoot manifest (see here). If it is not specified, the Gardener will try to find an adequate Seed cluster (one deployed in the same region at the same cloud provider) automatically.\nThe cloud provider secrets can be stored in any namespace. With SecretBindings one can reference a secret in the same or in another namespace. These binding objects can also be used to reference Quotas for the specific secret.\nConfiguration file for Gardener controller manager The Gardener controller manager does only support one command line flag which should be a path to a valid configuration file.\nPlease take a look at this example configuration.\n"
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/howto/missing-registry-permission/",
	"title": "Container image not pulled",
	"tags": [],
	"description": "Wrong Container Image or Invalid Registry Permissions",
	"content": " Problem Two of the most common problems are having the wrong container image specified or trying to use private images without providing registry credentials. These are especially tricky when starting to work with Kubernetes or wiring up the cluster for the first time.\nNote: There is no observable difference in Pod status between a missing image and incorrect registry permissions. In either case, Kubernetes will report an ErrImagePull status for the Pods. For this reason, this article deals with both scenarios.\nExample Let\u0026rsquo;s see an example. We\u0026rsquo;ll create a pod named fail referencing a non-existent Docker image:\nkubectl run -i --tty fail --image=tutum/curl:1.123456  the command prompt didn\u0026rsquo;t return and you can press ctrl+c\nError analyse We can then inspect our Pods and see that we have one Pod with a status of ErrImagePull or ImagePullBackOff.\nWDFM33957623A:$ (minikube) kubectl get pods NAME READY STATUS RESTARTS AGE client-5b65b6c866-cs4ch 1/1 Running 1 1m fail-6667d7685d-7v6w8 0/1 ErrImagePull 0 \u0026lt;invalid\u0026gt; vuejs-578574b75f-5x98z 1/1 Running 0 1d WDFM33957623A:$ (minikube)  For some additional information, we can describe the failing Pod.\nkubectl describe pod fail-6667d7685d-7v6w8  As you can see in the events section, your image can\u0026rsquo;t be pulled\nName:\tfail-6667d7685d-7v6w8 Namespace:\tdefault Node:\tminikube/192.168.64.10 Start Time:\tWed, 22 Nov 2017 10:01:59 +0100 Labels:\tpod-template-hash=2223832418 run=fail Annotations:\tkubernetes.io/created-by={\u0026quot;kind\u0026quot;:\u0026quot;SerializedReference\u0026quot;,\u0026quot;apiVersion\u0026quot;:\u0026quot;v1\u0026quot;,\u0026quot;reference\u0026quot;:{\u0026quot;kind\u0026quot;:\u0026quot;ReplicaSet\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;,\u0026quot;name\u0026quot;:\u0026quot;fail-6667d7685d\u0026quot;,\u0026quot;uid\u0026quot;:\u0026quot;cc4ccb3f-cf63-11e7-afca-4a7a1fa05b3f\u0026quot;,\u0026quot;a... . . . . Events: FirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage ---------\t--------\t-----\t----\t-------------\t--------\t------\t------- 1m\t1m\t1\tdefault-scheduler\tNormal\tScheduled\tSuccessfully assigned fail-6667d7685d-7v6w8 to minikube 1m\t1m\t1\tkubelet, minikube\tNormal\tSuccessfulMountVolume\tMountVolume.SetUp succeeded for volume \u0026quot;default-token-9fr6r\u0026quot; 1m\t6s\t4\tkubelet, minikube\tspec.containers{fail}\tNormal\tPulling\tpulling image \u0026quot;tutum/curl:1.123456\u0026quot; 1m\t5s\t4\tkubelet, minikube\tspec.containers{fail}\tWarning\tFailed\tFailed to pull image \u0026quot;tutum/curl:1.123456\u0026quot;: rpc error: code = Unknown desc = Error response from daemon: manifest for tutum/curl:1.123456 not found 1m\t\u0026lt;invalid\u0026gt;\t10\tkubelet, minikube\tWarning\tFailedSync\tError syncing pod 1m\t\u0026lt;invalid\u0026gt;\t6\tkubelet, minikube\tspec.containers{fail}\tNormal\tBackOff\tBack-off pulling image \u0026quot;tutum/curl:1.123456\u0026quot;  So then the question is: Why couldn\u0026rsquo;t Kubernetes pull the image?\nThere are three primary culprits besides network connectivity issues: - The image tag is incorrect - The image doesn\u0026rsquo;t exist - Kubernetes doesn\u0026rsquo;t have permissions to pull that image\nIf you don\u0026rsquo;t notice a typo in your image tag, then it\u0026rsquo;s time to test using your local machine. I usually start by running docker pull on my local development machine with the exact same image tag. In this case, I would run docker pull tutum/curl:1.123456\nIf this succeeds, then it probably means that Kubernetes doesn\u0026rsquo;t have correct permissions to pull that image.\nAdd the docker registry user/pwd to your cluster\nkubectl create secret docker-registry dockersecret --docker-server=https://index.docker.io/v1/ --docker-username=\u0026lt;username\u0026gt; --docker-password=\u0026lt;password\u0026gt; --docker-email=\u0026lt;email\u0026gt;  If the exact image tag fails, then I will test without an explicit image tag - docker pull tutum/curl - which will attempt to pull the latest tag. If this succeeds, then that means the original tag specified doesn\u0026rsquo;t exist. Go to the Docker registry and check which tags for this image are available.\nIf docker pull tutum/curl (without an exact tag) fails, then we have a bigger problem - that image does not exist at all in our image registry. By default, Kubernetes uses the Dockerhub registry. If you\u0026rsquo;re using Artifactory in the DMZ, you\u0026rsquo;ll need to specify the registry URL in the image string. Read Artifactory How To for more details.\n"
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/howto/image-pull-policy/",
	"title": "Container image not updating",
	"tags": [],
	"description": "Updating Images in your cluster during development",
	"content": " Preface A container image should use a fixed tag or the SHA of the image. It should not use the tags latest, head, canary, or other tags that are designed to be floating.\nProblem Most people I\u0026rsquo;ve talked to who have worked with Kubernetes have run into this problem, and it\u0026rsquo;s a real kicker. The story goes something like this:\n Deploy anything using an image tag (e.g. cp-enablement/awesomeapp:1.0) Fix a bug in awesomeapp Build a new image and push it with the same tag (cp-enablement/awesomeapp:1.0) Update my deployment Realize that the bug is still present Repeat 3-5 until you pull your hair out  The problem relates to how Kubernetes decides whether to do a docker pull when starting a container. Since we tagged our image as :1.0, the default pull policy is IfNotPresent. The Kubelet already has a local copy of cp-enablement/awesomeapp:1.0, so it doesn\u0026rsquo;t attempt to do a docker pull. When the new Pods come up, they\u0026rsquo;re still using the old broken Docker image.\nThere are three ways to resolve this:\n Switch to using the tag :latest (DO NOT DO THIS!) Specify ImagePullPolicy: always (not recomended). Use unique tags (best practice)  Solution In the quest to automate myself out of a job, I created a bash script that runs anytime to create a new tag and push them to the registry.\n#!/usr/bin/env bash # Set the docker image name and the coresponding repository # Ensure that you change them in the deployment.yml as well. # You must be logged in with docker login… # # CHANGE THIS TO YOUR Docker.io SETTINGS # PROJECT=awesomeapp REPOSITORY=cp-enablement # causes the shell to exit if any subcommand or pipeline returns a non-zero status. set -e # set debug mode #set -x # build my nodeJS app # npm run build # get latest version ID from the Docker.io registry and increment them # VERSION=$(curl https://registry.hub.docker.com/v1/repositories/$REPOSITORY/$PROJECT/tags | sed -e 's/[][]//g' -e 's/\u0026quot;//g' -e 's/ //g' | tr '}' '\\n' | awk -F: '{print $3}' | grep v| tail -n 1) VERSION=${VERSION:1} ((VERSION++)) VERSION=\u0026quot;v$VERSION\u0026quot; # build the new docker image # echo '\u0026gt;\u0026gt;\u0026gt; Building new image' # Due to a bug in Docker we need to analyse the log to find out if build passed (see https://github.com/dotcloud/docker/issues/1875) docker build --no-cache=true -t $REPOSITORY/$PROJECT:$VERSION . | tee /tmp/docker_build_result.log RESULT=$(cat /tmp/docker_build_result.log | tail -n 1) if [[ \u0026quot;$RESULT\u0026quot; != *Successfully* ]]; then exit -1 fi echo '\u0026gt;\u0026gt;\u0026gt; Push new image' docker push $REPOSITORY/$PROJECT:$VERSION  "
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/",
	"title": "Content",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.github.io/website/contribute/",
	"title": "Contribute",
	"tags": [],
	"description": "",
	"content": " graph TB; B(Contributor) B -- C{I want to..} C --|Code| D(\"fa:fa-code-fork \u0026lt;a href\u0026#61;\u0026#39;./code\u0026#39;\u0026gt;Code Contribute\u0026lt;/a\u0026gt;\") C --|Docs| E(\"fa:fa-paragraph \u0026lt;a href\u0026#61;\u0026#39;./docs\u0026#39;\u0026gt;Doc Contribute\u0026lt;/a\u0026gt;\")  Code of conduct All members of the Gardener community must abide by the CNCF Code of Conduct. Only by respecting each other can we develop a productive, collaborative community. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting gardener.opensource@sap.com and/or a Gardener project maintainer.\nContributing Gardener uses GitHub to manage reviews of pull requests.\n If you are a new contributor see: Steps to Contribute\n If you have a trivial fix or improvement, go ahead and create a pull request, addressing (with @...) a suitable maintainer of this repository (see CODEOWNERS of the repository you want to contribute to) in the description of the pull request.\n If you plan to do something more involved, first discuss your ideas on our mailing list. This will avoid unnecessary work and surely give you and us a good deal of inspiration.\n Relevant coding style guidelines are the Go Code Review Comments and the Formatting and style section of Peter Bourgon\u0026rsquo;s Go: Best Practices for Production Environments.\n  Steps to Contribute Should you wish to work on an issue, please claim it first by commenting on the GitHub issue that you want to work on it. This is to prevent duplicated efforts from contributors on the same issue.\nIf you have questions about one of the issues, with or without the tag, please comment on them and one of the maintainers will clarify it.\nIndividual Contributor License Agreement When you contribute (code, documentation, or anything else), be aware that we only accept contributions under the Gardener project\u0026rsquo;s license (see previous sections) and you need to agree to the Individual Contributor License Agreement. This applies to all contributors, including those contributing on behalf of a company. If you agree to its content, click on the link posted by the CLA assistant as a comment to the pull request. Click it to review the CLA, then accept it on the next screen if you agree to it. CLA assistant will save your decision for upcoming contributions and will notify you if there is any change to the CLA in the meantime.\nCorporate Contributor License Agreement If employees of a company contribute code, in addition to the individual agreement above, there needs to be one company agreement submitted. This is mainly for the protection of the contributing employees.\nAn authorized company representative needs to download, fill, and print the Corporate Contributor License Agreement form. Then either:\n Scan it and e-mail it to opensource@sap.com and gardener.opensource@sap.com Fax it to: +49 6227 78-45813 Send it by letter to: Industry Standards \u0026amp; Open Source Team, Dietmar-Hopp-Allee 16, 69190 Walldorf, Germany  The form contains a list of employees who are authorized to contribute on behalf of your company. When this list changes, please let us know.\nPull Request Checklist  Branch from the master branch and, if needed, rebase to the current master branch before submitting your pull request. If it doesn\u0026rsquo;t merge cleanly with master you may be asked to rebase your changes.\n Commits should be as small as possible, while ensuring that each commit is correct independently (i.e., each commit should compile and pass tests).\n If your patch is not getting reviewed or you need a specific person to review it, you can @-reply a reviewer asking for a review in the pull request or a comment, or you can ask for a review on our mailing list.\n Add tests relevant to the fixed bug or new feature.\n  Issues and Planning We use GitHub issues to track bugs and enhancement requests and ZenHub for planning. * Install the ZenHub Chrome plugin * Login to ZenHub * Open the Gardener ZenHub workspace\nSecurity Release Process See Security Release Process\n"
},
{
	"uri": "https://gardener.github.io/website/045_contribute/10_code/10-contribution_guide/",
	"title": "Contribution Guide",
	"tags": [],
	"description": "",
	"content": " Contributing to Gardener Code of conduct All members of the Gardener community must abide by the CNCF Code of Conduct. Only by respecting each other can we develop a productive, collaborative community. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting gardener.opensource@sap.com and/or a Gardener project maintainer.\nContributing Gardener uses GitHub to manage reviews of pull requests.\n If you are a new contributor see: Steps to Contribute\n If you have a trivial fix or improvement, go ahead and create a pull request, addressing (with @...) a suitable maintainer of this repository (see CODEOWNERS of the repository you want to contribute to) in the description of the pull request.\n If you plan to do something more involved, first discuss your ideas on our mailing list. This will avoid unnecessary work and surely give you and us a good deal of inspiration.\n Relevant coding style guidelines are the Go Code Review Comments and the Formatting and style section of Peter Bourgon\u0026rsquo;s Go: Best Practices for Production Environments.\n  Steps to Contribute Should you wish to work on an issue, please claim it first by commenting on the GitHub issue that you want to work on it. This is to prevent duplicated efforts from contributors on the same issue.\nIf you have questions about one of the issues, with or without the tag, please comment on them and one of the maintainers will clarify it.\nContributing Code You are welcome to contribute code to Gardener in order to fix a bug or to implement a new feature.\nThe following rules govern code contributions: * Contributions must be licensed under the Apache 2.0 License * You need to sign the Contributor License Agreement. We are using CLA assistant providing a click-through workflow for accepting the CLA. For company contributors additionally the company needs to sign a corporate license agreement. See the following sections for details.\nContributing Documentation You are welcome to contribute documentation to Gardener.\nThe following rules govern documentation contributions: * Contributions must be licensed under the Creative Commons Attribution 4.0 International License * You need to sign the Contributor License Agreement. We are using CLA assistant providing a click-through workflow for accepting the CLA. For company contributors additionally the company needs to sign a corporate license agreement. See the following sections for details.\nIndividual Contributor License Agreement When you contribute (code, documentation, or anything else), be aware that we only accept contributions under the Gardener project\u0026rsquo;s license (see previous sections) and you need to agree to the Individual Contributor License Agreement. This applies to all contributors, including those contributing on behalf of a company. If you agree to its content, click on the link posted by the CLA assistant as a comment to the pull request. Click it to review the CLA, then accept it on the next screen if you agree to it. CLA assistant will save your decision for upcoming contributions and will notify you if there is any change to the CLA in the meantime.\nCorporate Contributor License Agreement If employees of a company contribute code, in addition to the individual agreement above, there needs to be one company agreement submitted. This is mainly for the protection of the contributing employees.\nAn authorized company representative needs to download, fill, and print the Corporate Contributor License Agreement form. Then either:\n Scan it and e-mail it to opensource@sap.com and gardener.opensource@sap.com Fax it to: +49 6227 78-45813 Send it by letter to: Industry Standards \u0026amp; Open Source Team, Dietmar-Hopp-Allee 16, 69190 Walldorf, Germany  The form contains a list of employees who are authorized to contribute on behalf of your company. When this list changes, please let us know.\nPull Request Checklist  Branch from the master branch and, if needed, rebase to the current master branch before submitting your pull request. If it doesn\u0026rsquo;t merge cleanly with master you may be asked to rebase your changes.\n Commits should be as small as possible, while ensuring that each commit is correct independently (i.e., each commit should compile and pass tests).\n If your patch is not getting reviewed or you need a specific person to review it, you can @-reply a reviewer asking for a review in the pull request or a comment, or you can ask for a review on our mailing list.\n Add tests relevant to the fixed bug or new feature.\n  Issues and Planning We use GitHub issues to track bugs and enhancement requests and ZenHub for planning. * Install the ZenHub Chrome plugin * Login to ZenHub * Open the Gardener ZenHub workspace\nSecurity Release Process See Security Release Process\n"
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/howto/gardener_aws/",
	"title": "Create a kubernetes cluster in AWS with Gardener",
	"tags": [],
	"description": "How to create a Kubernetes Cluster with Gardener in AWS",
	"content": " Introduction Creating an Kubernetes cluster in the AWS Account is easy and the Gardener UI shall be self-expantory. For your convenience you will find screenshots on how to create a cluster here.\nGardener Create new Project in Gardener Create new Project\nCopy policy from the gardener AWS Create new policy Create new oplicy\nCreate a new technical user Create a new technical user\nsave the keys of the user, you will need it later on\nGardener Add AWS Secret Create a new Cluster Create a new cluster\nCopy kubeconfig "
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/howto/gardener_gcp/",
	"title": "Create a kubernetes cluster in GCP with Gardener",
	"tags": [],
	"description": "How to create a Kubernetes Cluster with Gardener in GCP",
	"content": " Introduction Creating an Kubernetes cluster in the GCP Account is easy and the Gardener UI shall be self-expantory. For your convenience you will find screenshots on how to create a cluster here.\nGardener Create new Project in Gardener Create new Project\nCheck which roles are required by the gardener GCP Create a new serviceaccount and assign roles Create a new serviceaccount\nCreate key for the serviceaccount Download the key of the serviceaccount as json save the keys of the user, you will need it later on\nEnable the Google compute API Enable the Google compute API Enable the Google IAM API Enable the Google IAM API Gardener Add GCP Secret Create a new Cluster Create a new cluster\nCopy kubeconfig "
},
{
	"uri": "https://gardener.github.io/website/development/usage/",
	"title": "Creating Shoots",
	"tags": [],
	"description": "",
	"content": " Creating / Deleting a Shoot cluster As you have already prepared an example Shoot manifest in the steps described in the development documentation, please open another Terminal pane/window with the KUBECONFIG environment variable pointing to the Garden development cluster and send the manifest to the Kubernetes API server:\n$ kubectl apply -f $GOPATH/src/github.com/gardener/gardener/dev/shoot-aws.yaml  You should see that the Gardener has immediately picked up your manifest and started to deploy the Shoot cluster.\nIn order to investigate what is happening in the Seed cluster, please download its proper Kubeconfig yourself (see next paragraph). The namespace of the Shoot cluster in the Seed cluster will look like that: shoot-johndoe-johndoe-1, whereas the first johndoe is your namespace in the Garden cluster (also called \u0026ldquo;project\u0026rdquo;) and the johndoe-1 suffix is the actual name of the Shoot cluster.\nTo connect to the newly created Shoot cluster, you must download its Kubeconfig as well. Please connect to the proper Seed cluster, navigate to the Shoot namespace, and download the Kubeconfig from the kubecfg secret in that namespace.\nIn order to delete your cluster, you have to set an annotation confirming the deletion first, and trigger the deletion after that. You can use the prepared delete-shoot script which takes the Shoot name as first parameter. The namespace can be specified by the second parameter, but it is optional. If you don\u0026rsquo;t state it, it defaults to your namespace (the username you are logged in with to your machine).\n$ ./hack/delete-shoot johndoe-1  Updating Shoot Cluster version and How Auto Update Feature is Handled If a shoot has .spec.maintenance.autoUpdate.kubernetesVersion: true in the manifest, and you update the .spec.\u0026lt;provider\u0026gt;.constraints.kubernetes.versions field in the CloudProfile used in the Shoot, then Gardener will apply Kubernetes patch releases updates automatically during the .spec.maintenance.timeWindow.\nSince Kubernetes follows Semantic Versioning, if indicated so, Gardener will automatically apply the patch release updates. But it will never auto update the Major or Minor releases since there is no effort to keep backward compatibility in those releases.\nMajor or Minor updates must be handled by updating the .spec.kubernetes.version field manually, theese updates will be executed immediately and will not wait for maintenance time window. Before applying such update on Minor or Major releases, operators should check for all the breaking chances introduced in the target release Changelog.\nE.g. If you have a shoot cluster with below field values (only related fields are shown):\nspec: kubernetes: version: 1.10.0 maintenance: timeWindow: begin: 220000+0000 end: 230000+0000 autoUpdate: kubernetesVersion: true  If you update the CloudProfile used in the Shoot and add 1.10.5 and 1.11.0 to the .spec.\u0026lt;provider\u0026gt;.constraints.kubernetes.versions list, the Shoot will be updated to 1.10.5 between 22:00-23:00 UTC. Your Shoot won\u0026rsquo;t be updated to 1.11.0 even though its the highest Kubernetes in the CloudProfile, this is because that woulnd\u0026rsquo;t be a patch release update but a minor release update, and potentially have breaking changes that could impact your deployed resources.\nIn this example if the operator wants to update the Kubernetes version to 1.11.0, he/she must update the Shoot\u0026rsquo;s .spec.kubernetes.version to 1.11.0 manually.\n"
},
{
	"uri": "https://gardener.github.io/website/curated-links/",
	"title": "Curated Links",
	"tags": [],
	"description": "",
	"content": "Curated Links A curated list of Kubernetes resources and projects    A curated list for awesome kubernetes sources Inspired by @sindresorhus\u0026rsquo; awesome\nSetup  Install Docker for Mac Install Docker for Windows Run a Kubernetes Cluster on your local machine  A place that marks the beginning of a journey  Kubernetes Community Overview and Contributions Guide by Ihor Dvoretskyi An Intro to Google’s Kubernetes and How to Use It by Laura Frank Getting Started on Kubernetes by Rajdeep Dua Kubernetes: The Future of Cloud Hosting by Meteorhacks Kubernetes by Google by Gaston Pantana Key Concepts by Arun Gupta Application Containers: Kubernetes and Docker from Scratch by Keith Tenzer Learn the Kubernetes Key Concepts in 10 Minutes by Omer Dawelbeit Top Reasons Businesses Should Move to Kubernetes Now by Mike Johnston The Children\u0026rsquo;s Illustrated Guide to Kubernetes by Deis :-) The ‘kubectl run’ command by Michael Hausenblas Docker Kubernetes Lab Handbook by Peng Xiao  Interactive Learning Environments Learn Kubernetes using an interactive environment without requiring downloads or configuration\n Interactive Tutorial Katacoda Play with Kubernetes Kubernetes Bootcamp  MOOC Courses / Tutorials List of available free online courses(MOOC) and tutorials\nCourses  Scalable Microservices with Kubernetes at Udacity Introduction to Kubernetes at edX  Tutorials  Kubernetes Tutorials by Kubernetes Team Kubernetes By Example by OpenShift Team Kubernetes Tutorial by Tutorialspoint   Package Managers  Helm KPM   RPC  gRPC Micro  Secret generation and management  Vault auth plugin backend: Kubernetes Vault controller kube-lego k8sec kubernetes-vault kubesec - Secure Secret management  Machine Learning  TensorFlow k8s mxnet-operator - Tools for ML/MXNet on Kubernetes. kubeflow - Machine Learning Toolkit for Kubernetes. seldon-core - Open source framework for deploying machine learning models on Kubernetes  Raspberry Pi Some of the awesome findings and experiments on using Kubernetes with Raspberry Pi. * Kubecloud * Setting up a Kubernetes on ARM cluster * Setup Kubernetes on a Raspberry Pi Cluster easily the official way! by Mathias Renner and Lucas Käldström * How to Build a Kubernetes Cluster with ARM Raspberry Pi then run .NET Core on OpenFaas by Scott Hanselman\nContributing Contributions are most welcome!\nThis list is just getting started, please contribute to make it super awesome.\n "
},
{
	"uri": "https://gardener.github.io/website/components/dns-cm/",
	"title": "DNS Controller Manager",
	"tags": [],
	"description": "",
	"content": " DNS Loadbalancer Controller Manager The DNS Load Balancer Controller Manager hosts kubernetes controllers managing DNS entries acting as kind of load balancer. Depending on health checks on explicitly maintained endpoints the endpoints are added or removed from an DNS entry.\nIt is primarily designed to support multi-cluster loadbalancing (see below)\nIt defines 3 new resource kinds using the api group loadbalancer.gardener.cloud and version v1beta1. - DNSProvider: A resource describing a dedicated DNS access - DNSLoadBalancer: a resource describing a dedicated load balancer defining the DNS name and the health check - DNSLoadBalancerEndpoint: a resource describing a dedicated load balancer target endpoint\nThe following DNS Provider types are supported so far: - AWS Route53\nControllers The controller manager hosts two different controllers:\nDNS Controller The DNS Controller uses the resources decribed above to main DNS entries. Is uses the DNSProvider resources to get access to various DNS accounts with different hosted zones. The matching provider for the DNS name of an DNSLoadBalancer is determined automatically. The DNSLoadBalancerEndpointresources are used as potential targets for the maintained DNS names.\nDNS Endpoint Controller The endpoint controller scans a cluster for annotated service and ingress resources looking for the annotation\n loadbalancer.gardener.cloud/dnsloadbalancer  expecting the name of the load balancer resource as value. For those matching resources it maintains endpoint resources (mentioned above).\nMulti Cluster Mode Basically both controllers can work on the same cluster. This would be a single cluster scenario. But for such a scenario the introduction of explicitly maintained loadbalancer and endpoint resources would be superfluous.\nThe intended scenario is a multi-cluster scenario, where the various endpoints reside in different clusters. Therefore the two controllers may use different target clusters for scanning.\nIf two kubeconfigs are configured for the controller manager, the endpoint controller scans the default (source) cluster for service and ingress resources and expects the load balancer and endpoint resources to be maintained in the second cluster.\nThe dns controller acts on the second cluster to look for DNS providers, loadbalancers and endpoints to maintain the desired DNS entries.\nThis second cluster should be shared among the various source clusters to maintain a central loadbalancing datasource.\nFor every source cluster the complete controller manager is deployed varying the first cluster access for the local cluster and using the second cluster access for the shared one.\nLeases The controllers request leases in the different clusters, therefore they can be run multiple times across the involved clusters.\nRun Modes The controller manager can be started for a single kind of controller or for both controllers at once. Nevertheless the DNS controller always requests its lease from the shared cluster. Therefore it effectivly runs only once in the complete landscape, even if started with each controller manager instance.\nIf the --watches option is used, the DNS controller doesn\u0026rsquo;t use the custom resources for the load balancer but reads the definitions from the given config file (legacy mode). In combination with using the static DNS providers this mode can be used to work standalone.\nIf the --providers option is used to select a static provider, every supported DNS provider type may provide a default provider according to the environment settings. If the value dynamic (default) is specified, only the DNSProvider resources found in the target cluster are used.\nCommand Line Interface This manager manages DNS LB endpoint resources for DNS Loadbalancer resources based on annotations in services and ingresses. Based on those endpoints a second controller manages DNS entries. The endpoint sources may reside in different kubernetes clusters than the one hosting the DNS loadbalancer and endpoint resources. Usage: dnslb-controller-manager [flags] Flags: --cluster string Cluster identity --controllers string Comma separated list of controllers to start (\u0026lt;name\u0026gt;,source,target,all) (default \u0026quot;all\u0026quot;) --dry-run Dry run for DNS controller --duration int Runtime before stop (in seconds) -h, --help help for dnslb-controller-manager --identity string DNS record identifer (default \u0026quot;GardenRing\u0026quot;) --interval int DNS check/update interval in seconds (default 30) --kubeconfig string path to the kubeconfig file -D, --log-level string log level (default \u0026quot;info\u0026quot;) --once only one update instread of loop --plugin-dir string directory containing go plugins for DNS provider types --port int http server endpoint port for health-check (default: 0=no server) --providers string Selection mode for DNS providers (static,dynamic,all,\u0026lt;type name\u0026gt;) (default \u0026quot;dynamic\u0026quot;) --targetkube string path to the kubeconfig file for shared virtual cluster --ttl int DNS record ttl in seconds (default 60) --watches string config file for watches  Custom Resource Definitions DNS Load Balancer apiVersion: loadbalancer.gardener.cloud/v1beta1 kind: DNSLoadBalancer metadata: name: test namespace: acme spec: DNSName: test.acme.com type: Balanced # or Exclusive healthPath: /healthz statusCode: 200 # default endpointValidityInterval: 5m # Optional status: active: - ipaddress: \u0026quot;172.18.117.33\u0026quot; name: \u0026quot;a-test-service\u0026quot; state: healthy message:  If the optional endpoint validity interval is specified, the endpoint controller generates endpoints with a limited lifetime, and updates it accordingly as long as it is running. The dns controller automatically discards outdated endpoint resources.\nDNS Load Balancer Endpoint apiVersion: loadbalancer.gardener.cloud/v1beta1 kind: DNSLoadBalancerEndpoint metadata: name: a-test-service namespace: acme spec: ipaddress: 172.18.117.33 # or cname loadbalancer: test status: active: true healthy: true validUntil: 2018-07-24T11:34:44Z  The validUtil status property is managed by the endpoint controller, if the loadbalancer resource requests it by specifying a validity interval for endpoints.\nDNS Provider apiVersion: loadbalancer.gardener.cloud/v1beta1 kind: DNSProvider metadata: name: aws namespace: acme spec: type: aws scope: type: Selected # or Cluster/Namespace namespaces: - acme secretRef: name: route53  A provider may only be used for a load balancer resource if it is in the scope of the provider. The following scopes are supported:\n Cluster: (default) valid for all namespaces in kubernetes cluster Namespace: only valid for the namespace of the provider resource Selected: valid for the explicitly managed namespace list in property namespaces  Supported DNS Provider Types For every provider type multiple provider (with different credentials) may be configured by deploying the appropriate DNSProvider resources.\nAdditional provider types can be added by go plugins (see below). Plugins are enabled by specifying the --plugin-dir option.\nAWS Route53 The AWS Route53 provider type is selected by using the type name aws.\nThe secret must have the fields:\n|Name|Meaning| |\u0026ndash;|\u0026ndash;| |AWS_ACCESS_KEY_ID|The aws access key id| |AWS_SECRET_ACCESS_KEY|The aws secret access key|\nHTTP Endpoints If the controller manager is called with the --port option using a value larger than zero an https server is started serving two endpoints:\nHealth Endpoint A health endpoint with path /healthz is provided at the given port. It reports status code 200 if everything looks fine. The timestamps of the internal check keys are reported as content.\nMetrics Endpoint A metrics endpoint (for prometheus) is provided with the path /metrics . It supports five metrics:\n   Metric Label Meaning     endpoint_health  Health status of an endpoint (0/1)    loadbalancer Load balancer name    endpoint Endpoint name   endpoint_active  Active status of an endpoint (assigned to DNS entry)    loadbalancer Load balancer name    endpoint Endpoint name   endpoint_hosts  Hostname for an endpoint resource with health status    endpoint Endpoint name    host Hostname   loadbalancer_health  Health status of a load balancer (0/1)    loadbalancer Load balancer name   loadbalancer_dnsnames  DNS names of a load balancer with health status    loadbalancer Load balancer name    dnsname DNS name of the load balancer   dns_reconcile_duration  Duration of a DNS reconcilation run   dns_reconcile_interval  Duration between two DNS reconcilations    Plugins Go plugins can be used to add new independently developed DNS provider types. The plugins must be placed in a dedicated folder, which is specified by the --plugin-dir option\nA DNS provider type must implement the DNSProviderType interface found in package github.com/gardener/dnslb-controller-manager/pkg/controller/dns/provider.\nThe main package must provide a variable called Name of type string containing the name of the plugin. To register a provider type it has to implement an init function registering the provided provider types. For example:\n func init() { provider.RegisterProviderType(\u0026quot;aws\u0026quot;, \u0026amp;AWSProviderType{}) }  The specified name can then be used in the DNSProvider kubernetes resources to add a dedicated set of hosted zones handled by this provider type.\n"
},
{
	"uri": "https://gardener.github.io/website/components/dashboard/",
	"title": "Dashboard",
	"tags": [],
	"description": "",
	"content": " Gardener Dashboard Development Setup Install Install client and server dependencies\nnpm install --prefix frontend npm install --prefix backend  Configuration KUBECONFIG If the dashboard is not running in the Garden Cluster you have to point the kubeconfig to Garden Cluster. This can be done in the default kubeconfig file in ${HOME}/.kube/config or by the KUBECONFIG environment variable.\nGARDENER_CONFIG The configuration file of the Gardener Dashboard can be specified as first command line argument or as environment variable GARDENER_CONFIG at the server process. If nothing is specified the default location is ${HOME}/.gardener/config.yaml.\nA local configuration example for minikube and dex could look like follows:\nport: 3030 logLevel: debug logFormat: text jwt: audience: gardener issuer: \u0026amp;issuer https://minikube:32001 algorithms: [ RS256 ] jwks: ca: | -----BEGIN CERTIFICATE----- MIIC5z... -----END CERTIFICATE----- strictSsl: false rejectUnauthorized: true cache: false rateLimit: false jwksRequestsPerMinute: 5 jwksUri: https://minikube:32001/keys frontend: dashboardUrl: pathname: /api/v1/namespaces/kube-system/services/kubernetes-dashboard/proxy/ kubernetesVersions: - 1.8.4 - 1.7.9 cloudProviders: aws: volumeTypes: - gp2 machineTypes: - m4.large - m4.xlarge openstack: volumeTypes: - default machineTypes: - medium_2_4 - medium_4_8 oidc: authority: *issuer client_id: gardener redirect_uri: http://localhost:8080/callback response_type: 'token id_token' scope: 'openid email profile groups audience:server:client_id:gardener audience:server:client_id:kube-kubectl' loadUserInfo: false  Run locally (during development) Run the backend server with hot reload under localhost:3030.\nnpm run dev --prefix backend  Run the frontend server with hot reload under localhost:8080.\nnpm run dev --prefix frontend  All request to /api and /config.json with be proxied by default to the backend server.\nBuild Build frontend artifacts for production with minification\nmake build  The build results will be written to frontend/dist. The static resource path public of the backend server is symlinked to this directory.\nRelease Publish a new container image and publish to Google Container Registry.\nnpm run build --prefix frontend  This expects valid GCR credentials located at ${HOME}/.config/gcloud/gcr-readwrite.json. It will build a new image and pushes it to the container registry.\nPeople The following SAP developers contributed to this project until this initial contribution was published as open source.\n   contributor commits (%) +lines -lines first commit last commit     Holger Koser 313 (42%) 57878 18562 2017-07-13 2018-01-23   Andreas Herz 307 (41%) 13666 11099 2017-07-14 2017-10-27   Peter Sutter 99 (13%) 4838 3967 2017-11-07 2018-01-23   Gross, Lukas 31 (4%) 400 267 2018-01-10 2018-01-23    It is derived from the historical, internal gardener-ui repository at commit eeb623d60c86e6037c0e1dc2bdd9e54663bf41a8.\nLicense Apache License 2.0\nCopyright 2018 The Gardener Authors\n"
},
{
	"uri": "https://gardener.github.io/website/045_contribute/10_code/20_dependencies/",
	"title": "Dependencies",
	"tags": [],
	"description": "",
	"content": " Testing We follow the BDD-style testing principles and are leveraging the Ginkgo framework along with Gomega as matcher library. In order to execute the existing tests, you can use\n$ make test  There is an additional command for analyzing the code coverage of the tests. Ginkgo will generate standard Golang cover profiles which will be translated into a HTML file by the Go Cover Tool. Another command helps you to clean up the filesystem from the temporary cover profile files and the HTML report:\n$ make test-cov $ open gardener.coverage.html $ make test-clean  Dependency management We are using Dep as depedency management tool. In order to add a new package dependency to the project, you can perform dep ensure -add \u0026lt;PACKAGE\u0026gt; or edit the Gopkg.toml file and append the package along with the version you want to use as a new [[constraint]].\nUpdating dependencies The Makefile contains a rule called revendor which performs a dep ensure -update and a dep prune command. This updates all the dependencies to its latest versions (respecting the constraints specified in the Gopkg.toml file). The command also installs the packages which do not yet exist in the vendor folder but are specified in the Gopkg.toml (in case you have added new ones).\n$ make revendor   The depencendies are installed into the vendor folder which should be added to the VCS.  :warning: Make sure that you test the code after you have updated the dependencies!\n"
},
{
	"uri": "https://gardener.github.io/website/045_contribute/10_code/30_deploy_seed_into_aks/",
	"title": "Deploy into AKS",
	"tags": [],
	"description": "",
	"content": " Deploying the Gardener and a Seed into an AKS cluster This document demonstrates how to install Gardener into an existing AKS cluster. We\u0026rsquo;ll use a single cluster for the sake of simplicity to host both Gardener and a seed to the same cluster.\nPlease note that this document is to provide you an example installation and is not to be used in a production environment since there are some certificates hardcoded, non-HA and non-TLS-enabled etcd setup.\nHigh Level Overview In this example we\u0026rsquo;ll follow these steps to create a Seed cluster on AKS: 1. Prerequisites * AWS credentials for Route 53 Hosted Zone * Deploy AKS cluster * Install default ClusterRoles * Install Tiller to AKS * Deploy stable/nginx-ingress chart to AKS * Create wildcard DNS record for Seed ingress * Create Azure Service Principle to get Azure credentials * Install gardenctl 2. Install Gardener * Create garden namespace * Deploy etcd * Deploy Gardener Helm Chart 3. Create a CloudProfile 4. Define Seed cluster in Gardener * Create the Seed resource definition with its Secret 5. Create a Shoot cluster * Create a Project (namespace) for Shoots * Create a SecretBinding and related Secret * Create the Shoot resource * Cluster Resources After Shoot is Created * Troubleshooting Shoot Creation Issues 6. Access Shoot cluster 7. Delete Shoot cluster\nPrerequisites Summary of prerequisites: - An Azure AKS cluster with: - default ClusterRoles defined (not defined by default since RBAC is not enabled), - Tiller deployed with cluster-admin ClusterRole, - stable/nginx-ingress Helm chart deployed, - a wildcard DNS record pointing the ingress, - az command line client configured for related subscription, - An Azure service principle to provide Azure credentials to Gardener, - A Route53 Hosted Zone and AWS account credentials with permissions on that Route53 Zone, - aws command line client configured for this account, - gardenctl command line client configured for the AKS cluster\u0026rsquo;s kubeconfig\nNote: We use a Route53 Hosted Zone even if we are deploying on Azure, this is due to lack of Azure DNS implementation in Gardener, it only supports aws-route53 or google-clouddns for now.\nAWS credentials for Route 53 Hosted Zone You need to provide credentials for AWS with permission to access Route53 Hosted Zone. In this example we\u0026rsquo;ll assume your domain for the Hosted Zone is .your.domain.here.\nHOSTED_ZONE_ID= # place your AWS Route53 hostedZoneID here  Create an AWS user, define policy to allow permission for the Hosted Zone and note the hostedZoneID, accessKeyID and secretAccessKey for later use.\nDeploy AKS cluster Here you can find a summary for creating an AKS cluster, if you already have one, skip this. You can skip the Azure HTTP addons when installing the AKS cluster, we\u0026rsquo;ll install stable/nginx-ingress Helm chart as you can see below.\naz group create --name garden-1 --location eastus az aks create --resource-group garden-1 --name garden-1 --node-count 3 --generate-ssh-keys -s Standard_DS4_v2 az aks get-credentials --resource-group garden-1 --name garden-1  Install default ClusterRoles On Azure, RBAC support is planned for GA and not enabled yet, so ClusterRole definitions are not in place and this causes Gardener Helm chart to fail. Below instead of removing RBAC support from Gardener installation we are faking the RBAC support by manually creating ClusterRoles on our AKS cluster.\nNote: We are using v1.9.6 in our AKS cluster, mind the version in the url below and please use the rules for the right version.\n# manually create ClusterRoles in AKS, required until RBAC support is added to AKS kubectl apply -f https://raw.githubusercontent.com/kubernetes/kubernetes/v1.9.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/cluster-roles.yaml --validate=false  Install Tiller to AKS Helm must run with a ClusterRole (cluster-admin) with enough permissions even if RBAC is not enabled, this is an enforcement needs to be addressed on Helm side not on AKS/Kubernetes. So let\u0026rsquo;s create a Tiller ServiceAccount with cluster-admin ClusterRole and deploy it: (example/aks/tiller-rbac-config.yaml).\n# Reference: https://github.com/kubernetes/helm/blob/master/docs/rbac.md kubectl apply -f example/aks/tiller-rbac-config.yaml helm init --service-account tiller  Deploy stable/nginx-ingress chart to AKS At the moment all Ingress resources created by the Gardener are using the nginx annotations. This is the simplest solution at the moment, because it works across cloud providers. But in the future it\u0026rsquo;s planned to make it more customizable.\nAKS Ingress documentation tells about installing Ingress controller with RBAC disabled, you can also follow that one but since we already deployed the default ClusterRoles above, we are now able to install the chart with RBAC enabled.\nhelm upgrade --install --namespace kube-system nginx-ingress stable/nginx-ingress  Create wildcard DNS record for the ingress You need to pick a wildcard subdomain matching your Route53 Hosted Zone here. This ingress wildcard record is supposed to be part of the Seed cluster rather than Gardener cluster, in our example we\u0026rsquo;ll use *.seed-1.your.domain.here.\nAssuming you have the AWS cli for your Route53 Hosted Zone is configured on your local, here we\u0026rsquo;ll create the wildcard DNS record using the awless. You can also use the AWS console or any other tool of your choice to create the wildcard record:\nHOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026quot;HOSTED_ZONE_ID is missing\u0026quot;} --query 'HostedZone.Name' --output text) INGRESS_DOMAIN=\u0026quot;seed-1.${HOSTED_ZONE_DOMAIN%%.}\u0026quot; # Get LB IP address from `kubectl -n kube-system get svc shared-ingress-nginx-ingress-controller` LB_IP=$(kubectl -n kube-system get svc nginx-ingress-controller --template '{{(index .status.loadBalancer.ingress 0).ip}}') awless create record \\ zone=$HOSTED_ZONE_ID \\ name=\u0026quot;*.$INGRESS_DOMAIN\u0026quot; \\ value=$LB_IP \\ type=A \\ ttl=300  Create Azure Service Principle to get Azure credentials We need client_id and client_secret to allow Gardener to reach Azure services, we can generate a pair by creating a Service Principle on Azure:\n$ az ad sp create-for-rbac --role=\u0026quot;Contributor\u0026quot; Retrying role assignment creation: 1/36 { \u0026quot;appId\u0026quot;: \u0026quot;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026quot;, #az_client_id \u0026quot;displayName\u0026quot;: \u0026quot;azure-cli-2018-05-23-16-15-49\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;http://azure-cli-2018-05-23-16-15-49\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026quot;, #az_client_secret \u0026quot;tenant\u0026quot;: \u0026quot;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026quot; #az_tenant_id }  Let\u0026rsquo;s define some env varibles for later use\nCLIENT_ID= # place your Azure Service Principal appId CLIENT_SECRET= # place your Azure Service Principal password here  Install gardenctl In this example we\u0026rsquo;ll be using gardenctl to interact with Gardener. You can install gardenctl following instruction in its repo: https://github.com/gardener/gardenctl\nHere is a simple configuration for gardenctl:\n$ cat ~/.garden/config gardenClusters: - name: dev kubeConfig: ~/.kube/config  Install Gardener Create garden namespace This is where we deploy Gardener components.\nkubectl apply -f example/00-namespace-garden.yaml  Deploy etcd Since Gardener is an extension API Server, it can share the etcd backing native Kubernetes cluster\u0026rsquo;s API Server, and hence explicit etcd installation is optional. But in our case we have no access to the control plane components of the AKS cluster and we have to deploy our own etcd ourselves for Gardener.\nThe example/aks/etcd.yaml file used here is a modified version of the etcd manifest used in hack/dev-setup script to stay close to dev setup. This one deploys an etcd with its data in a PVC rather than a hostPath to provide persistency even when its POD jumps between different hosts.\nNote: This etcd installation doesn\u0026rsquo;t provide HA. But etcd will be auto recovered by the Deployment. This could be sufficient for some deployments but may not be suitable for production usage. Also note that this etcd is not deployed with TLS enabled and doesn\u0026rsquo;t use certificates for authentication.\nkubectl apply -f example/aks/etcd.yaml  Check etcd logs and its started suceesfully:\nkubectl -n garden get deploy,svc etcd kubectl -n garden logs deployment/etcd  Deploy Gardener Helm Chart You need to update these Route53 related fields in the example/aks/gardener-values.yaml since garden-controller-manager is the components that creates DNS records in Route53: * controller.internalDomain.hostedZoneID * controller.internalDomain.domain: Here pick a subdomain for your Gardener to maintain DNS records for your Shoot clusters. This domain has to be within your Route53 Hosted Zone. e.g. garden-1.your.domain.here * controller.internalDomain.credentials * controller.internalDomain.secretAccessKey\nHOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026quot;HOSTED_ZONE_ID is missing\u0026quot;} --query 'HostedZone.Name' --output text) GARDENER_DOMAIN=\u0026quot;garden-1.${HOSTED_ZONE_DOMAIN%%.}\u0026quot; ACCESS_KEY_ID=$(aws configure get aws_access_key_id) SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key) sed -i \\ -e \u0026quot;s/hostedZoneID: Z3K\\*\\*\\*/hostedZoneID: $HOSTED_ZONE_ID/\u0026quot; \\ -e \u0026quot;s/domain: subdomain.matching.domain.in.hoztedzone.id/domain: $GARDENER_DOMAIN/\u0026quot; \\ -e \u0026quot;s@accessKeyID: access_key_id@accessKeyID: $ACCESS_KEY_ID@\u0026quot; \\ -e \u0026quot;s@secretAccessKey: secret_access_key@secretAccessKey: $SECRET_ACCESS_KEY@\u0026quot; \\ example/aks/gardener-values.yaml  After updating these fields in the file, run:\nhelm upgrade --install --namespace garden garden charts/gardener -f example/aks/gardener-values.yaml  Validate the Gardener is deployed:\nhelm status garden # Wait for `STATUS: DEPLOYED` kubectl -n garden get deploy,pod -l app=gardener # Better if you leave two terminals open in for below commands, and # keep an eye on whats going on behind the scenes as you create/delete # Gardener specific resources (Seed, CloudProfile, SecretBinding, Shoot). kubectl -n garden logs -f deployment/gardener-apiserver # confirm no issues kubectl -n garden logs -f deployment/gardener-controller-manager # confirm no issues, except some \u0026quot;Failed to list *v1beta1...\u0026quot; messages  Note: This is not meant to be used in production. You may not want to use apiserver.insecureSkipTLSVerify=true, the hardcoded apiserver certificates, and insecure (non-tls enabled) etcd. But for the sake of keeping this example simple you can just keep those values as they are.\nCreate a CloudProfile We need to create a CloudProfile to be referred from the Shoot (example/30-cloudprofile-azure.yaml):\nkubectl apply -f example/30-cloudprofile-azure.yaml  Validate that CloudProfile is created:\nkubectl describe -f example/30-cloudprofile-azure.yaml  Define Seed cluster in Gardener In our setup we\u0026rsquo;ll use the cluster for Gardener also as a Seed, this saves us from creating a new Kubernetes cluster. But you can also create an explicit cluster for the Seed. Seed cluster can also be placed into any other cloud provider or on prem. But keep in mind that below steps may differ if you use a different cluster for seed.\nCurrently, a Seed cluster is just a Kubeconfig for the Gardener. The seed cluster could have been created by any tool, Gardener only cares about having a valid Kubeconfig to talk to its API.\nCreate the Seed resource definition with its Secret Lets start with the required seed secret first. Here we need to provide it\u0026rsquo;s cloud provider credentials and kubeconfig in the seed secret. Update example/40-secret-seed-azure.yaml and place the secrets for your environment: * data.subscriptionID: you can learn this one with az account show * data.tenantID: from az ad sp create-for-rbac output as you can see above * data.clientID: from az ad sp create-for-rbac output as you can see above * data.clientSecret: from az ad sp create-for-rbac output as you can see above * data.kubeconfig: you can get this one with az aks get-credentials --resource-group garden-1 --name garden-1 -f - | base64)\nNote: All of the above values must be base64 encoded. If you skip this it will hurt you later.\nSUBSCRIPTION_ID=$(az account list -o json | jq -r '.[] | select(.isDefault == true) | .id') TENANT_ID=$(az account show -o tsv --query 'tenantId') KUBECONFIG_FOR_SEED_CLUSTER=$(az aks get-credentials --resource-group garden-1 --name garden-1 -f -) sed -i \\ -e \u0026quot;s@base64(subscription-id)@$(echo $SUBSCRIPTION_ID | tr -d '\\n' | base64)@\u0026quot; \\ -e \u0026quot;s@base64(tenant-id)@$(echo \u0026quot;$TENANT_ID\u0026quot; | tr -d '\\n' | base64)@\u0026quot; \\ -e \u0026quot;s@base64(client-id)@$(echo \u0026quot;${CLIENT_ID:?\u0026quot;CLIENT_ID is missing\u0026quot;}\u0026quot; | tr -d '\\n' | base64)@\u0026quot; \\ -e \u0026quot;s@base64(client-secret)@$(echo \u0026quot;${CLIENT_SECRET:?\u0026quot;CLIENT_SECRET is missing\u0026quot;}\u0026quot; | tr -d '\\n' | base64)@\u0026quot; \\ -e \u0026quot;s@base64(kubeconfig-for-seed-cluster)@$(echo \u0026quot;$KUBECONFIG_FOR_SEED_CLUSTER\u0026quot; | base64)@\u0026quot; \\ example/40-secret-seed-azure.yaml  After updating the fields, create the Seed secret:\nkubectl apply -f example/40-secret-seed-azure.yaml  Before creating Seed, we need to update the example/50-seed-azure.yaml file and update: * spec.networks: IP ranges used in your AKS cluster. * spec.ingressDomain: Place here the wildcard domain you have for the ingress controller (we created this record in prerequisites). Gardener doesn\u0026rsquo;t create this DNS records but assumes its created ahead of time, Seed clusters are not provisioned by Gardener. * spec.cloud.region: eastus (the region of the existing AKS cluster)\nHOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026quot;HOSTED_ZONE_ID is missing\u0026quot;} --query 'HostedZone.Name' --output text) INGRESS_DOMAIN=\u0026quot;seed-1.${HOSTED_ZONE_DOMAIN%%.}\u0026quot; # discover AKS CIDRs NODE_CIDR=$(az network vnet list -g MC_garden-1_garden-1_eastus -o json | jq -r '.[] | .subnets[] | .addressPrefix') POD_CIDR=$(kubectl -n kube-system get daemonset/kube-proxy -o yaml | grep cluster-cidr= | grep -v annotations | cut -d = -f2) SERVICE_CIDR=10.0.0.0/16 # This one is hardcoded for now, not easy to discover sed -i \\ -e \u0026quot;s/ingressDomain: dev.azure.seed.example.com/ingressDomain: $INGRESS_DOMAIN/\u0026quot; \\ -e \u0026quot;s/region: westeurope/region: eastus/\u0026quot; \\ -e \u0026quot;s@nodes: 10.240.0.0/16@nodes: $NODE_CIDR@\u0026quot; \\ -e \u0026quot;s@pods: 10.241.128.0/17@pods: $POD_CIDR@\u0026quot; \\ -e \u0026quot;s@services: 10.241.0.0/17@services: $SERVICE_CIDR@\u0026quot; \\ example/50-seed-azure.yaml  Now we are ready to create the seed:\nkubectl apply -f example/50-seed-azure.yaml  Check the logs in gardener-controller-manager and also wait for seed to be Ready: True. This means gardener-controller-manager is able to reach the Seed cluster with the credentials you provide.\n$ kubectl get seed azure NAME DOMAIN CLOUDPROFILE REGION READY azure seed-1.your.domain.here azure azure True $ gardenctl target garden dev KUBECONFIG=/Users/user/.kube/config $ gardenctl ls seeds seeds: - seed: azure  If something goes wrong verify that you provided right credentials, and base64 encoded strings of those in the secret. Also check the status field in the Seed resource and gardener-controller-manager logs:\n$ kubectl get seed azure -o json | jq .status { \u0026quot;conditions\u0026quot;: [ { \u0026quot;lastTransitionTime\u0026quot;: \u0026quot;2018-05-31T14:56:49Z\u0026quot;, \u0026quot;message\u0026quot;: \u0026quot;all checks passed\u0026quot;, \u0026quot;reason\u0026quot;: \u0026quot;Passed\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;True\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;Available\u0026quot; } ] }  Create a Shoot cluster Create a Project (namespace) for Shoots In this step we create a namespace in Gardener cluster to keep Shoot resource definitions. A project in Gardener terminology is simply a namespace that holds group of Shoots, during this example we\u0026rsquo;ll deploy a single Shoot. (Mind the extra labels defined in example/00-namespace-garden-dev.yaml).\nkubectl apply -f example/00-namespace-garden-dev.yaml  You can check the projects via gardenctl:\n$ gardenctl target garden dev $ gardenctl ls projects projects: - project: garden-dev  Create a SecretBinding and related Secret We\u0026rsquo;ll use same Azure credentials with example/40-secret-seed-azure.yaml, this is due to the fact that we use the same Azure Subscription for the Shoot and Seed clusters. Differently from the Seed secret, in this one we don\u0026rsquo;t need to provide kubeconfig since the Shoot cluster will be provisioned by Gardener, and we need to provide credentials for Route53 DNS records management.\nUpdate example/70-secret-cloudprovider-azure.yaml and place the secrets for your environment: * data.subscriptionID: you can learn this one with az account show * data.tenantID: from az ad sp create-for-rbac output as you can see above * data.clientID: from az ad sp create-for-rbac output as you can see above * data.clientSecret: from az ad sp create-for-rbac output as you can see above * data.accessKeyID: You need to add this field for Route53 records to be updated. * data.secretAccessKey: You need to add this field for Route53 records to be updated.\nNote: All of the above values must be base64 encoded. If you skip this it will hurt you later.\nSUBSCRIPTION_ID=$(az account list -o json | jq -r '.[] | select(.isDefault == true) | .id') TENANT_ID=$(az account show -o tsv --query 'tenantId') ACCESS_KEY_ID=$(aws configure get aws_access_key_id) SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key) sed -i \\ -e \u0026quot;s@base64(subscription-id)@$(echo $SUBSCRIPTION_ID | tr -d '\\n' | base64)@\u0026quot; \\ -e \u0026quot;s@base64(tenant-id)@$(echo \u0026quot;$TENANT_ID\u0026quot; | tr -d '\\n' | base64)@\u0026quot; \\ -e \u0026quot;s@base64(client-id)@$(echo \u0026quot;${CLIENT_ID:?\u0026quot;CLIENT_ID is missing\u0026quot;}\u0026quot; | tr -d '\\n' | base64)@\u0026quot; \\ -e \u0026quot;s@base64(client-secret)@$(echo \u0026quot;${CLIENT_SECRET:?\u0026quot;CLIENT_SECRET is missing\u0026quot;}\u0026quot; | tr -d '\\n' | base64)@\u0026quot; \\ -e \u0026quot;\\$a\\ \\ accessKeyID: $(echo $ACCESS_KEY_ID | tr -d '\\n' | base64 )\u0026quot; \\ -e \u0026quot;\\$a\\ \\ secretAccessKey: $(echo $SECRET_ACCESS_KEY | tr -d '\\n' | base64 )\u0026quot; \\ example/70-secret-cloudprovider-azure.yaml  After updating the fields, create the cloud provider secret:\nkubectl apply -f example/70-secret-cloudprovider-azure.yaml  And create the SecretBinding resource to allow Gardener use that secret (example/80-secretbinding-cloudprovider-azure.yaml):\nsed -i \\ -e 's/# namespace: .*/ namespace: garden-dev/' \\ example/80-secretbinding-cloudprovider-azure.yaml kubectl apply -f example/80-secretbinding-cloudprovider-azure.yaml  Check the logs in gardener-controller-manager, there should not be any problems reported.\nCreate the Shoot resource Update the fields in example/90-shoot-azure.yaml: * spec.cloud.region: eastus (this must match the seed cluster\u0026rsquo;s region) * spec.dns.domain: This is used to specify the base domain for your api (and other in the future) endpoint(s). For example when johndoe-azure.garden-dev.your.domain.here is used as a value, then your apiserver is available at api.johndoe-azure.garden-dev.your.domain.here * spec.dns.hostedZoneID: This field doesn\u0026rsquo;t exist in the example you need to add this field and place the Route53 Hosted Zone ID. * spec.addons.kube-lego.email: This is the email address used when using kube-lego. See kube-lego Environment Variables\nHOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026quot;HOSTED_ZONE_ID is missing\u0026quot;} --query 'HostedZone.Name' --output text) SHOOT_DOMAIN=\u0026quot;johndoe-azure.garden-dev.${HOSTED_ZONE_DOMAIN%%.}\u0026quot; KUBE_LEGO_EMAIL=$(git config user.email) sed -i \\ -e \u0026quot;s/region: westeurope/region: eastus/\u0026quot; \\ -e \u0026quot;s/domain: johndoe-azure.garden-dev.example.com/domain: $SHOOT_DOMAIN/\u0026quot; \\ -e \u0026quot;/domain:/a\\ \\ \\ \\ hostedZoneID: $HOSTED_ZONE_ID\u0026quot; \\ -e \u0026quot;s/email: john.doe@example.com/email: $KUBE_LEGO_EMAIL/\u0026quot; \\ example/90-shoot-azure.yaml  And let\u0026rsquo;s create the Shoot resource:\nkubectl apply -f example/90-shoot-azure.yaml  After creating the Shoot resource, gardener-controller-manager will pick it up and start provisioning the Shoot cluster.\nFollow the logs in your console with gardener-controller-manager, starting like below you\u0026rsquo;ll see plenty of Waiting and Executing, etc. logs and many tasks will keep repeating:\ntime=\u0026quot;2018-06-09T07:35:45Z\u0026quot; level=info msg=\u0026quot;[SHOOT RECONCILE] garden-dev/johndoe-azure\u0026quot; time=\u0026quot;2018-06-09T07:35:46Z\u0026quot; level=info msg=\u0026quot;Starting flow Shoot cluster creation\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:35:46Z\u0026quot; level=info msg=\u0026quot;Executing (*Botanist).DeployExternalDomainDNSRecord\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:35:46Z\u0026quot; level=info msg=\u0026quot;Executing (*Botanist).DeployNamespace\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:35:46Z\u0026quot; level=info msg=\u0026quot;Executing (*Botanist).DeployKubeAPIServerService\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:35:46Z\u0026quot; level=info msg=\u0026quot;Executing (*Botanist).DeployBackupNamespaceFromShoot\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:35:46Z\u0026quot; level=info msg=\u0026quot;Waiting for Terraform validation Pod 'johndoe-azure.external-dns.tf-pod-d8f66' to be completed...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:35:51Z\u0026quot; level=info msg=\u0026quot;Waiting for Terraform validation Pod 'johndoe-azure.external-dns.tf-pod-d8f66' to be completed...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:35:51Z\u0026quot; level=info msg=\u0026quot;Executing (*Botanist).MoveBackupTerraformResources\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:35:52Z\u0026quot; level=info msg=\u0026quot;Executing (*Botanist).WaitUntilKubeAPIServerServiceIsReady\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:35:52Z\u0026quot; level=info msg=\u0026quot;Waiting until the kube-apiserver service is ready...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:35:52Z\u0026quot; level=info msg=\u0026quot;Executing (*Botanist).DeployBackupInfrastructure\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:35:52Z\u0026quot; level=info msg=\u0026quot;Executing (*Botanist).WaitUntilBackupInfrastructureReconciled\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:35:52Z\u0026quot; level=info msg=\u0026quot;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:35:56Z\u0026quot; level=info msg=\u0026quot;Waiting for Terraform validation Pod 'johndoe-azure.external-dns.tf-pod-d8f66' to be completed...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:35:57Z\u0026quot; level=info msg=\u0026quot;Waiting until the kube-apiserver service is ready...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:35:57Z\u0026quot; level=info msg=\u0026quot;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:36:01Z\u0026quot; level=info msg=\u0026quot;Waiting for Terraform validation Pod 'johndoe-azure.external-dns.tf-pod-d8f66' to be completed...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:36:02Z\u0026quot; level=info msg=\u0026quot;Waiting until the kube-apiserver service is ready...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:36:02Z\u0026quot; level=info msg=\u0026quot;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure ...  At this stage you should be waiting for a while until the Shoot cluster is provisioned and initial resources are deployed.\nDuring the provisioning you can also check output of these commands to have a better understanding about what\u0026rsquo;s going on in the seed cluster:\n$ gardenctl ls shoots projects: - project: garden-dev shoots: - johndoe-azure $ gardenctl ls issues issues: - project: garden-dev seed: azure shoot: johndoe-azure health: Unknown status: lastOperation: description: Executing DeployKubeAddonManager, ReconcileMachines. lastUpdateTime: 2018-06-09 08:40:20 +0100 IST progress: 74 state: Processing type: Create $ kubectl -n garden-dev get shoot johndoe-azure NAMESPACE NAME SEED DOMAIN VERSION CONTROL NODES SYSTEM LATEST garden-dev johndoe-azure azure johndoe-azure.garden-dev.your.domain.here 1.10.1 True True True Succeeded $ kubectl -n garden-dev describe shoot johndoe-azure ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Reconciling 1h gardener-controller-manager [BrXWiztO] Reconciling Shoot cluster state Normal Reconciling 59m gardener-controller-manager [rBFsfwU5] Reconciling Shoot cluster state Normal Reconciling 59m gardener-controller-manager [2HAbm45D] Reconciling Shoot cluster state Warning ReconcileError 48m gardener-controller-manager [2HAbm45D] Failed to reconcile Shoot cluster state: Errors occurred during flow execution: '(*Botanist).EnsureIngressDNSRecord' returned '`.status.loadBalancer.ingress[]` has no elements yet, i.e. external load balancer has not been created (is your quota limit exceeded/reached?)' Normal Reconciling 48m gardener-controller-manager [S1QA0ksz] Reconciling Shoot cluster state Normal Reconciling 47m gardener-controller-manager [lvcSKy1Q] Reconciling Shoot cluster state Normal Reconciling 47m gardener-controller-manager [MddMyk8W] Reconciling Shoot cluster state Normal Reconciling 47m gardener-controller-manager [XDAAWABd] Reconciling Shoot cluster state Normal Reconciling 46m gardener-controller-manager [6HYH9Psz] Reconciling Shoot cluster state Normal Reconciling 46m gardener-controller-manager [rhL38ym4] Reconciling Shoot cluster state Warning ReconcileError 35m gardener-controller-manager [rhL38ym4] Failed to reconcile Shoot cluster state: Errors occurred during flow execution: '(*Botanist).EnsureIngressDNSRecord' returned '`.status.loadBalancer.ingress[]` has no elements yet, i.e. external load balancer has not been created (is your quota limit exceeded/reached?)' Normal Reconciling 35m gardener-controller-manager [BOt4Nvso] Reconciling Shoot cluster state Normal Reconciling 35m gardener-controller-manager [JPtmXmxD] Reconciling Shoot cluster state Normal Reconciling 34m gardener-controller-manager [ldHsVA6G] Reconciling Shoot cluster state Normal Reconciled 31m gardener-controller-manager [ldHsVA6G] Reconciled Shoot cluster state Normal Reconciling 26m gardener-controller-manager [yBh2IBOF] Reconciling Shoot cluster state Normal Reconciled 24m gardener-controller-manager [yBh2IBOF] Reconciled Shoot cluster state Normal Reconciling 16m gardener-controller-manager [bqmFtHUA] Reconciling Shoot cluster state Normal Reconciled 14m gardener-controller-manager [bqmFtHUA] Reconciled Shoot cluster state Normal Reconciling 6m gardener-controller-manager [7QgHE5CH] Reconciling Shoot cluster state Normal Reconciled 3m gardener-controller-manager [7QgHE5CH] Reconciled Shoot cluster state  Check Shoot cluster:\n$ gardenctl target garden dev KUBECONFIG=/Users/user/.kube/config $ gardenctl target project garden-dev $ gardenctl target shoot johndoe-azure KUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml $ gardenctl kubectl cluster-info Kubernetes master is running at https://api.johndoe-azure.garden-dev.your.domain.here KubeDNS is running at https://api.johndoe-azure.garden-dev.your.domain.here/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy kubernetes-dashboard is running at https://api.johndoe-azure.garden-dev.your.domain.here/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.  Cluster Resources After Shoot is Created After the Shoot has been created the summary of the resources in the AKS cluster handled by Gardener will be something like this:\nnon-namespaced resources CloudProfile: azure Seed: azure # cloud.profile:azure, cloud.region:eastus, secretRef.name:seed-azure, secretRef.namespace: garden Namespace: garden Secret: seed-azure # aks credentials, kubeconfig # No other resources with any kind handled by Gardener # Gardener components as well lives in this namespace Namespace: garden-dev # maps to \u0026quot;project:dev\u0026quot; in Gardener Secret: core-azure # credentials for aks + aws (for route53) SecretBinding: core-azure # secretRef.name:core-azure Shoot: johndoe-azure # seed:azure, secretBindingRef.name:core-azure Namespace: shoot--dev--johndoe-azure # These are automatically created once Shoot resource is created AzureMachineClass: shoot--dev--johndoe-azure-cpu-worker-8506a MachineDeployment: shoot--dev--johndoe-azure-cpu-worker MachineSet: shoot--dev--johndoe-azure-cpu-worker-849bbbf75 Machine: shoot--dev--johndoe-azure-cpu-worker-849bbbf75-b42vh BackupInfra: shoot--dev--johndoe-azure--c1b3b # seed:azure, shootUID: shoot.status.UID. # Many other resources created as part of shoot cluster, # but only above ones are handled by Gardener Namespace: backup--shoot--dev--johndoe-azure--c1b3b # Secrets and configMap having info related to backup infrastructure # are created by Gardener.  Troubleshooting Shoot Creation Issues For any issue happening during Shoot provisioning, you can consult the gardener-controller-manager logs, or the state in the shoot resource, gardenctl also provides a command to check Shoot cluster states:\n# check gardener-controller-manager logs kubectl -n garden logs -f deployment/gardener-controller-manager # kubectl describe can provide you a human readable output of # same information in below gardenctl command. kubectl -n garden-dev describe shoot johndoe-azure # also try cheking the machine-controller-manager logs of the shoot kubectl logs -n shoot--dev--johndoe-azure deployment/machine-controller-manager  With gardenctl:\n$ gardenctl ls issues issues: - project: garden-dev seed: azure shoot: health: Ready status: johndoe-azure lastError: \u0026quot;Failed to reconcile Shoot cluster state: Errors occurred during flow execution: '(*Botanist).DeployExternalDomainDNSRecord' returned 'Terraform execution ... lastOperation: description: \u0026quot;Failed to reconcile Shoot cluster state: Errors occurred during flow execution: '(*Botanist).DeployExternalDomainDNSRecord' returned 'Terraform ... lastUpdateTime: 2018-06-03 09:48:00 +0100 IST progress: 100 state: Failed type: Reconcile  Access Shoot cluster The gardenctl tool provides a convenient wrapper to operate on both cluster and cloud providers, here are some commands you can run\n# select target shoot cluster gardenctl ls gardens gardenctl target garden dev gardenctl ls projects gardenctl target shoot johndoe-azure # issue Azure client (az) commands on target shoot gardenctl az aks list # issue kubectl commands on target shoot gardenctl kubectl -- version --short # '--' is required if you want to # pass any args starting with '-' # open prometheus, alertmanager, grafana without having to find # the user/pass for each gardenctl show prometheus gardenctl show grafana gardenctl show alertmanager  Easiest way to obtain kubeconfig of the shoot cluster:\n$ gardenctl target shoot johndoe-azure KUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml $ export KUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml $ # From now on your local kubectl will be operating on target shoot $ kubectl cluster-info # will show your shoot cluster info $ unset KUBECONFIG # reset to your default kubectl  The shoot cluster\u0026rsquo;s kubeconfig is being kept in a secret in the project namespace:\nkubectl -n shoot--dev--johndoe-azure get secret kubecfg -o jsonpath='{.data.kubeconfig}' | base64 -D \u0026gt; /tmp/johndoe-azure-kubeconfig.yaml export KUBECONFIG=/tmp/johndoe-azure-kubeconfig.yaml  Delete Shoot cluster Deleting a Shoot cluster is not straight forward, and this is to protect users from undesired/accidental cluster deletion. One has to place some special annotations to get a Shoot cluster removed. We use the hack/delete-shoot for this purpose.\nPlease refer to Creating / Deleting a Shoot cluster document for more details.\nhack/delete-shoot johndoe-azure garden-dev  "
},
{
	"uri": "https://gardener.github.io/website/045_contribute/10_code/30_deploy_into_minikube/",
	"title": "Deploy into Minikube",
	"tags": [],
	"description": "",
	"content": " Deploying the Gardener into a Minikube with Local Provider Prerequisites Make sure that you have completed the following steps:\n Installing Golang environment Installing kubectl and helm Installing Minikube Installing iproute2 Installing Local Installing Virtualbox Get the sources Test nip.io  Running Then in a terminal execute:\n# Minikube requires extra memory for this setup $ minikube start --cpus=3 --memory=4096 --extra-config=apiserver.authorization-mode=RBAC # Allow Tiller and Dashboard to run in RBAC mode $ kubectl create clusterrolebinding add-on-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:default # Start Helm's Tiller $ helm init # Deploy Gardener with local specific configuration $ helm install charts/gardener \\ --name gardener \\ --namespace garden \\ --values=charts/gardener/values.yaml \\ --values=charts/gardener/local-values.yaml # Check that everything is deployed successfully and running without a problem $ kubectl -n garden get pods NAME READY STATUS RESTARTS AGE gardener-apiserver-d5989f856-swgbg 2/2 Running 0 32s gardener-controller-manager-6f7bd556d6-p98fx 1/1 Running 0 32s $ make dev-setup-local namespace \u0026quot;garden-dev\u0026quot; created cloudprofile \u0026quot;local\u0026quot; created [..]  You can now continue with Check Local Setup\n"
},
{
	"uri": "https://gardener.github.io/website/045_contribute/10_code/27_deploy_into_cluster/",
	"title": "Deploy into a Cluster",
	"tags": [],
	"description": "",
	"content": " Deploying the Gardener into a Kubernetes cluster As already mentioned, the Gardener is designed to run as API server extension in an existing Kubernetes cluster. In order to deploy it, you require valid Kubernetes manifests. We use Helm in order to generate these manifests. The respective Helm chart for the Gardener can be found here. In order to deploy it, execute\n$ helm install charts/gardener --name gardener --wait   You can configure the Helm chart by modifying the allowed configuration values. Please note that all resources and deployments need to be created in the garden namespace (not overrideable).  :warning: The Seed Kubernetes clusters need to have a nginx-ingress-conroller deployed to make the Gardener work properly. Moreover, there should exist a DNS record *.ingress.\u0026lt;SEED-CLUSTER-DOMAIN\u0026gt; where \u0026lt;SEED-CLUSTER-DOMAIN\u0026gt; is the value of the domain field of a Seed cluster resource.\n"
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/howto/dockerfile_pitfall/",
	"title": "Dockerfile pitfalls",
	"tags": [],
	"description": "Common Dockerfile pitfalls",
	"content": " Using :latest Many Dockerfiles use the FROM package:latest pattern at the top of their Dockerfiles to pull the latest image from a Docker registry.\nBad Dockerfile\nFROM alpine . . .  While simple, using the latest tag for an image means that your build can suddenly break if that image gets updated. This can lead to problems where everything builds fine locally (because your local cache thinks it is the latest) while a build server may fail, because some Pipelines makes a clean pull on every build. Additionally, troubleshooting can prove to be difficult, since the maintainer of the Dockerfile didn\u0026rsquo;t actually make any changes.\nTo prevent this, just make sure you use a specific tag of an image (example: alpine:3.3 or hashtag tomcat@sha256:c34ce3c1fcc0c7431e1392cc3abd0dfe2192ffea1898d5250f199d3ac8d8720f). This will ensure your Dockerfile remains immutable.\nGood Dockerfile\nFROM alpine:3.7 . . .  Running apt/apk/yum update Running apt-get install is one of those things virtually every Debian-based Dockerfile will have. This is due to satiate some external package requirements in order to run your code. But, using apt-get as an example, comes with its fair share of gotchas.\napt-get upgrade\nThis will update all your packages to their latests versions, which can be bad because it prevents your Dockerfile from creating consistent, immutable builds.\napt-get update in a different line than running your apt-get install command.\nRunning apt-get update as a single line entry will get cached by the build and won\u0026rsquo;t actually run every time you need to run apt-get install. Instead, make sure you run apt-get update in the same line with all the packages to ensure all are updated correctly.\nBuild small container images Building small container image will enable fast pod startup time, quick restart of pod in case of failure or rolling update. Typically image based on the popular Alpine Linux project is much smaller than most distribution based images (~5MB), and results in much slimmer images in general. For most popular languages and products, there are usually an official Alpine Linux image, e.g. golang, nodejs and postgres.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE postgres 9.6.9-alpine 6583932564f8 13 days ago 39.26 MB postgres 9.6 d92dad241eff 13 days ago 235.4 MB postgres 10.4-alpine 93797b0f31f4 13 days ago 39.56 MB  In addition, for compiled languages such as Go or C++ which does not requires build time tooling during runtime, it is recommended to avoid build time tooling in the final images. With Docker\u0026rsquo;s support for multi-stages builds this can be easily achieved with minimal effort. Such an example can be found here. Google\u0026rsquo;s distroless image is also a good base image.\n"
},
{
	"uri": "https://gardener.github.io/website/contribute/docs/",
	"title": "Documentation",
	"tags": [],
	"description": "",
	"content": " Contributing Documentation How to Contribute to the Open Source Project Gardener    You are welcome to contribute documentation to Gardener.\nThe following rules govern documentation contributions: * Contributions must be licensed under the Creative Commons Attribution 4.0 International License * You need to sign the Contributor License Agreement. We are using CLA assistant providing a click-through workflow for accepting the CLA. For company contributors additionally the company needs to sign a corporate license agreement. See the following sections for details.\n "
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/howto/dynamic-pvc/",
	"title": "Dynamic Volume Provisioning",
	"tags": [],
	"description": "How to dynamically provision volume",
	"content": " Introduction This tutorial is complementary to guide \u0026ldquo;Running postgres on Kubernetes\u0026rdquo; which covers an end to end scenario from \u0026ldquo;provisioning persistent volume (PV)\u0026rdquo; by administrator to \u0026ldquo;consuming PV with Persistent Volume Claim(PVC)\u0026rdquo; by developer. However, administrators bear the burden of making calls to their cloud or storage provider to create new storage volumes, and then create PersistentVolume objects to represent them in Kubernetes.\nIn this tutorial dynamic provision is presented through an example to eliminate the administrative task of pre-provisioning storage. The examples runs postgres database on K8S, and dynamic provisioning is used to provide storage for two seperate mount volumes.\nRun postgres database A yaml file defines following resources on K8S:\n PersistentVolumeClaim (PVC) Deployment  PersistentVolumeClaim apiVersion: v1 # pvc for postgredb-pvc kind: PersistentVolumeClaim metadata: name: postgresdb-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 9Gi storageClassName: 'default'  In above definition a pvc is created which uses storage class \u0026lsquo;default\u0026rsquo;. Storage class are essentially blueprints that abstract away the underlying storage provider, as well as other parameters, like disk-type (e.g.; solid-state vs standard disks).\nThe default storage class has annotation {\u0026ldquo;storageclass.kubernetes.io/is-default-class\u0026rdquo;:\u0026ldquo;true\u0026rdquo;}.\n$ kubectl describe sc default Name: default IsDefaultClass: Yes Annotations: kubectl.kubernetes.io/last-applied-configuration={\u0026quot;apiVersion\u0026quot;:\u0026quot;storage.k8s.io/v1beta1\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;StorageClass\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{\u0026quot;storageclass.kubernetes.io/is-default-class\u0026quot;:\u0026quot;true\u0026quot;},\u0026quot;labels\u0026quot;:{\u0026quot;addonmanager.kubernetes.io/mode\u0026quot;:\u0026quot;Exists\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;default\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;\u0026quot;},\u0026quot;parameters\u0026quot;:{\u0026quot;type\u0026quot;:\u0026quot;gp2\u0026quot;},\u0026quot;provisioner\u0026quot;:\u0026quot;kubernetes.io/aws-ebs\u0026quot;} ,storageclass.kubernetes.io/is-default-class=true Provisioner: kubernetes.io/aws-ebs Parameters: type=gp2 AllowVolumeExpansion: \u0026lt;unset\u0026gt; MountOptions: \u0026lt;none\u0026gt; ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: \u0026lt;none\u0026gt;  A Persistent Volume is automatically created when it is dynamically provisioned. In following example, the PVC is defined as \u0026ldquo;postgresdb-pvc\u0026rdquo;, and a pv \u0026ldquo;pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb\u0026rdquo; is created and associated with pvc automatically.\n$ kubectl create -f .\\postgres_deployment.yaml persistentvolumeclaim \u0026quot;postgresdb-pvc\u0026quot; created $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Delete Bound default/postgresdb-pvc default 3s $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE postgresdb-pvc Bound pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO default 8s  Notice that the RECLAIM POLICY is Delete (default value), which is one of the two reclaim policies, the other one being Retain. (A third policy Recycle has been deprecated). In case of Delete, PV is deleted automatically when PVC is removed, and the data on the PVC will also be lost. On the other hand, PV with Retain policy will not be deleted if PVC is removed, and moved to Release status, so that data can be recovered by Administrators later.\nTo change the reclaim policy, you can use kubectl patch command as described here or use kubectl edit pv \u0026lt;pv-name\u0026gt; to edit online as below:\n$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Delete Bound default/postgresdb-pvc default 44m # change the relcaim policy from \u0026quot;Delete\u0026quot; to \u0026quot;Retain\u0026quot; $ kubectl edit pv pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb persistentvolume \u0026quot;pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb\u0026quot; edited # check the reclaim policy afterwards $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Retain Bound default/postgresdb-pvc default 45m  Deployment Once PVC is created, you can use it in your container via volumes.persistentVolumeClaim.claimName. In below example, pvc postgresdb-pvc is mounted as readable and writable, and in volumeMounts two different paths in container are mounted to two subfolders: \u0026ldquo;data\u0026rdquo; and \u0026ldquo;logs\u0026rdquo;.\napiVersion: \u0026quot;extensions/v1beta1\u0026quot; # deployment kind: Deployment metadata: name: postgres namespace: default labels: app: postgres annotations: deployment.kubernetes.io/revision: \u0026quot;1\u0026quot; spec: replicas: 1 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 selector: matchLabels: app: postgres template: metadata: name: postgres labels: app: postgres spec: containers: - name: postgres image: \u0026quot;cpettech.docker.repositories.sap.ondemand.com/jtrack_postgres:howto\u0026quot; env: - name: POSTGRES_USER value: postgres - name: POSTGRES_PASSWORD value: p5FVqfuJFrM42cVX9muQXxrC3r8S9yn0zqWnFR6xCoPqxqVQ - name: POSTGRES_INITDB_XLOGDIR value: \u0026quot;/var/log/postgresql/logs\u0026quot; ports: - containerPort: 5432 volumeMounts: - mountPath: /var/lib/postgresql/data name: postgre-db subPath: data # https://github.com/kubernetes/website/pull/2292. Solve the issue of crashing initdb due to non-empty directory (i.e. lost+found) - mountPath: /var/log/postgresql/logs name: postgre-db subPath: logs volumes: - name: postgre-db persistentVolumeClaim: claimName: postgresdb-pvc readOnly: false imagePullSecrets: - name: cpettechregistry  To check the mount points from within the container:\n$ kubectl get po NAME READY STATUS RESTARTS AGE postgres-7f485fd768-c5jf9 1/1 Running 0 32m $ kubectl exec -it postgres-7f485fd768-c5jf9 bash root@postgres-7f485fd768-c5jf9:/# ls /var/lib/postgresql/data/ base pg_clog pg_dynshmem pg_ident.conf pg_multixact pg_replslot pg_snapshots pg_stat_tmp pg_tblspc PG_VERSION postgresql.auto.conf postmaster.opts global pg_commit_ts pg_hba.conf pg_logical pg_notify pg_serial pg_stat pg_subtrans pg_twophase pg_xlog postgresql.conf postmaster.pid root@postgres-7f485fd768-c5jf9:/# ls /var/log/postgresql/logs/ 000000010000000000000001 archive_status  Deleting PersistentVolumeClaim In case of \u0026ldquo;Delete\u0026rdquo; policy, deleting PVC will also delete its associated PV. If \u0026ldquo;Retain\u0026rdquo; is the reclaim policy, the PV will change to status from Bound to Released when pvc is deleted.\n# Check pvc and pv before deletion $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE postgresdb-pvc Bound pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO default 50m $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Retain Bound default/postgresdb-pvc default 50m # delete pvc $ kubectl delete pvc postgresdb-pvc persistentvolumeclaim \u0026quot;postgresdb-pvc\u0026quot; deleted # pv changed to status \u0026quot;Released\u0026quot; $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Retain Released default/postgresdb-pvc default 51m  "
},
{
	"uri": "https://gardener.github.io/website/045_contribute/10_code/10_env/",
	"title": "Enviroment",
	"tags": [],
	"description": "",
	"content": " Preparing the setup Conceptionally, the Gardener is designated to run in containers within a Pod inside an Kubernetes cluster. It extends the API via the user-aggregated API server concepts. However, if you want to develop it, you may want to work locally with the Gardener without building a Docker image and deploying it to a cluster each and every time. That means that the Gardener runs outside a Kubernetes cluster which requires providing a Kubeconfig in your local filesystem and point the Gardener to it when starting it (see below).\nFurther details could be found in\n principles of Kubernetes, and its components Kubernetes Development Guide architecture of the Garden  This setup is based on minikube, a Kubernetes cluster running on a single node.\nInstalling Golang environment Install the latest version of Golang (at least v1.9.2 is required). For Mac OS you could use Homebrew:\n$ brew install golang  For other OS, please check Go installation documentation.\nMake sure to set your $GOPATH environment variable properly (conventionally, it points to $HOME/go).\nFor your convenience, you can add the bin directory of the $GOPATH to your $PATH: PATH=$PATH:$GOPATH/bin, but it is not necessarily required.\nWe use Dep for managing Golang package dependencies. Please install it on Mac OS via\n$ brew install dep  On other OS please check the Dep installation documentation and the Dep releases page. After downloading the appropriate release in your $GOPATH/bin folder you need to make it executable via chmod +x \u0026lt;dep-release\u0026gt; and to rename it to dep via mv dep-\u0026lt;release\u0026gt; dep.\nGolint In order to perform linting on the Go source code, please install Golint:\n$ go get -u github.com/golang/lint/golint  Ginkgo and Gomega In order to perform tests on the Go source code, please install Ginkgo and Gomega. Please make yourself familiar with both frameworks and read their introductions after installation:\n$ go get -u github.com/onsi/ginkgo/ginkgo $ go get -u github.com/onsi/gomega  Installing kubectl and helm As already mentioned in the introduction, the communication with the Gardener happens via the Kubernetes (Garden) cluster it is targeting. To interact with that cluster, you need to install kubectl.\nOn Mac OS run\n$ brew install kubernetes-cli  Please check the kubectl installation documentation for other OS.\nYou may also need to develop Helm charts or interact with Tiller using the Helm CLI:\nOn Mac OS run\n$ brew install kubernetes-helm  On other OS please check the Helm installation documentation.\nInstalling git We use git as VCS which you need to install.\nOn Mac OS run\n$ brew install git  On other OS, please check the Git installation documentation.\nInstalling openvpn We use OpenVPN to establish network connectivity from the control plane running in the Seed cluster to the Shoot\u0026rsquo;s worker nodes running in private networks. To harden the security we need to generate another secret to encrypt the network traffic (details). Please install the openvpn binary. On Mac OS run\n$ brew install openvpn  On other OS, please check the OpenVPN downloads page.\nInstalling Minikube You\u0026rsquo;ll need to have minikube installed and running.\nInstalling iproute2 iproute2 provides a collection of utilities for network administration and configuration.\nOn Mac OS run\n$ brew install iproute2mac  [Optional] Installing Docker In case you want to build Docker images for the Gardener you have to install Docker itself. We recommend using Docker for Mac OS X which can be downloaded from here.\nOn other OS, please check the Docker installation documentation.\n[Optional] Installing gcloud SDK In case you have to create a new release or a new hotfix of the Gardener you have to push the resulting Docker image into a Docker registry. Currently, we are using the Google Container Registry (this could change in the future). Please follow the official installation instructions from Google.\nInstalling Vagrant In case you want to run the gardener-local-provider and test the creation of Shoot clusters on your machine you have to install Vagrant.\nPlease make sure that the executable bsdtar is available on your system.\nInstalling Virtualbox In this local setup a virtualizer is needed. Here, Virtualbox is used. However, Vagrant supports other virtualizers as well. Please check the Vagrant documentation for further details.\nTest nip.io nip.io is used as an unmanaged DNS implementation for the local setup. Some ISPs don\u0026rsquo;t handle nip.io very well. Test NS resolution:\nnslookup 192.168.99.201.nip.io Server: 8.8.8.8 Address: 8.8.8.8#53 Non-authoritative answer: Name: 192.168.99.201.nip.io Address: 192.168.99.201  If there is an error, switch your DNS server to 8.8.8.8 / 8.8.4.4 or 1.1.1.1.\nLocal Gardener setup This setup is only meant to be used for developing purposes, which means that only the control plane of the Gardener cluster is running on your machine.\nGet the sources Clone the repository from GitHub into your $GOPATH.\n$ mkdir -p $GOPATH/src/github.com/gardener $ cd $GOPATH/src/github.com/gardener $ git clone git@github.com:gardener/gardener.git $ cd gardener  Start the Gardener :warning: Before you start developing, please ensure to comply with the following requirements:\n You have understood the principles of Kubernetes, and its components, what their purpose is and how they interact with each other. You have understood the architecture of the Garden, and what the various clusters are used for.  The development of the Gardener could happen by targeting any cluster. You basically need a Garden cluster (e.g., a Minikube cluster) and one Seed cluster per cloud provider and per data center/region. You can configure the Gardener controller manager to watch all namespaces for Shoot manifests or to only watch one single namespace.\nThe commands below will configure your minikube with the absolute minimum resources to launch Gardener API Server and Gardener Controller Manager on a local machine.\nStart minikube First, start minikube with at least Kubernetes v1.9.x, e.g. via minikube --kubernetes-version=v1.9.0\n$ minikube start --kubernetes-version=v1.9.0 Starting local Kubernetes v1.9.0 cluster... [...] kubectl is now configured to use the cluster.  Prepare the Gardener The Gardener exposes the API servers of Shoot clusters via Kubernetes services of type LoadBalancer. In order to establish stable endpoints (robust against changes of the load balancer address), it creates DNS records pointing to these load balancer addresses. They are used internally and by all cluster components to communicate. You need to have control over a domain (or subdomain) for which these records will be created. Please provide an internal domain secret (see this for an example) which contains credentials with the proper privileges. Further information can be found here.\n$ make dev-setup namespace \u0026quot;garden\u0026quot; created namespace \u0026quot;garden-dev\u0026quot; created secret \u0026quot;internal-domain-unmanaged\u0026quot; created deployment \u0026quot;etcd\u0026quot; created service \u0026quot;etcd\u0026quot; created service \u0026quot;gardener-apiserver\u0026quot; created endpoints \u0026quot;gardener-apiserver\u0026quot; created apiservice \u0026quot;v1beta1.garden.sapcloud.io\u0026quot; created  Run the Gardener API Server and the Gardener Controller Manager Next, you need to run the Gardener API Server and the Gardener Controller Manager using rules in the Makefile.\n$ make start-api [restful] 2018/02/01 15:39:43 log.go:33: [restful/swagger] listing is available at https:///swaggerapi [restful] 2018/02/01 15:39:43 log.go:33: [restful/swagger] https:///swaggerui/ is mapped to folder /swagger-ui/ I0201 15:39:43.750573 84958 serve.go:89] Serving securely on [::]:8443 [...]  In another terminal, launch the Gardener Controller Manager\n$ make start time=\u0026quot;2018-02-20T13:24:39+02:00\u0026quot; level=info msg=\u0026quot;Starting Gardener controller manager...\u0026quot; time=\u0026quot;2018-02-20T13:24:39+02:00\u0026quot; level=info msg=\u0026quot;Feature Gates: \u0026quot; time=\u0026quot;2018-02-20T13:24:39+02:00\u0026quot; level=info msg=\u0026quot;Gardener controller manager HTTP server started (serving on 0.0.0.0:2718)\u0026quot; time=\u0026quot;2018-02-20T13:24:39+02:00\u0026quot; level=info msg=\u0026quot;Found internal domain secret internal-domain-unmanaged for domain nip.io.\u0026quot; time=\u0026quot;2018-02-20T13:24:39+02:00\u0026quot; level=info msg=\u0026quot;Successfully bootstrapped the Garden cluster.\u0026quot; time=\u0026quot;2018-02-20T13:24:39+02:00\u0026quot; level=info msg=\u0026quot;Gardener controller manager (version 0.2.0) initialized.\u0026quot; time=\u0026quot;2018-02-20T13:24:39+02:00\u0026quot; level=info msg=\u0026quot;Quota controller initialized.\u0026quot; time=\u0026quot;2018-02-20T13:24:39+02:00\u0026quot; level=info msg=\u0026quot;CloudProfile controller initialized.\u0026quot; time=\u0026quot;2018-02-20T13:24:39+02:00\u0026quot; level=info msg=\u0026quot;SecretBinding controller initialized.\u0026quot; time=\u0026quot;2018-02-20T13:24:39+02:00\u0026quot; level=info msg=\u0026quot;Watching all namespaces for Shoot resources...\u0026quot; time=\u0026quot;2018-02-20T13:24:39+02:00\u0026quot; level=info msg=\u0026quot;Shoot controller initialized.\u0026quot; time=\u0026quot;2018-02-20T13:24:39+02:00\u0026quot; level=info msg=\u0026quot;Seed controller initialized.\u0026quot; [...]  :information_source: Your username is inferred from the user you are logged in with on your machine. The version is incremented based on the content of the VERSION file. The version is important for the Gardener in order to identify which Gardener version has last operated a Shoot cluster.\nThe Gardener should now be ready to operate on Shoot resources. You can use\n$ kubectl get shoots No resources found.  to operate against your local running Gardener API server.\n Note: It may take several seconds until the minikube cluster recognizes that the Gardener API server has been started and is available. No resources found is the expected result of our initial development setup.\n Configure minikube to act as Gardener and Seed Cluster The Gardener Local Provider gives you the ability to create Shoot clusters on your local machine without the need to have an account on a Cloud Provider. Please make sure that Vagrant is installed (see section Installing Vagrant)\nMake sure that you already run make dev-setup and that the Gardener API server and the Gardener controller manager are running via make start-api and make start as described before.\nNext, you need to configure minikube to work as the Gardener and as the Seed cluster in such a way that it uses the local Vagrant installation to create the Shoot clusters.\n$ make dev-setup-local cloudprofile \u0026quot;local\u0026quot; created secret \u0026quot;dev-local\u0026quot; created secretbinding \u0026quot;core-local\u0026quot; created Cluster \u0026quot;gardener-dev\u0026quot; set. User \u0026quot;gardener-dev\u0026quot; set. Context \u0026quot;gardener-dev\u0026quot; modified. Switched to context \u0026quot;gardener-dev\u0026quot;. secret \u0026quot;seed-local-dev\u0026quot; created seed \u0026quot;local-dev\u0026quot; created  Check Vagrant setup To be sure that the Vagrant has been successfully installed and configured, test your setup:\n$ cd vagrant $ vagrant up Bringing machine 'core-01' up with 'virtualbox' provider... ==\u0026gt; core-01: Importing base box 'coreos-stable'... ==\u0026gt; core-01: Configuring Ignition Config Drive ==\u0026gt; core-01: Matching MAC address for NAT networking... [...]  If successful, delete your machine before continuing:\n$ vagrant destroy --force ==\u0026gt; core-01: Forcing shutdown of VM... ==\u0026gt; core-01: Destroying VM and associated drives... $ cd $GOPATH/src/github.com/gardener/gardener  Start the Gardener Local Provider The Seed cluster provides the possibility to create Shoot clusters on several cloud provider. The Gardener Provider implements a common interface to all supported cloud providers. Here, the corresponding Gardener Provider for Local is used.\nBy executing\n$ make start-local 2018/02/14 10:53:34 Listening on :3777 2018/02/14 10:53:34 Vagrant directory /Users/foo/go/src/github.com/gardener/gardener/vagrant 2018/02/14 10:53:34 user-data path /Users/foo/git/go/src/github.com/gardener/gardener/dev/user-data  the Gardener Local Provider is started.\nAt this point three processes should run in an individual terminal, the Gardener API server, the Gardener controller manager and finally the Gardener Local Provider.\nCreate, access and delete a Shoot Cluster Now, you can create a Shoot cluster by running\n$ kubectl apply -f dev/90-shoot-local.yaml shoot \u0026quot;local\u0026quot; created  When the Shoot API server is created you can download the kubeconfig for it and access it:\n$ kubectl --namespace shoot--dev--local get secret kubecfg -o jsonpath=\u0026quot;{.data.kubeconfig}\u0026quot; | base64 --decode \u0026gt; dev/shoot-kubeconfig # Depending on your Internet speed, it can take some time, before your node reports a READY status. $ kubectl --kubeconfig dev/shoot-kubeconfig get nodes NAME STATUS ROLES AGE VERSION 192.168.99.201.nip.io Ready node 1m v1.9.1   Note: It is required that your minikube has network connectivity to the nodes created by Vagrant.\n For additional debugging on your Vagrant node you can ssh into it\n$ cd vagrant $ vagrant ssh  To delete the Shoot cluster\n$ ./hack/delete-shoot local garden-dev shoot \u0026quot;local\u0026quot; deleted shoot \u0026quot;local\u0026quot; patched  Limitations Currently, there are some limitations in the local Shoot setup which need to be considered. Please keep in mind that this setup is intended to be used by Gardener developers.\n The cloud provider allows to choose from a various list of different machine types. This flexibility is not available in this setup on a single local machine. However, it is possible to specify the Shoot nodes resources (cpu and memory) used by Vagrant in this configuration file. In the Shoot creation process the Machine Controller Manager plays a central role. Due to the limitation in this setup this component is not used. It is not yet possible to create Shoot clusters consisting of more than one worker node. Cluster Autoscaling therefore is not supported It is not yet possible to create two or more Shoot clusters in parallel The Shoot API Server is exposed via a NodePort. In a cloud setup a LoadBalancer would be used The communication between the Seed and the Shoot Clusters uses VPN tunnel. In this setup tunnels are not needed since all components run on localhost  Additional information In order to ensure that a specific Seed cluster will be chosen, add the .spec.cloud.seed field (see here for an example Shoot manifest).\nPlease take a look at the example manifests folder to see which resource objects you need to install into your Garden cluster.\n"
},
{
	"uri": "https://gardener.github.io/website/045_contribute/20_documentation/25_markup/expand/",
	"title": "Expand",
	"tags": [],
	"description": "Displays an expandable/collapsible section of text on your page",
	"content": " The Expand shortcode displays an expandable/collapsible section of text on your page. Here is an example\n  Expand me...   Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n  Usage this shortcode takes exactly one optional parameter to define the text that appears next to the expand/collapse icon. (default is \u0026ldquo;Expand me\u0026hellip;\u0026rdquo;)\n{{%expand \u0026quot;Is this learn theme rocks ?\u0026quot; %}}Yes !.{{% /expand%}}    Is this learn theme rocks ?   Yes !  \nDemo {{%expand%}} Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. {{% /expand%}}    Expand me...   Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n  "
},
{
	"uri": "https://gardener.github.io/website/010-blog/2018_week_17/",
	"title": "Frontend HTTPS",
	"tags": [],
	"description": "",
	"content": " For encrypted communication between the client to the load balancer, you need to specify a TLS private key and certificate to be used by the ingress controller.\nFor the ingress controller to use the certificate and private key stored in a Kubernetes secret, user needs to specify the secret name in the TLS configuration section of their ingress spec. The secret is assumed to exist in the same namespace as the ingress.\n..read on HTTPS - Self Signed Certificates how to configure it.\n"
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/howto/gpu/",
	"title": "GPU Enabled Cluster",
	"tags": [],
	"description": "Setting up a GPU Enabled Cluster for Deep Learning",
	"content": " Intro Be aware, that the following sections might be opinionated. Kubernetes, and the GPU support in particular, is a rapidly developing environment, which means that this guide is likely to be outdated sometimes. For this reason, contributions are highly appreciated.\nCreate a Cluster First thing first, let’s create a k8s cluster with GPU accelerated nodes. In this example we will use AWS p2.xlarge EC2 instance because it\u0026rsquo;s the cheapest available option at the moment. If you are also trying this out I\u0026rsquo;d suggest you using this instance type to avoid hitting your bill heavily. More or less 1€/hour per GPU\nInstall NVidia Driver as Deamonset apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: nvidia-driver-installer namespace: kube-system labels: k8s-app: nvidia-driver-installer spec: template: metadata: labels: name: nvidia-driver-installer k8s-app: nvidia-driver-installer spec: hostPID: true initContainers: - image: squat/modulus:13bb5a54d558ddce0ad2a7702fdd18083f0b4ac7 name: modulus args: - compile - nvidia - \u0026quot;390.48\u0026quot; securityContext: privileged: true env: - name: MODULUS_CHROOT value: \u0026quot;true\u0026quot; - name: MODULUS_INSTALL value: \u0026quot;true\u0026quot; - name: MODULUS_INSTALL_DIR value: /opt/drivers - name: MODULUS_CACHE_DIR value: /opt/modulus/cache - name: MODULUS_LD_ROOT value: /root volumeMounts: - name: etc-coreos mountPath: /etc/coreos readOnly: true - name: usr-share-coreos mountPath: /usr/share/coreos readOnly: true - name: ld-root mountPath: /root - name: module-cache mountPath: /opt/modulus/cache - name: module-install-dir-base mountPath: /opt/drivers - name: dev mountPath: /dev containers: - image: \u0026quot;gcr.io/google-containers/pause:3.1\u0026quot; name: pause tolerations: - key: \u0026quot;nvidia.com/gpu\u0026quot; effect: \u0026quot;NoSchedule\u0026quot; operator: \u0026quot;Exists\u0026quot; volumes: - name: etc-coreos hostPath: path: /etc/coreos - name: usr-share-coreos hostPath: path: /usr/share/coreos - name: ld-root hostPath: path: / - name: module-cache hostPath: path: /opt/modulus/cache - name: dev hostPath: path: /dev - name: module-install-dir-base hostPath: path: /opt/drivers  Install Device Plugin apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: nvidia-gpu-device-plugin namespace: kube-system labels: k8s-app: nvidia-gpu-device-plugin #addonmanager.kubernetes.io/mode: Reconcile spec: template: metadata: labels: k8s-app: nvidia-gpu-device-plugin annotations: scheduler.alpha.kubernetes.io/critical-pod: '' spec: priorityClassName: system-node-critical volumes: - name: device-plugin hostPath: path: /var/lib/kubelet/device-plugins - name: dev hostPath: path: /dev containers: - image: \u0026quot;k8s.gcr.io/nvidia-gpu-device-plugin@sha256:0842734032018be107fa2490c98156992911e3e1f2a21e059ff0105b07dd8e9e\u0026quot; command: [\u0026quot;/usr/bin/nvidia-gpu-device-plugin\u0026quot;, \u0026quot;-logtostderr\u0026quot;, \u0026quot;-host-path=/opt/drivers/nvidia\u0026quot;] name: nvidia-gpu-device-plugin resources: requests: cpu: 50m memory: 10Mi limits: cpu: 50m memory: 10Mi securityContext: privileged: true volumeMounts: - name: device-plugin mountPath: /device-plugin - name: dev mountPath: /dev updateStrategy: type: RollingUpdate  Test To run an example training on a GPU node, start first a base image with Tensorflow with GPU support \u0026amp; Keras\napiVersion: apps/v1 kind: Deployment metadata: name: deeplearning-workbench namespace: default spec: replicas: 1 selector: matchLabels: app: deeplearning-workbench template: metadata: labels: app: deeplearning-workbench spec: containers: - name: deeplearning-workbench image: afritzler/deeplearning-workbench resources: limits: nvidia.com/gpu: 1 tolerations: - key: \u0026quot;nvidia.com/gpu\u0026quot; effect: \u0026quot;NoSchedule\u0026quot; operator: \u0026quot;Exists\u0026quot;  Now exec into the container and start an example Keras traing\nkubectl exec -it deeplearning-workbench-8676458f5d-p4d2v -- /bin/bash cd /keras/example python imdb_cnn.py  Acknowledgments \u0026amp; References  Andreas Fritzler from the Gardener Core team for the R\u0026amp;D and providing this setup. Build and install NVIDIA driver on CoreOS Nvidia Device Plugin  "
},
{
	"uri": "https://gardener.github.io/website/components/gardner/",
	"title": "Gardener",
	"tags": [],
	"description": "",
	"content": " Gardener  \nThe Gardener implements the automated management and operation of Kubernetes clusters as a service and aims to support that service on multiple Cloud providers (AWS, GCP, Azure, OpenStack). Its main principle is to use Kubernetes itself as base for its tasks.\nIn essence, the Gardener is an extension API server along with a bundle of Kubernetes controllers which introduces new API objects in an existing Kubernetes cluster (which is called Garden cluster) in order to use them for the management of further Kubernetes clusters (which are called Shoot clusters). To do that reliably and to offer a certain quality of service, it requires to control the main components of a Kubernetes cluster (etcd, API server, controller manager, scheduler). These so-called control plane components are hosted in Kubernetes clusters themselves (which are called Seed clusters).\nPlease find more information regarding the concepts and a detailed description of the architecture in our Gardener Wiki and our blog post on kubernetes.io.\nTo start using or developing the Gardener locally See our documentation in the /docs repository, please find the index here.\nSetting up your own Gardener landscape in the cloud Take a look at our landscape setup template to bootstrap your own Gardener system.\nFeedback and Support Feedback and contributions are always welcome. Please report bugs or suggestions about our Kubernetes clusters as such or the Gardener itself as GitHub issues or join our Slack workspace (find the self-service invitation link here).\n"
},
{
	"uri": "https://gardener.github.io/website/",
	"title": "Gardener",
	"tags": [],
	"description": "",
	"content": "Kubernetes Clusters as a Service  Gardener The Kubernetes Botanist   Many Open Source tools exist which help in creating and updating single Kubernetes clusters. However, the more clusters you need the harder it becomes to operate, monitor, manage and keep all of them alive and up-to-date. And that is exactly what project Gardener focuses on.       Our Mission  100% Kubernetes Inspired by the possibilities of Kubernetes and the ability to self-host, the foundation of Gardener is Kubernetes itself. Gardener applies a special pattern catering to the needs of operating a huge number of clusters with minimal total cost of ownership. Overall, reusing Kubernetes primitives in Gardeners core architecture simplifies deployment, scaling \u0026amp; patching/updating of all control planes under Gardener's management.  100% Fast \u0026amp; Simple We agree that cloud native will become the foundation and the de facto standard for shipping software fast, simple and reliable using Kubernetes as the new \"virtualisation\" \u0026amp; deployment fabric/underlay. We expect that Kubernetes will become a forcing function.       Create clusters via self service Simple and powerful  Despite requiring only the familiar kubectl command line tool for managing all of Gardener, we provide a central dashboard for comfortable interaction. It enables users to easily keep track of their clusters’ health, and operators to monitor, debug, and analyze the clusters they are responsible for.    More focused on the duties of developers and operators, the Gardener command line client gardenctl simplifies administrative tasks by introducing easy higher-level abstractions with simple commands that allow to condense and multiplex information \u0026 actions from/to a set of seed and shoot clusters.  The clusters are self-healing, auto-scaling - and if you choose to - also auto-updating. The Gardener will show you details on your cluster like the Kubernetes dashboard URL or the credentials you need to access it via kubectl.  For a high level discussion on the motivation for Gardener and its architecture, read the blog on kubernetes.io     Full transparency of CNCF conformance test results 100% Kubernetes  Starting from Kubernetes release 1.10 the conformance test results of clusters provided by the Gardener are published on Testgrid.  One major goal of Gardener is to provide Kubernetes clusters which completely satisfy the requirements of the CNCF.     For this the CNCF launched in 2017 a certification program. As of the Kubernetes release 1.8 the clusters created by the Gardener are already officially certified by the CNCF. In addition, the CNCF offers the public dashboard Testgrid, where besides others the conformance test results are published. A typical use case is to add a publish step into an existing CI/CD pipelines as a target for the conformance test results. Testgrid offers an easy way to investigate historical conformance test results by visualizing the result of each test and by providing access to the test logs.     start contributing Project members  SAP is working on Gardener since mid 2017 and is focused on building up a project that can easily be evolved and extended. Consequently, we are looking for further partners and contributors to the project now. As outlined above, we completely rely on Kubernetes primitives, add-ons, and specifications and adapt its innovative cloud native approach. We are looking forward to aligning with and contributing to the Kubernetes community, especially with the upcoming cluster and machine specifications from SIG Cluster Lifecycle.     Feedback    The Gardener is fully Open Source and developed in the public on GitHub. Feedback is always welcome. Please report bugs or suggestions about our Kubernetes clusters as such at the gardener project and about the user interface at the dashboard project.      Start Contributing If you also see the potential of the Gardener project then please learn more about it on GitHub.   Github     "
},
{
	"uri": "https://gardener.github.io/website/about/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": " Gardener  \nThe Gardener implements the automated management and operation of Kubernetes clusters as a service and aims to support that service on multiple Cloud providers (AWS, GCP, Azure, OpenStack). Its main principle is to use Kubernetes itself as base for its tasks.\nIn essence, the Gardener is an extension API server along with a bundle of Kubernetes controllers which introduces new API objects in an existing Kubernetes cluster (which is called Garden cluster) in order to use them for the management of further Kubernetes clusters (which are called Shoot clusters). To do that reliably and to offer a certain quality of service, it requires to control the main components of a Kubernetes cluster (etcd, API server, controller manager, scheduler). These so-called control plane components are hosted in Kubernetes clusters themselves (which are called Seed clusters).\nPlease find more information regarding the concepts and a detailed description of the architecture in our Gardener Wiki and our blog post on kubernetes.io.\nTo start using or developing the Gardener locally See our documentation in the /docs repository, please find the index here.\nSetting up your own Gardener landscape in the cloud Take a look at our landscape setup template to bootstrap your own Gardener system.\nFeedback and Support Feedback and contributions are always welcome. Please report bugs or suggestions about our Kubernetes clusters as such or the Gardener itself as GitHub issues or join our Slack workspace (find the self-service invitation link here).\n"
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/app/https/",
	"title": "HTTPS with self Signed Certificate",
	"tags": [],
	"description": "HTTPS with self Signed Certificate",
	"content": " Configuring ingress with front-end TLS It is alyways recommended to enable encryption for services to prevent traffic interception and man-in-the-middle attacks - even in DEV environments.\nYou should configure front-end Transport Layer Security (TLS) so that the ingress controller can secure access to a service from the client to the load balancer by using HTTPS.\nWe will use basic procedure here. If your configuration requires advanced security options, please refer to official CloudFlare\u0026rsquo;s cfssl documentation.\nBefore you begin At first, you need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using Gardener\nInstall CFSSL The first step in securing Docker and Kubernetes is to set up a PKI infrastructure for managing TLS certificates.\nInitialize a CA Before we can generate any certs we need to initialize a CA.\nmkdir cfssl cd cfssl cfssl print-defaults config \u0026gt; ca-config.json cfssl print-defaults csr \u0026gt; ca-csr.json  Configure CA options Now we can configure signing options inside ca-config.json config file. Default options contain following preconfigured fields:\n profiles: www with server auth (TLS Web Server Authentication) X509 V3 extension and client with client auth (TLS Web Client Authentication) X509 V3 extension. expiry: with 8760h default value (or 365 days)  For compliance let\u0026rsquo;s edit the ca-config.json file and rename www profile into server\nEdit the ca-csr.json to your needs. See example below. Keep in mind that the hosts entries must match all your ingress entries.\nexample ca-csr.json\n{ \u0026quot;CN\u0026quot;: \u0026quot;Gardener Self Signed CA\u0026quot;, \u0026quot;hosts\u0026quot;: [ \u0026quot;ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026quot;, \u0026quot;api.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026quot; ], \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;ecdsa\u0026quot;, \u0026quot;size\u0026quot;: 256 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;US\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;CA\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;San Francisco\u0026quot; } ] }  And generate CA with defined options:\ncfssl gencert -initca ca-csr.json | cfssljson -bare ca -  You\u0026rsquo;ll get following files:\n ca-key.pem ca.csr ca.pem  Note: Please keep ca-key.pem file in safe. This key allows to create any kind of certificates within your CA.\nGenerate server certificate cfssl print-defaults csr \u0026gt; server.json  Most important values for server certificate are Common Name (CN) and hosts. We have to substitute them, for example:\n{ \u0026quot;CN\u0026quot;: \u0026quot;Gardener Self Signe CA\u0026quot;, \u0026quot;hosts\u0026quot;: [ \u0026quot;ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026quot;, \u0026quot;api.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026quot; ], \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;ecdsa\u0026quot;, \u0026quot;size\u0026quot;: 256 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;US\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;CA\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;San Francisco\u0026quot; } ] }  Now we are ready to generate server certificate and private key:\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server server.json | cfssljson -bare server  You\u0026rsquo;ll get following files: - server-key.pem - server.csr - server.pem\nConfigure Kubernetes ingress with TLS To configure front-end TLS, you need to create a TLS certificate (already done above), create a Kubernetes secret, update applicable .yaml files, apply your .yaml file changes, regenerate ingress controllers, and visit the application.\nCreate Kubernetes secret kubectl create secret tls tls-secret --key ./server-key.pem --cert server.pem  Create Service / Ingress now you can referenc ethe TLS secret within your ingress definition\nexample ingress definition\napiVersion: v1 kind: Service metadata: labels: app: node-server name: node-svc namespace: default spec: type: NodePort ports: - port: 8080 selector: app: node-server --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: node-ingress spec: tls: - hosts: - ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com secretName: tls-secret rules: - host: ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com http: paths: - backend: serviceName: node-svc servicePort: 8080  "
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/howto/debug-a-pod/",
	"title": "How to debug a pod",
	"tags": [],
	"description": "Your pod doesn&#39;t run as expected. Are there any log files? Where? How could I debug a pod?",
	"content": " Introduction Kubernetes offers powerful options to get more details about startup or runtime failures of pods as e.g. described in Application Introspection and Debugging or Debug Pods and Replication Controllers.\nIn order to identify pods with potential issus you could e.g. run kubectl get pods --all-namespaces | grep -iv Running to filter out the pods which are not in the state Running. One of frequent error state is CrashLoopBackOff, which tells that a pod crashes right after the start. Kubernetes then tries to restart the pod but again, but often the pod startup fails again.\nHere is a short list of possible reasons which might lead to a pod crash: 1. error during image pull caused by e.g. wrong/missing secrets or wrong/missing image 1. the app runs in an error state caused e.g. by missing environmental variables (ConfigMaps) or secrets 1. liveness probe failed 1. too high resource consumption (memory and/or CPU) or too strict quota settings 1. persistent volumes can\u0026rsquo;t be created/mounted 1. the container image is not updated\nBasically, the commands kubectl logs ... and kubectl describe ... with different parameters are used to get more detailed information. By calling e.g. kubectl logs --help you get more detailed information about the command and its parameters.\nIn the next sections you\u0026rsquo;ll find some basic approaches to get some ideas what went wrong.\nRemarks:\n- Even if the pods seem to be running as the status Running indicates, a high counter of the Restarts shows potential problems - There is as well an interactive Tutorial Troubleshooting with Kubectl available which explains basic debugging activities - The examples below are deployed into the namespace default. In case you want to change it use the optional parameter --namespace \u0026lt;your-namespace\u0026gt; to select the target namespace. They require Kubernetes release \u0026ge; 1.8.\nPrerequisites Your deployment was successful (no logical/syntactical errors in the manifest files), but the pod(s) aren\u0026rsquo;t running.\nError caused by wrong image name You run kubectl describe pod \u0026lt;your-pod\u0026gt; \u0026lt;your-namespace\u0026gt; to get detailed information about the pod startup.\nIn the Events section, you get an error message like Failed to pull image ... and Reason: Failed. The pod is in state ImagePullBackOff.\nThe example below is based on demo in Kubernetes documentation. In all examples the default namespace is used.\nFirst, cleanup with\nNext, create a resource based on the yaml content below ```yaml apiVersion: v1 kind: Pod metadata: name: termination-demo spec: containers: - name: termination-demo-container image: debiann command: [\u0026quot;/bin/sh\u0026quot;] args: [\u0026quot;-c\u0026quot;, \u0026quot;sleep 10 \u0026amp;\u0026amp; echo Sleep expired \u0026gt; /dev/termination-log\u0026quot;]  kubectl describe pod termination-demo lists in the Event section the content\nEvents: FirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage ---------\t--------\t-----\t----\t-------------\t--------\t------\t------- 2m\t2m\t1\tdefault-scheduler\tNormal\tScheduled\tSuccessfully assigned termination-demo to ip-10-250-17-112.eu-west-1.compute.internal 2m\t2m\t1\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tNormal\tSuccessfulMountVolume\tMountVolume.SetUp succeeded for volume \u0026quot;default-token-sgccm\u0026quot; 2m\t1m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tPulling\tpulling image \u0026quot;debiann\u0026quot; 2m\t1m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tWarning\tFailed\tFailed to pull image \u0026quot;debiann\u0026quot;: rpc error: code = Unknown desc = Error: image library/debiann:latest not found 2m\t54s\t10\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tWarning\tFailedSync\tError syncing pod 2m\t54s\t6\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tBackOff\tBack-off pulling image \u0026quot;debiann\u0026quot;  The error message with Reason: Failed tells that there is an error during pulling the image. A closer look at the image name indicates a misspelling.\nError in the app runs in an error state caused e.g. by missing environmental variables (ConfigMaps) or secrets This example illustrates the behavior in case of the app expects environment variables but the corresponding Kubernetes artifacts are missing.\nFirst, cleanup with\nkubectl delete deployment termination-demo kubectl delete configmaps app-env  Next, deploy this manifest\napiVersion: apps/v1beta2 kind: Deployment metadata: name: termination-demo labels: app: termination-demo spec: replicas: 1 selector: matchLabels: app: termination-demo template: metadata: labels: app: termination-demo spec: containers: - name: termination-demo-container image: debian command: [\u0026quot;/bin/sh\u0026quot;] args: [\u0026quot;-c\u0026quot;, \u0026quot;sed \\\u0026quot;s/foo/bar/\\\u0026quot; \u0026lt; $MYFILE\u0026quot;]  Now, the command kubectl get pods lists the pod termination-demo-xxx in the state Error or CrashLoopBackOff. The command kubectl describe pod termination-demo-xxx tells that there is no error during startup but gives no clue about what caused the crash.\nEvents: FirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage ---------\t--------\t-----\t----\t-------------\t--------\t------\t------- 19m\t19m\t1\tdefault-scheduler\tNormal\tScheduled\tSuccessfully assigned termination-demo-5fb484867d-xz2x9 to ip-10-250-17-112.eu-west-1.compute.internal 19m\t19m\t1\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tNormal\tSuccessfulMountVolume\tMountVolume.SetUp succeeded for volume \u0026quot;default-token-sgccm\u0026quot; 19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tPulling\tpulling image \u0026quot;debian\u0026quot; 19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tPulled\tSuccessfully pulled image \u0026quot;debian\u0026quot; 19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tCreated\tCreated container 19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tStarted\tStarted container 19m\t14m\t24\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tWarning\tBackOff\tBack-off restarting failed container 19m\t4m\t69\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tWarning\tFailedSync\tError syncing pod  The command kubectl get logs termination-demo-xxx gives access to the output, the application writes on stderr and stdout. In this case, you get an output like\n/bin/sh: 1: cannot open : No such file  So you need to have a closer look at the application. In this case the environmental variable MYFILEis missing. To fix this issue you could e.g. add a ConfigMap to your deployment as it is shown in the manifest listed below.\napiVersion: v1 kind: ConfigMap metadata: name: app-env data: MYFILE: \u0026quot;/etc/profile\u0026quot; --- apiVersion: apps/v1beta2 kind: Deployment metadata: name: termination-demo labels: app: termination-demo spec: replicas: 1 selector: matchLabels: app: termination-demo template: metadata: labels: app: termination-demo spec: containers: - name: termination-demo-container image: debian command: [\u0026quot;/bin/sh\u0026quot;] args: [\u0026quot;-c\u0026quot;, \u0026quot;sed \\\u0026quot;s/foo/bar/\\\u0026quot; \u0026lt; $MYFILE\u0026quot;] envFrom: - configMapRef: name: app-env  Note that once you fix the error and re-run the scenario, you might still see the pod in CrashLoopBackOff status. It is because the container finishes the command sed ... and run to completion. In order to keep the container in Running status, a long running task is required, e.g.\napiVersion: v1 kind: ConfigMap metadata: name: app-env data: MYFILE: \u0026quot;/etc/profile\u0026quot; SLEEP: \u0026quot;5\u0026quot; --- apiVersion: apps/v1beta2 kind: Deployment metadata: name: termination-demo labels: app: termination-demo spec: replicas: 1 selector: matchLabels: app: termination-demo template: metadata: labels: app: termination-demo spec: containers: - name: termination-demo-container image: debian command: [\u0026quot;/bin/sh\u0026quot;] # args: [\u0026quot;-c\u0026quot;, \u0026quot;sed \\\u0026quot;s/foo/bar/\\\u0026quot; \u0026lt; $MYFILE\u0026quot;] args: [\u0026quot;-c\u0026quot;, \u0026quot;while true; do sleep $SLEEP; echo sleeping; done;\u0026quot;] envFrom: - configMapRef: name: app-env  Too high resource consumption (memory and/or CPU) or too strict quota settings You can optionally specify the amount of memory and/or CPU your container gets during runtime. In case these settings are missing, the default requests settings are taken: CPU: 0m (in Milli CPU) and RAM: 0Gi which indicate no other limits than the ones of the node(s) itself. More details e.g. about to configure limits see e.g. Configure Default Memory Requests and Limits for a Namespace,\nIn case your application needs more resources, Kubernetes distinguishes between requests and limit settings: requests specify the guaranteed amount of resource, whereas limit tells Kubernetes the maximum amount of resource the container might need. Mathematically both settings could be described by the relation 0 \u0026lt;= requests \u0026lt;= limit. For both settings you need to consider the total amount of resources your nodes provide. For a detailed description of the concept see Resource Quality of Service in Kubernetes.\nUse kubectl describe nodes to get a first overview of the resource consumption in your cluster. Of special interest are the figures indicating the amount of CPU and Memory Requests at the bottom of the output.\nThe next example demonstrates what happens in case the CPU request is too high in order to be managed by your cluster.\nFirst, cleanup with\nkubectl delete deployment termination-demo kubectl delete configmaps app-env  Next, adapt the cpu below in the yaml below to be slightly higher than the remaining cpu resources in your cluster and deploy this manifest. In this example 600m (milli CPUs) are requested in a Kubernetes system with a single 2 Core worker node which results in an error message.\napiVersion: apps/v1beta2 kind: Deployment metadata: name: termination-demo labels: app: termination-demo spec: replicas: 1 selector: matchLabels: app: termination-demo template: metadata: labels: app: termination-demo spec: containers: - name: termination-demo-container image: debian command: [\u0026quot;/bin/sh\u0026quot;] args: [\u0026quot;-c\u0026quot;, \u0026quot;sleep 10 \u0026amp;\u0026amp; echo Sleep expired \u0026gt; /dev/termination-log\u0026quot;] resources: requests: cpu: \u0026quot;600m\u0026quot;  The command kubectl get pods lists the pod termination-demo-xxx in the state Pending. More details on why this happens could be found by using the command kubectl describe pod termination-demo-xxx:\n$ kubectl describe po termination-demo-fdb7bb7d9-mzvfw Name: termination-demo-fdb7bb7d9-mzvfw Namespace: default ... Containers: termination-demo-container: Image: debian Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; Command: /bin/sh Args: -c sleep 10 \u0026amp;\u0026amp; echo Sleep expired \u0026gt; /dev/termination-log Requests: cpu: 6 Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-t549m (ro) Conditions: Type Status PodScheduled False Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 9s (x7 over 40s) default-scheduler 0/2 nodes are available: 2 Insufficient cpu.  More details in - Managing Compute Resources for Containters - Resource Quality of Service in Kubernetes\nRemark:\n- This example works similarly when specifying a too high request for memory - In case you configured a autoscaler range when creating your Kubernetes cluster another worker node will be spinned up automatically if you didn\u0026rsquo;t reach the maximum number of worker nodes - In case of your app is running out of memory (the memory settings are too small), you typically find OOMKilled (Out Of Memory) message in the Events section fo the kubectl describe pod ... output\nThe container image is not updated You applied a fix in your app, created a new container image and pushed it into your container repository. After redeploying your Kubernetes manifests you expected to get the updated app, but still the same bug is in the new deployment present.\nThis behavior is related to how Kubernetes decides whether to pull a new docker image or to use the cached one.\nIn case you didn\u0026rsquo;t change the image tag, the default image policy IfNotPresent tells Kubernetes to use the cached image (see Images).\nAs a best practice you should not use the tag latest and change the image tag in case you changed anything in your image (see Configuration Best Practices).\nPlease have a look at this FAQ Container Image not updating for further details.\nLinks  Application Introspection and Debugging Debug Pods and Replication Controllers Logging Architecture Configure Default Memory Requests and Limits for a Namespace Managing Compute Resources for Containters Resource Quality of Service in Kubernetes Interactive Tutorial Troubleshooting with Kubectl Images Kubernetes Best Practises  "
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/howto/helm/",
	"title": "How to install Helm Chart in your shoot cluster",
	"tags": [],
	"description": "How to install Helm Charts in your shoot cluster",
	"content": " Basically, Helm Charts can be installed as described e.g. in the Helm QuickStart Guide. However, our clusters come with RBAC enabled by default and Helm must be installed as follows:\nCreate a Service Account Create a service account via the following command:\ncat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: v1 kind: ServiceAccount metadata: name: helm namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: helm roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: helm namespace: kube-system EOF  Initialise Helm Initialise Helm via helm init --service-account helm. You can now use helm.\nIn case of failure In case you have already executed helm init, but without the above service account, you will get the following error: Error: User \u0026quot;system:serviceaccount:kube-system:default\u0026quot; cannot list configmaps in the namespace \u0026quot;kube-system\u0026quot;. (get configmaps) (e.g. when you run helm list). You will now need to delete the Tiller deployment (Helm backend implicitly deployed to the Kubernetes cluster when you call helm init) as well as the local Helm files (usually $HELM_HOME is set to ~/.helm):\nkubectl delete deployment tiller-deploy --namespace=kube-system kubectl delete service tiller-deploy --namespace=kube-system rm -rf ~/.helm/  Now follow the instructions above. For more details see this Kubernetes Helm issue #2687.\n"
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/howto/",
	"title": "Howtos",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/howto/content_trust/",
	"title": "Integrity and Immutability",
	"tags": [],
	"description": "Ensure that you get always the right image",
	"content": " Introduction When transferring data among networked systems, trust is a central concern. In particular, when communicating over an untrusted medium such as the internet, it is critical to ensure the integrity and immutability of all the data a system operates on. Especially if you use Docker Engine to push and pull images (data) to a public registry.\nThis immutability offers me a guarantee that any and all containers that I instantiate will be absolutely identical at inception. Surprise surprise, deterministic operations.\nA Lesson in Deterministic Ops Docker Tags are about as reliable and disposable as this guy down here.\nSeems simple enough. You have probably already deployed hundreds of YAML\u0026rsquo;s or started endless count of Docker container.\ndocker run --name mynginx1 -P -d nginx:1.13.9  or\napiVersion: extensions/v1beta1 kind: Deployment metadata: name: rss-site spec: replicas: 1 template: metadata: labels: app: web spec: containers: - name: front-end image: nginx:1.13.9 ports: - containerPort: 80  But Tags are mutable and humans are prone to error. Not a good combination. Here we’ll dig into why the use of tags can be dangerous and how to deploy your containers across a pipeline and across environments, you guessed it, with determinism in mind.\nI want to ensure that whether it’s today or 5 years from now, that specific deployment uses the very same image that I defined. Any updates or newer versions of an image should be executed as a new deployment. The solution: digest\nA digest takes the place of the tag when pulling an image, for example, to pull the above image by digest, run the following command:\ndocker run --name mynginx1 -P -d nginx@sha256:4771d09578c7c6a65299e110b3ee1c0a2592f5ea2618d23e4ffe7a4cab1ce5de  You can now make sure that the same image is always loaded at every deployment. It doesn\u0026rsquo;t matter if the TAG of the image has been changed or not. This solves the problem of repeatability.\nContent Trust However, there’s an additionally hidden danger. It is possible for an attacker to replace a server image with another one infected with malware.\nDocker Content trust gives you the ability to verify both the integrity and the publisher of all the data received from a registry over any channel.\nPrior to version 1.8, Docker didn’t have a way to verify the authenticity of a server image. But in v1.8, a new feature called Docker Content Trust was introduced to automatically sign and verify the signature of a publisher.\nSo, as soon as a server image is downloaded, it is cross-checked with the signature of the publisher to see if it tampered in any way. This solves the problem of trust.\n"
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/howto/bash_kubeconfig/",
	"title": "KUBECONFIG context as bash prompt",
	"tags": [],
	"description": "Expose the active kubeconfig into the bash",
	"content": " Use the Kubernetes command-line tool, kubectl, to deploy and manage applications on Kubernetes. Using kubectl, you can inspect cluster resources; create, delete, and update components\nBy default, kubectl configuration is located at ~/.kube/config.\nSuppose you have two clusters, one for development work and one for scratch work.\nHow to handle this easily without copy the used configuration always to the right place?\nExport KUBECONFIG bash$ export KUBECONFIG=\u0026lt;PATH-TO-M\u0026gt;-CONFIG\u0026gt;/kubeconfig-dev.yaml  All of them above are well known and every Kubernetes developer knowns them well.\nHow to determine which cluster is used by the kubectl command?\nDetermine active cluster bash$ kubectl cluster-info Kubernetes master is running at https://api.dev.garden.shoot.canary.k8s-hana.ondemand.com KubeDNS is running at https://api.dev.garden.shoot.canary.k8s-hana.ondemand.com/api/v1/proxy/namespaces/kube-system/services/kube-dns To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. bash$  Display cluster in the bash - Linux and alike I found this tip on Stackoverflow and find it so good that it is worthwhile to write an HwoTo about it. Edit your ~/.bash_profile and add the code below to it.\nprompt_k8s(){ k8s_current_context=$(kubectl config current-context 2\u0026gt; /dev/null) if [[ $? -eq 0 ]] ; then echo -e \u0026quot;(${k8s_current_context}) \u0026quot;; fi } PS1+='$(prompt_k8s)'  After this your bash command prompt contains the active KUBECONFIG context and you always know which cluster is active - develop or production.\ne.g.\nbash$ bash$ bash$ bash$ bash$ export KUBECONFIG=/Users/d023280/Documents/workspace/gardener-ui/kubeconfig_gardendev.yaml bash (garden_dev)$  Now you have the (garden_dev) prefix in the bash command prompt.\nThis helps immensely to avoid thoughtless mistakes.\nDisplay cluster in the PowerShell - Windows Display current k8s cluster in the title of PowerShell window.\nCreate a profile file for your shell under %UserProfile%\\Documents\\Windows­PowerShell\\Microsoft.PowerShell_profile.ps1\nCopy following code to Microsoft.PowerShell_profile.ps1\nfunction prompt_k8s { $k8s_current_context = (kubectl config current-context) | Out-String if($?) { return $k8s_current_context }else { return \u0026quot;No K8S contenxt found\u0026quot; } } $host.ui.rawui.WindowTitle = prompt_k8s  If you want to switch to different cluster, you can set KUBECONFIG to new value, and re-run the file Microsoft.PowerShell_profile.ps1\n"
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/howto/antipattern/",
	"title": "Kubernetes Antipattern",
	"tags": [],
	"description": "Common Antipattern for Kubernetes and Docker",
	"content": " This HowTo will cover common kubernetes anti-patterns that we have seen over the past months.\nRunning as root user. Whenever possible, do not run containers as root users. One could be tempted to say that Kubernetes Pods and Node are well separated. The host and the container share the same kernel. If the container is compromised, a root user can damage the underlying node.\nCheck out the very good presentation of Liz Rice at the KubeCon 2018 \nUse RUN groupadd -r anygroup \u0026amp;\u0026amp; useradd -r -g anygroup myuser to create a group and a user in it. Use the USER command to switch to this user. Note that you can also consider providing an explicit UID/GID if required. Following is an example:\nARG GF_UID=\u0026quot;500\u0026quot; ARG GF_GID=\u0026quot;500\u0026quot; # add group \u0026amp; user RUN groupadd -r -g $GF_GID appgroup \u0026amp;\u0026amp; \\ useradd appuser -r -u $GF_UID -g appgroup USER appuser  Data or logs in containers Containers are ideal for stateless applications and should be transient. This means that no data or logs should be stored in the container, as they are lost when the container is closed. If absolutely necessary, you can use persistence volumes instead to persist them outside the containers. Using ELK stack could be another options for storing and processing log files.\nUsing the IP addresses of the pod Each pod is assigned an IP address. It is necessary for pods to communicate with each other to build an application, e.g. an application must communicate with a database. Existing pods are terminated and new pods are constantly started. If you rely on the IP address of the pod/container, the application configuration must be constantly updated. This makes the application fragile. Create services instead. These form a logical name that can be assigned independently of the growing and shrinking number of containers. Services are the basic concept for the load balancing within Kubernetes.\nMore than one process in a container A docker file provides a CMD and ENTRYPOINT to start the image. CMD is often used around a script that makes a configuration and then starts the container. Do not try to start multiple processes with this script. It is important to consider the separation of concerns when creating docker images. This makes managing your containers, collecting logs and updating each process all the more difficult. You can split the image into multiple containers and manage them independently - even in one pod. It should also be borne in mind that Kubernetes only monitors the process with PID=1. If more than one process is started within a container, then these no longer fall under the control of Kubernetes.\nImages from a running container A new image can be created with the docker commit command. This is useful if changes have been made to the container and you want to persist them for later error analysis. However, the images created with it are not reproducible and completely worthless for a CI/CD environment. Furthermore, another developer cannot recognize which components the image contains. Instead, always make changes to the docker file, close existing containers and start a new container with the updated image.\nPasswords in Docker Image 💀 Do not save passwords in the Docker file. They are in plain text and are checked into a repository. That makes them completely vulnerable even if you are using a private repository like the Artifactory. Always use Secrets or ConfigMaps to set up your deployment environment or mount passwords as persistent volumes.\nUsing the \u0026lsquo;latest\u0026rsquo; tag Starting an image with tomcat is tempting. If no tags are specified, a container is started with the tomcat:latest image. This image may no longer be up to date and refers to an older version instead. Running a production application requires complete control of the environment with exact versions of the image. Make sure you always use a tag or even better the sha256 hash of the image e.g. tomcat@sha256:c34ce3c1fcc0c7431e1392cc3abd0dfe2192ffea1898d5250f199d3ac8d8720f. Why use the sha256 hash? TAGs are volatile and can be overwritten by a developer at any time. In this case you don\u0026rsquo;t have complete control over your image - which is bad.\nDifferent images per environment Don\u0026rsquo;t create different images for development, testing, staging and production environments. The image should be the source of truth and should only be created once and pushed to the repository. This image:tag should be used for different environments in the future.\nStart order of pods Applications often depend on containers being started in a certain order. For example, a database container must be up and running before an application can connect to it. The application should be resilient to such changes, as the db pod can be unreachable or restarted at any time. The application container should be able to handle such situations without terminating or crash itself immediately. A good pattern here is to configure \u0026ldquo;readiness probe\u0026rdquo; accordingly and tell Kubernetes that the app pod is not operable at the moment. InitContainers can be used to control the start sequence if dependency exist among multiple pods\nAdditional anti-patterns and patterns\u0026hellip; In the community vast experience have been collected to improve stability and usability of Docker and Kubernetes. Instead of copy \u0026amp; past the content, please refer to following links in recognition of respective work. - Kubernetes Production Patterns\n"
},
{
	"uri": "https://gardener.github.io/website/010-blog/2018_week_06/",
	"title": "Kubernetes is available in Docker for Mac 17.12 CE",
	"tags": [],
	"description": "",
	"content": "    Kubernetes is only available in Docker for Mac 17.12 CE and higher, on the Edge channel. Kubernetes support is not included in Docker for Mac Stable releases. To find out more about Stable and Edge channels and how to switch between them, see General configuration.   \n Docker for Mac 17.12 CE (and higher) Edge includes a standalone Kubernetes server that runs on your Mac, so that you can test deploying your Docker workloads on Kubernetes.\nThe Kubernetes client command, kubectl, is included and configured to connect to the local Kubernetes server. If you have kubectl already installed and pointing to some other environment, such as minikube or a GKE cluster, be sure to change context so that kubectl is pointing to docker-for-desktop:\n\u0026hellip;see more on Docker.com\nI recommend to setup your shell to see which KUBECONFIG is active.\n"
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/howto/landscape-setup/",
	"title": "Landscape Setup",
	"tags": [],
	"description": "",
	"content": " Gardener Setup Scripts This is the installation manual for a simple Gardener setup. It is part of the landscape-setup-template project. You can find further information there.\nPrerequisites Before getting started make sure you have the following at hand:\n You need a cloud account with sufficient quota to set up a Kubernetes cluster with a couple of VMs. This project currently supports AWS and Openstack. A Linux machine (virtual machine is fine) or a Mac with basic tools such as a git client and the Docker runtime installed.  Gardener Installation Follow these steps to install Gardener. Do not proceed to the next step in case of errors.\nTLDR If you are already familiar with the installation procedure and just want a short summary of the commands you have to use, here it is:\n# setup git clone --recursive https://github.com/gardener/landscape-setup-template.git landscape cd landscape/setup ./docker_run.sh ./deploy_kubify.sh ./deploy_gardener.sh # optional: certmanager cd components ./deploy.sh certmanager # ------------------------------------------------------------------- # teardown cd /landscape k8s/bin/tf destroy -force setup/cleanup.sh  Step 1: Clone the Repositories and get Dependencies Get the landscape-setup-template from GitHub and initialize the submodules:\ngit clone --recursive https://github.com/gardener/landscape-setup-template.git landscape cd landscape  This repository will contain all passwords and keys for your landscape. You will be in trouble if you loose them so we recommend that you store this landscape configuration in a private repository. It might be a good idea to change the origin so you do not accidentally publish your secrets to the public template repository.\nStep 2: Configure the Landscape There is a landscape_config.yaml file in the landscape project. This is the only file that you need to modify - all other configuration files will be derived from this and the landscape_base.yaml file. The latter one contains the merging instructions as well as technical configurations and it shouldn\u0026rsquo;t be touched unless you know what you are doing.\nStep 3: Build and Run Docker Container First, you need to change into the setup folder:\ncd setup  Then run the container:\n./docker_run.sh  After this,\n you will be connected to the container via an interactive shell the landscape folder will be mounted in that container your current working directory will be setup folder setup/init.sh is sourced, meaning  the environment variables will be set kubectl will be configured to communicate with your cluster   The docker_run.sh script searches for the image locally and pulls it from an image repository, if it isn\u0026rsquo;t found. If pulling the image doesn\u0026rsquo;t work - which will probably be the case if the version in the setup/VERSION file doesn\u0026rsquo;t match a release version of the setup submodule - you can use the docker_build.sh script to build the image locally.\nMost of the scripts need a landscape.yaml file, that can be generated from a merge of landscape_base.yaml and landscape_config.yaml. The init.sh script (which is sourced in the docker_run.sh script, see above) will do that automatically, if it doesn\u0026rsquo;t already exist. In case you want to overwrite an existing landscape.yaml file or generate it manually, you can use this script:\n./build_landscape_yaml.sh  Step 4: Create a Kubernetes Cluster via Kubify You can use this script to run the cluster setup:\n./deploy_kubify.sh  The script will wait some time for the cluster to come up and then partly validate that the cluster is ready.\nIf you get errors during the cluster setup, just try to run the script again.\nOnce completed the following command should show all deployed pods:\nroot@c41327633d6d:/landscape# kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system etcd-operator-75dcfcf4f7-xkm4h 1/1 Running 0 6m kube-system heapster-c8fb4f746-tvts6 2/2 Running 0 2m kube-system kube-apiserver-hcdnc 1/1 Running 0 6m [...]  Step 4.5: Workaround (Automated) There is currently an issue with session affinities in Kubernetes, which can break your cluster. While the problem has been fixed (see https://github.com/kubernetes/kubernetes/commit/f2405cf2942739996af2bb76347c2cb0641153aa), the corresponding Kubernetes version is not yet included in this project.\nUntil that happens, the workaround is to remove the following lines from the kubernetes service:\n sessionAffinity: ClientIP sessionAffinityConfig: clientIP: timeoutSeconds: 10800  Kubernetes will add sessionAffinity: None on itself.\nThis will happen automatically at the end of the deploy_kubify.sh script.\nStep 5-9: Gardener Setup (Automated) Steps 5-9 are automated. In case you need more control follow the instructions below for manually running them.\n./deploy_gardener.sh  After successful completion, you can either continue with step 10 (optional), or start using the Gardener (see Accessing the Dashboard).\nAccessing the Dashboard After step 9 you will be able to access the Gardener dashboard. This example assumes that your cluster is located at mycluster.example.org - just replace that part with whatever you put in the clusters.dns.domain_name entry in the landscape_config.yaml file.\nThe print_dashboard_urls.sh script constructs both URLs from the domain name given in the landscape.yaml file and prints them. It is called automatically at the end of the Gardener deploy script.\nFirst, open https://identity.ingress.mycluster.example.org. Your browser will show a warning regarding untrusted self-signed certificates, you need to ignore that warning. You will then see a nearly blank page with some 404 message. If you skip this step, you will still be able to see the dashboard in the next step, but the login button probably won\u0026rsquo;t work.\nNow you can open the dashboard at https://dashboard.ingress.mycluster.example.org. Here you need to ignore a similar warning again, then you should see the dashboard. You can login using the options you have specified in the identity chart part of the landscape_config.yaml.\nStep 5-9: Gardener Setup (Manual) The commands shown below need to be run from within the components directory of the setup folder:\ncd /landscape/setup/components  Step 5: Generate Certificates These are the self-signed certificates used for the dashboard and identity ingresses (if you are on the internet you can later get letsencrypt issued certificates).\n./deploy.sh cert  Step 6: Deploy tiller Tiller is needed to deploy Helm charts in order to deploy Gardener and other needed components\n./deploy.sh helm-tiller  Step 7: Deploy Gardener Now we can deploy Gardener. If the previous steps were executed successfully this should be completed in a couple of seconds.\n./deploy.sh gardener  You might see a couple of messages like these:\nGardener API server not yet reachable. Waiting...  while the script waits for the Gardener to start. Once Gardener is up when the deployment script finished you can verify the correct setup by running the following command:\nkubectl get shoots No resources found.  As we do not have a seed cluster yet we cannot create any shoot clusters. The Gardener itself is installed in the garden namespace:\nkubectl get po -n garden NAME READY STATUS RESTARTS AGE gardener-apiserver-56cc665667-nvrjl 1/1 Running 0 6m gardener-controller-manager-5c9f8db55-hfcts 1/1 Running 0 6m  Step 8: Register Garden Cluster as Seed Cluster In heterogeneous productive environments one would run Gardener and seed in separate clusters but for simplicity and resource consumption reasons we will register the Gardener cluster that we have just created also as the seed cluster. Make sure that the seed_config in the landscape file is correct and matches the region that you are using. Keep in mind that image ids differ between regions as well.\n./deploy.sh seed-config  That\u0026rsquo;s it! If everything went fine you should now be able to create shoot clusters. You can start with a sample manifest and create a shoot cluster by standard Kubernetes means:\nkubectl apply -f shoot-aws.yaml  Step 9: Install Identity and Dashboard Creating clusters based on a shoot manifest is quite nice but also a little complex. While almost all aspects of a shoot cluster can be configured it can be quite difficult for beginners, so go on and install the dashboard:\n./deploy.sh identity [...] ./deploy.sh dashboard [...]  Now you should be able to open the \u0026ldquo;Gardener\u0026rdquo; dashboard and start creating shoot clusters.\nStep 10: Apply Valid Certificates (optional) Ensure that you are in the components directory for installing the certmanager:\ncd /landscape/setup/components  Using the Gardener Dashboard with self-signed certificates is awkward and some browsers even prevent you from accessing it altogether.\nThe following command will install the cert-manager and request valid letsencrypt certificates for both the identity and dashboard ingresses:\n./deploy.sh certmanager  After one to two minutes valid certificates should be installed.\nLetsencrypt limits how many certificates you can get for the same host within a short time. To avoid hitting these limits, you can use the letsencrypt staging server for testing, which has a significantly higher rate limit but produces untrusted certificates.\nTo use the staging server, change the URL in components/certmanager/cert-manager-issuer.yaml.tmpl to https://acme-staging-v02.api.letsencrypt.org/directory.\nTearing Down the Landscape Make sure that you delete all shoot clusters prior to tearing down the cluster created by Kubify (either by deleting them in the Gardener dashboard or by using the kubectl command). The following command should not return any shoot clusters:\nkubectl get shoots --all-namespaces No resources found.  There is a delete-shoot script in order to delete shoot clusters.\nNext run terraform in order to delete the cluster:\ncd /landscape k8s/bin/tf destroy [...] Plan: 0 to add, 0 to change, 170 to destroy. Do you really want to destroy? Terraform will destroy all your managed infrastructure, as shown above. There is no undo. Only 'yes' will be accepted to confirm. Enter a value:  Enter yes when you are sure that you want to delete the cluster.\nCleanup If you have created and destroyed a cluster and want to restart it, there are some files you have to delete to clean up the directory.\nATTENTION: Only do this if you are sure the cluster has been completely destroyed! Since this removes the terraform state, an automated deletion of resources won\u0026rsquo;t be possible anymore - you will have to clean up any leftovers manually.\nsetup/cleanup.sh  This will reset your landscape folder to its initial state.\nThe script takes an optional \u0026ldquo;-y\u0026rdquo; argument to skip the confirmation.\n"
},
{
	"uri": "https://gardener.github.io/website/components/mcm/",
	"title": "Machine Controller Manager",
	"tags": [],
	"description": "",
	"content": " machine-controller-manager \nMachine Controller Manager (MCM) manages VMs as another kubernetes custom resource. It provides a declarative way to manage VMs. The current implementation supports AWS, GCP, Azure and Openstack. It can easily extended to support other cloud providers as well.\nExample of managing machine:\nkubectl create/get/delete machine vm1  Key terminologies Nodes/Machines/VMs are different terminologies used to represent similar things. We use these terms in the following way\n VM: A virtual machine running on any cloud provider. Node: Native kubernetes node objects. The objects you get to see when you do a \u0026ldquo;kubectl get nodes\u0026rdquo;. Although nodes can be either physical/virtual machines, for the purposes of our discussions it refers to a VM. Machine: A VM that is provisioned/managed by the Machine Controller Manager.  Design of Machine Controller Manager See the design documentation in the /docs/design repository, please find the design doc here.\nTo start using or developing the Machine Controller Manager See the documentation in the /docs repository, please find the index here.\n"
},
{
	"uri": "https://gardener.github.io/website/045_contribute/20_documentation/25_markup/",
	"title": "Markdown",
	"tags": [],
	"description": "",
	"content": "Hugo uses Markdown for its simple content format. However, there are a lot of things that Markdown doesn’t support well. You could use pure HTML to expand possibilities.\nBut this happens to be a bad idea. Everyone uses Markdown because it\u0026rsquo;s pure and simple to read even non-rendered. You should avoid HTML to keep it as simple as possible.\nTo avoid this limitations, Hugo created shortcodes. A shortcode is a simple snippet inside a page.\nGardener provides multiple shortcodes on top of existing ones.\n Attachments  The Attachments shortcode displays a list of files attached to a page.\n Button  Nice buttons on your page.\n Expand  Displays an expandable/collapsible section of text on your page\n Mermaid  Generation of diagram and flowchart from text in a similar manner as markdown\n Notice  Disclaimers to help you structure your page\n "
},
{
	"uri": "https://gardener.github.io/website/045_contribute/20_documentation/25_markup/mermaid/",
	"title": "Mermaid",
	"tags": [],
	"description": "Generation of diagram and flowchart from text in a similar manner as markdown",
	"content": " Mermaid is a library helping you to generate diagram and flowcharts from text, in a similar manner as Markdown.\nJust insert your mermaid code in the mermaid shortcode and that\u0026rsquo;s it.\nFlowchart example {{\u0026lt;mermaid align=\u0026quot;left\u0026quot;\u0026gt;}} graph LR; A[Hard edge] --\u0026gt;|Link text| B(Round edge) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result one] C --\u0026gt;|Two| E[Result two] {{\u0026lt; /mermaid \u0026gt;}}  renders as\ngraph LR; A[Hard edge] --|Link text| B(Round edge) B -- C{Decision} C --|One| D[Result one] C --|Two| E[Result two]  Sequence example {{\u0026lt;mermaid\u0026gt;}} sequenceDiagram participant Alice participant Bob Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts \u0026lt;br/\u0026gt;prevail... John--\u0026gt;Alice: Great! John-\u0026gt;Bob: How about you? Bob--\u0026gt;John: Jolly good! {{\u0026lt; /mermaid \u0026gt;}}  renders as\nsequenceDiagram participant Alice participant Bob Alice-John: Hello John, how are you? loop Healthcheck John-John: Fight against hypochondria end Note right of John: Rational thoughts prevail... John--Alice: Great! John-Bob: How about you? Bob--John: Jolly good!  GANTT Example {{\u0026lt;mermaid\u0026gt;}} gantt dateFormat YYYY-MM-DD title Adding GANTT diagram functionality to mermaid section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d section Critical tasks Completed task in the critical line :crit, done, 2014-01-06,24h Implement parser and jison :crit, done, after des1, 2d Create tests for parser :crit, active, 3d Future task in critical line :crit, 5d Create tests for renderer :2d Add to mermaid :1d {{\u0026lt; /mermaid \u0026gt;}}  render as\ngantt dateFormat YYYY-MM-DD title Adding GANTT diagram functionality to mermaid section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d section Critical tasks Completed task in the critical line :crit, done, 2014-01-06,24h Implement parser and jison :crit, done, after des1, 2d Create tests for parser :crit, active, 3d Future task in critical line :crit, 5d Create tests for renderer :2d Add to mermaid :1d  "
},
{
	"uri": "https://gardener.github.io/website/010-blog/2018_week_09/",
	"title": "Namespace Isolation",
	"tags": [],
	"description": "",
	"content": "\u0026hellip;or DENY all traffic from other namespaces\nYou can configure a NetworkPolicy to deny all the traffic from other namespaces while allowing all the traffic coming from the same namespace the pod deployed to. There are many reasons why you may chose to employ Kubernetes network policies: - Isolate multi-tenant deployments - Regulatory compliance - Ensure containers assigned to different environments (e.g. dev/staging/prod) cannot interfere with one another\n..read on Namespace Isolation how to configure it.\n"
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/howto/network-isolation/",
	"title": "Namespace Isolation",
	"tags": [],
	"description": "",
	"content": " \u0026hellip;or DENY all traffic from other namespaces\nYou can configure a NetworkPolicy to deny all the traffic from other namespaces while allowing all the traffic coming from the same namespace the pod deployed to.\nThere are many reasons why you may chose to employ Kubernetes network policies: - Isolate multi-tenant deployments - Regulatory compliance - Ensure containers assigned to different environments (e.g. dev/staging/prod) cannot interfere with one another\nKubernetes network policies are application centric compared to infrastructure/network centric standard firewalls. There are no explicit CIDR or IP used for matching source or destination IP’s. Network policies build up on labels and selectors which are key concepts of Kubernetes that are used to organize (for e.g all DB tier pods of app) and select subsets of objects.\nExample We create two nginx HTTP-Server in two namespaces and block all traffic between the two namespaces. E.g. you are unable to get content from namespace1 if you are sitting in namespace2.\nSetup the namespaces # create two namespaces for test purpose kubectl create ns customer1 kubectl create ns customer2 # create a standard HTTP web server kubectl run nginx --image=nginx --replicas=1 --port=80 -n=customer1 kubectl run nginx --image=nginx --replicas=1 --port=80 -n=customer2 # expose the port 80 for external access kubectl expose deployment nginx --port=80 --type=NodePort -n=customer1 kubectl expose deployment nginx --port=80 --type=NodePort -n=customer2  Test without NP Create a pod with curl preinstalled inside the namespace customer1\n# create a \u0026quot;bash\u0026quot; pod in one namespace kubectl run -i --tty client --image=tutum/curl -n=customer1  try to curl the exposed nginx server to get the default index.html page. Execute this in the bash prompt of the pod created above.\n# get the index.html from the nginx of the namespace \u0026quot;customer1\u0026quot; =\u0026gt; success curl http://nginx.customer1 # get the index.html from the nginx of the namespace \u0026quot;customer2\u0026quot; =\u0026gt; success curl http://nginx.customer2  Both calls are done in a pod within namespace customer1 and both nginx servers are always reachable, no matter in what namespace.\nTest with NP Install the NetworkPolicy from your shell\nkind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: deny-from-other-namespaces spec: podSelector: matchLabels: ingress: - from: - podSelector: {}   it applies the policy to ALL pods in the named namespace as the spec.podSelector.matchLabels is empty and therefore selects all pods. it allows traffic from ALL pods in the named namespace, as spec.ingress.from.podSelector is empty and therefore selects all pods.  kubectl apply -f ./network-policy.yaml -n=customer1 kubectl apply -f ./network-policy.yaml -n=customer2  after this curl http://ngin.customer2 shouldn\u0026rsquo;t work anymore if you are a service inside the namespace customer1 and vice versa\nMore You can get more information how to configure the NetworkPolicies on:\n Calico WebSite Kubernetes NP Recipes  "
},
{
	"uri": "https://gardener.github.io/website/010-blog/2018_week_08_2/",
	"title": "Namespace Scope",
	"tags": [],
	"description": "",
	"content": "Should I use: ❌ one namespace per user/developer? ❌ one namespace per team? ❌ one per service type? ❌ one namespace per application type? 😄 one namespace per running instance of your application? \nApply the pattern of the Principle of Least Privilege\nAll user accounts at all times should run with as few privileges as possible, and also launch applications with as few privileges as possible. If you share a cluster for different user separated by a namespace, all user has access to all namespaces and services per default. It can happen that a user accidentally uses and destroys the namespace of a productive application or the namespace of another developer.\nKeep in mind: By default namespace does not provide: * Network isolation * Access Control * Audit Logging on user level\n"
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/howto/nload/",
	"title": "Network Monitor for Nodes",
	"tags": [],
	"description": "How to monitor the network traffic for a dedicated node",
	"content": " Intro nload is a linux tool which monitors network traffic and bandwidth usage in real time. It visualizes the in- and outgoing traffic on the terminal using two graphs and provides additional info like the total amount of transfered data and min/max network usage.\nWe generate docker container to monitor the traffic of a Node within your Kubernetes cluster\nWhy Why use such a simple tool when there are excellent monitoring solutions like Grafana or Prometheus out there? Sometimes it is necessary to do some fast debug sessions in a productive cluster to detect problems. It is not practicable to install and configure Grafana for this \u0026ldquo;just for a while\u0026rdquo; and then get used to it.\nSometimes it is the little helpers who can make life easier for you for the first time.\nBut in the end you are right: grafana or prometheus should be the tool of choice for monitoring your cluster.\n#!/bin/bash Install the DaemonSet into your cluster.\nkubectl create -f https://github.wdf.sap.corp/raw/d023280/kube-nload/master/yaml/deamonset.yaml  Get the Pods.\nRemember that, because of the DaemenSet, every node in Kubernetes has a nload pod running.\nbash$\u0026gt; kubectl get pods NAME READY STATUS RESTARTS AGE nload-cxdxj 1/1 Running 0 4s nload-gxzjc 1/1 Running 0 4s nload-m485v 1/1 Running 0 4s nload-tfpnw 1/1 Running 0 4s nload-x8bnj 1/1 Running 0 4s privileged-pod 1/1 Running 0 4h tts-server-79584868b6-d44ll 1/1 Running 0 3d tts-server-79584868b6-kqlsl 1/1 Running 1 3d tts-server-79584868b6-vqb4g 1/1 Running 0 3h wrk-64d6db6d85-gphv5 1/1 Running 0 9d wrk-64d6db6d85-rqht9 1/1 Running 0 9d wrk-64d6db6d85-szr8n 1/1 Running 0 9d  Get into a Pod.\nGet into any pod to monitor the traffic of the related Node! and not the Pod. The reason for this is that we use a privileged pod which has access to the host.\nbash$\u0026gt; kubectl exec -ti nload-cxdxj bash  and monitor the eth0 network device\n$\u0026gt; nload eth0  Recording Sources Feel free to clone the repo: https://github.wdf.sap.corp/d023280/kube-nload\n"
},
{
	"uri": "https://gardener.github.io/website/045_contribute/20_documentation/25_markup/notice/",
	"title": "Notice",
	"tags": [],
	"description": "Disclaimers to help you structure your page",
	"content": " The notice shortcode shows 4 types of disclaimers to help you structure your page.\nNote {{% notice note %}} A notice disclaimer {{% /notice %}}  renders as\nA notice disclaimer\n Info {{% notice info %}} An information disclaimer {{% /notice %}}  renders as\nAn information disclaimer\n Tip {{% notice tip %}} A tip disclaimer {{% /notice %}}  renders as\nA tip disclaimer\n Warning {{% notice warning %}} An warning disclaimer {{% /notice %}}  renders as\nA warning disclaimer\n "
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/howto/container-startup/",
	"title": "Orchestration of containers startup",
	"tags": [],
	"description": "How to orchestrate startup sequence of multiple containers",
	"content": " Disclaimer If application depends on certain services which are packaged and deployed through different containers, instead of relying on certain starting sequence of containers in order to run successfully, the application should be implemented in a way to handle unavailability of the depending services. The approaches described in the tutorial should be used in consultation with your architect.\nIntroduction Kubernetes offers a feature called InitContainers to perform some tasks during a pod\u0026rsquo;s initialization. In this tutorial we demonstrate how to use it orchestrate starting sequence of multiple containers. The tutorial uses the example app url-shortener which consists of two components:\n postgresql database webapp which depends on postgresql database and provides two endpoints: create a short url from a given location, and redirect from a given short URL to the corresponding target location.  This app represents the minimal example where an application relies on another service or database. In this example, if the application starts before database is ready, the application will fail as shown below:\n$ kubectl logs webapp-958cf5567-h247n time=\u0026quot;2018-06-12T11:02:42Z\u0026quot; level=info msg=\u0026quot;Connecting to Postgres database using: host=`postgres:5432` dbname=`url_shortener_db` username=`user`\\n\u0026quot; time=\u0026quot;2018-06-12T11:02:42Z\u0026quot; level=fatal msg=\u0026quot;failed to start: failed to open connection to database: dial tcp: lookup postgres on 100.64.0.10:53: no such host\\n\u0026quot; $ kubectl get po -w NAME READY STATUS RESTARTS AGE webapp-958cf5567-h247n 0/1 Pending 0 0s webapp-958cf5567-h247n 0/1 Pending 0 0s webapp-958cf5567-h247n 0/1 ContainerCreating 0 0s webapp-958cf5567-h247n 0/1 ContainerCreating 0 1s webapp-958cf5567-h247n 0/1 Error 0 2s webapp-958cf5567-h247n 0/1 Error 1 3s webapp-958cf5567-h247n 0/1 CrashLoopBackOff 1 4s webapp-958cf5567-h247n 0/1 Error 2 18s webapp-958cf5567-h247n 0/1 CrashLoopBackOff 2 29s webapp-958cf5567-h247n 0/1 Error 3 43s webapp-958cf5567-h247n 0/1 CrashLoopBackOff 3 56s  If the restartPolicy is set to Always (default) in yaml, the application will continue to restart the pod with an exponential back-off delay in case of failure.\nUsing InitContaniner To avoid such situation, InitContainers can be defined which are executed prior to the application container. If one InitContainers fails, the application container won\u0026rsquo;t be triggered.\napiVersion: apps/v1beta1 kind: Deployment metadata: name: webapp spec: template: metadata: labels: app: webapp spec: initContainers: # check if DB is ready, and only continue when true - name: check-db-ready image: postgres:9.6.5 command: ['sh', '-c', 'until pg_isready -h postgres -p 5432; do echo waiting for database; sleep 2; done;'] containers: - image: xcoulon/go-url-shortener:0.1.0 name: go-url-shortener env: - name: POSTGRES_HOST value: postgres - name: POSTGRES_PORT value: \u0026quot;5432\u0026quot; - name: POSTGRES_DATABASE value: url_shortener_db - name: POSTGRES_USER value: user - name: POSTGRES_PASSWORD value: mysecretpassword ports: - containerPort: 8080  In above example, the InitContainers uses docker image postgres:9.6.5 which is different from the application container. This also brings the advantage of not having to include unnecessary tools (e.g. pg_isready) in the application container.\nWith introduction of InitContainers, the pod startup will look like following in case database is not available yet:\n$ kubectl get po -w NAME READY STATUS RESTARTS AGE nginx-deployment-5cc79d6bfd-t9n8h 1/1 Running 0 5d privileged-pod 1/1 Running 0 4d webapp-fdcb49cbc-4gs4n 0/1 Pending 0 0s webapp-fdcb49cbc-4gs4n 0/1 Pending 0 0s webapp-fdcb49cbc-4gs4n 0/1 Init:0/1 0 0s webapp-fdcb49cbc-4gs4n 0/1 Init:0/1 0 1s $ kubectl logs webapp-fdcb49cbc-4gs4n Error from server (BadRequest): container \u0026quot;go-url-shortener\u0026quot; in pod \u0026quot;webapp-fdcb49cbc-4gs4n\u0026quot; is waiting to start: PodInitializing  To try out yourself, the complete deployment definition can be found here.\n"
},
{
	"uri": "https://gardener.github.io/website/045_contribute/20_documentation/10_organisation/",
	"title": "Organisation",
	"tags": [],
	"description": "",
	"content": " Content Organisation This site uses Hugo. In Hugo, content organization is a core concept.\nHugo Tip: Start Hugo with hugo server --navigateToChanged for content edit-sessions.\n Page Lists Page Order The documentation side menu, the documentation page browser etc. are listed using Hugo\u0026rsquo;s default sort order, which sorts by weight (from 1), date (newest first) and finally by the link title.\nGiven that, if you want to move a page or a section up, set a weight in the page\u0026rsquo;s front matter:\ntitle: My Page weight: 10  For page weights, it can be smart not to use 1, 2, 3 \u0026hellip;, but some other interval, say 10, 20, 30\u0026hellip; This allows you to insert pages where you want later.\n "
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/howto/service-cache-control/",
	"title": "Out-Dated HTML and JS files delivered",
	"tags": [],
	"description": "My Content is always out-dated - why?",
	"content": " Problem After updating your HTML and JS sources in your web application, the kubernetes cluster delivers outdated versions - why?\nPreamble By default, the Kubernetes service pods are not accessible over the external network, but only from other pods within the Kubernetes cluster.\nThe Gardener cluster has a built-in configuration for HTTP load balancing called Ingress, the rules for external connectivity to Kubernetes services. Users who want external access to their Kubernetes services create an ingress resource that defines rules, including the URI path, backing service name, and other information. The Ingress controller can then automatically program a frontend load balancer to enable Ingress configuration.\nExample Ingress Configuration apiVersion: extensions/v1beta1 kind: Ingress metadata: name: vuejs-ingress spec: rules: - host: test.ingress.\u0026lt;GARDENER-CLUSTER\u0026gt;.\u0026lt;GARDENER-PROJECT\u0026gt;.shoot.canary.k8s-hana.ondemand.com http: paths: - backend: serviceName: vuejs-svc servicePort: 8080  where: - \u0026lt;GARDENER-CLUSTER\u0026gt;: The cluster name in the gardener - \u0026lt;GARDENER-PROJECT\u0026gt;: You project name in the gardener\nThat\u0026rsquo;s the crux of the matter. The ingress controller we are using is NGINX.\n NGINX is a software load balancer, web server, and content cache built on top of open source NGINX. NGINX Plus has exclusive production‑ready features on top of what\u0026rsquo;s available in the open source offering, including session persistence, configuration via API, and active health checks. Use NGINX Plus instead of your hardware load balancer and get the freedom to innovate without being constrained by infrastructure.\n NGINX caches the content as specified in the HTTP header. If the HTTP header is missing, it is assumed that the cache is forever and NGINX never updates the content in the stupidest case.\nSolution In general you can avoid this pitfall with one of the solutions below: - use a cache buster + HTTP-Cache-Control(prefered) - use HTTP-Cache-Control with a lower timeframe - disable the caching in the ingress (just for dev purpose)\nPlease use your preferred search engine to get advice how to set the HTTP Header or setup a cache buster for your web framework (e.g. Express/NodeJS, SpringBoot,\u0026hellip;)\nBelow is an example how to disable the cache control for your ingress done with an annotation in your ingress YAML (just for dev purpose).\n--- apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: ingress.kubernetes.io/cache-enable: \u0026quot;false\u0026quot; name: vuejs-ingress spec: rules: - host: test.ingress.\u0026lt;GARDENER-CLUSTER\u0026gt;.\u0026lt;GARDENER-PROJECT\u0026gt;.shoot.canary.k8s-hana.ondemand.com http: paths: - backend: serviceName: vuejs-svc servicePort: 8080  "
},
{
	"uri": "https://gardener.github.io/website/045_contribute/10_code/37_process/",
	"title": "Process",
	"tags": [],
	"description": "",
	"content": " Creating a new Feature If you want to contribute to the Gardener, please do that always on a dedicated branch on your own fork named after the purpose of the code changes, for example feature/helm-integration. Please do not forget to rebase your branch regularly.\n If you have finished your work, please create a pull request based on master. It will be reviewed and merged if no further changes are requested from you. :warning: Please ensure that your modifications pass the lint checks, formatting checks, static code checks, and unit tests by executing  :rotating_light: Please run ./hack/generate-code whenever you modify the any API within pkg/apis.\n$ make verify  Please do not file your pull request unless you receive a successful response from here!\nCreating a new Release or a Hotfix Please refer to the Gardener contributor guide.\n"
},
{
	"uri": "https://gardener.github.io/website/010-blog/2018_week_27/",
	"title": "ReadWriteMany - Dynamically Provisioned PV’s Using Amazon EFS",
	"tags": [],
	"description": "",
	"content": " The efs-provisioner allows you to mount EFS storage as PersistentVolumes in kubernetes. It consists of a container that has access to an AWS EFS resource. The container reads a configmap which contains the EFS filesystem ID, the AWS region and the name you want to use for your efs-provisioner. This name will be used later when you create a storage class.\nWhy EFS  One app, multiple nodes, same file(s) When you have an application that requires multiple virtual machines to access the same file system at the same time, AWS EFS is a tool that you can use. EFS support encryption. EFS is SSD based storage and its storage capacity and pricing will scale in or out as needed, so there is no need for the system administrator to do additional operations. It can grow to a petabyte scale. EFS now supports NFSv4 lock upgrading and downgrading, so yes, you can use sqlite with EFS… even if it was possible to do it before. Easy to setup  Why Not EFS  Sometimes when you think about using a service like EFS, you may also think about Cloud Lock and its negative sides Making an EFS backup may decrease your production FS performance; the throughput used by backup counts towards your total file system throughput. EFS is expensive compared to EBS (probably 2x more EBS pricing) EFS is not the magical solution for all your distributed FS problems, it can be slow in many cases. Test, benchmark and measure to ensure your if EFS is a good solution for your use case. EFS distributed architecture results in a latency overhead for each file read/write operation. If you have the possibility to use a CDN, don’t use EFS, keep just files that can not be stored in a CDN. It is evident to say, but don’t use EFS as a caching system, sometimes you could be doing this unintentionally. Last but not least, even if EFS is a fully managed NFS, you will face performance problems in many cases, resolving this could take some time and needs some efforts, so brace yourself.   ..read some more on ReadWriteMany.\n"
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/app/read-write-many/",
	"title": "ReadWriteMany with AWS",
	"tags": [],
	"description": "Dynamically Provisioned PV’s Using Amazon EFS",
	"content": " ReadWriteMany with PersistenceVolumes The efs-provisioner allows you to mount EFS storage as PersistentVolumes in kubernetes. It consists of a container that has access to an AWS EFS resource. The container reads a configmap which contains the EFS filesystem ID, the AWS region and the name you want to use for your efs-provisioner. This name will be used later when you create a storage class.\n Warning: Dynamically Provisioned PersistentVolumes have a default persistentVolumeReclaimPolicy of DELETE, which means if the PersistentVolumeClaim ever gets unbound from the PV, it will delete all the data. If you want to change this you need to change the persistentVolumeReclaimPolicy of the created PV’s to “Retain”.\n Before introducing how to use AWS EFS for Kubernetes, let me recap some terms of Persistent Volumes.\n PV (Persistent Volume): PV is a piece of storage, it can be NFS, iSCSI, EBS, EFS… The purpose of having PV is to decouple the storage from pod’s lifecycle. PVC (Persistent Volume Claim): PVC provides the method for pods to use PV, it includes the request storage size and access mode. If the PV supports ReadWriteMany access mode, then its PVC can be used by multiple pods. Storage Class: This is the abstract layer of PV which hides the implementation of PV from end users.  Why EFS  One app, multiple nodes, same file(s) When you have an application that requires multiple virtual machines to access the same file system at the same time, AWS EFS is a tool that you can use. EFS support encryption. EFS is SSD based storage and its storage capacity and pricing will scale in or out as needed, so there is no need for the system administrator to do additional operations. It can grow to a petabyte scale. EFS now supports NFSv4 lock upgrading and downgrading, so yes, you can use sqlite with EFS… even if it was possible to do it before. Easy to setup  Why Not EFS  Sometimes when you think about using a service like EFS, you may also think about Cloud Lock and its negative sides Making an EFS backup may decrease your production FS performance; the throughput used by backup counts towards your total file system throughput. EFS is expensive compared to EBS (probably 2x more EBS pricing) EFS is not the magical solution for all your distributed FS problems, it can be slow in many cases. Test, benchmark and measure to ensure your if EFS is a good solution for your use case. EFS distributed architecture results in a latency overhead for each file read/write operation. If you have the possibility to use a CDN, don’t use EFS, keep just files that can not be stored in a CDN. It is evident to say, but don’t use EFS as a caching system, sometimes you could be doing this unintentionally. Last but not least, even if EFS is a fully managed NFS, you will face performance problems in many cases, resolving this could take some time and needs some efforts, so brace yourself.   Design With the above points in your mind, let us have a look the architecture.\nSetup Basically, there are four steps:\n Prepare your Gardener cluster and deploy the kube2iam addon Create EFS (only the first time): It includes the tasks: create the EFS in the right subnets, setup the security groups to allow Kubernetes nodes to access and enable DNS support/resolution in your VPC. Create StorageClass for EFS via efs-provisioner (only the first time). efs-provisioner runs as a container which plays the role of EFS broker. It allows other pods to mount EFS as the persistent volumes. Just be aware of that EFS is built on top of NFS4, so you need to have nfs-common packages installed in your Kubernetes nodes Create a PVC to use the StorageClass for EFS. Just note that EFS has unlimited storage, so the storage size request actually does not take any effects here. But you still have to keep it to pass the syntax check.  Prepare your K8S cluster Edit the garden cluster shoot manifest (shown below) and paste the highlighted AWS IAM role shoot.yaml snippet at the right place in your shoot YAML. This updates your cluster and creates the correct IAM role on AWS for you.\nreconciling the AWS IAM role can take some minutes\u0026hellip;.\nCreate a EFS Storage After editing the Gardener cluster YAML you are ready to create the EFS storage. - Select EFS Service - Create an EFS Storage (shown in the screencast below) - Select the right VPC - Add the new create IAM role to the EFS permission - Copy the access path or EFS id\nConfigure the Provisioner Update the configmap with the File system ID and Amazon EC2 region of the EFS file system you wish to provision NFS PVs from.\napiVersion: v1 data: aws.region: \u0026lt;EFS-REGION\u0026gt; file.system.id: \u0026lt;EFS-ID\u0026gt; provisioner.name: example.com/aws-efs kind: ConfigMap metadata: name: efs-provisioner  Authorization the Provisioner Your cluster has RBAC enabled per default and you must authorize the provisioner. If you are in a namespace/project other than default either edit efs-rbac.yaml or edit the policy command accordingly.\nDeploy the Provisioner Update the provisioner and add the correct EFS endpoint URL at the end of the file.\n. . volumes: - name: pv-volume nfs: \u0026lt;EFS-ID\u0026gt;.efs.\u0026lt;EFS-REGION\u0026gt;.amazonaws.com path: / . .  Once you have finished configuring the class to have the name you chose when deploying the provisioner and the parameters you want, you can use it.\nAll of the configurations must be done just once. After you have deployed the provisioner to your cluster you can create your PVC and bind them to the PODs\nAll of them are covered in the ./yaml/xxxx.yaml files and can be adapted to your needs. Additional a small demo application is part of this repository to cover the usage of the EFS storage.\n"
},
{
	"uri": "https://gardener.github.io/website/045_contribute/10_code/40_repositories/",
	"title": "Repositories",
	"tags": [],
	"description": "",
	"content": " Repositories of required components The Gardener deploys a lot of helping components which are required to make the Shoot cluster functional. Please find a list of those repositories below.\nRepository list  Machine Controller Manager - a component which manages VMs/nodes as part of declarative custom resources inside Kubernetes. VPN - a set of components which establish connectivity from a pod running in the Seed cluster to the networks of a Shoot cluster (which are usually private). Terraformer - a can execute Terraform configuration and is designed to run as a pod inside a Kubernetes cluster. AWS Load Balancer Readvertiser - a component which is used to keep AWS Shoot cluster API servers reachable. Ingress Default Backend - a component which serves a static HTML page that is shown for all incoming requests to nginx that are not controlled by an Ingress object.  "
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/howto/ssh-into-node/",
	"title": "SSH into my worker nodes",
	"tags": [],
	"description": "SSH into my worker nodes",
	"content": " Term clarification We are talking about SSH into a node, and not open a shell in an existing pod or rather container. For ways to SSH into conatiner please check official kubernetes tutorial\nWhy When we hear this question, we return with another question: \u0026ldquo;Why would you need to?\u0026rdquo;. The background of this question is that all VMs are ephemeral (cattle, no pets). Any machine can be terminated any time. When updating a cluster, this will even happen to all machines (one by one). Anyway, sometimes curiosity is the driving factor and in this case, that\u0026rsquo;s a good thing.\nHow We plan to implement bastion-on-demand/web-console eventually, but it is not yet available. The next best alternative is to create a pod with elevated permissions by doing the following:\ncreate a new file privileged-pod.yaml with the content below\napiVersion: v1 kind: Pod metadata: name: privileged-pod namespace: default spec: containers: - name: busybox image: busybox resources: limits: cpu: 200m memory: 100Mi requests: cpu: 100m memory: 50Mi stdin: true securityContext: privileged: true volumeMounts: - name: host-root-volume mountPath: /host readOnly: true volumes: - name: host-root-volume hostPath: path: / hostNetwork: true hostPID: true restartPolicy: Never  kubectl create -f privileged-pod.yaml  Now you can look around in the pod:\nkubectl exec -ti privileged-pod sh ps aux ip a ls -la /host  Run as root using node\u0026rsquo;s file system instead of container\u0026rsquo;s:\nchroot /host/  Then you can run commands such as docker ps\nDon\u0026rsquo;t forget to delete your pod afterwards:\nkubectl delete pod privileged-pod  "
},
{
	"uri": "https://gardener.github.io/website/010-blog/2018_week_10/",
	"title": "Shared storage with S3 backend",
	"tags": [],
	"description": "",
	"content": "The storage is definitely the most complex and important part of an application setup, once this part is completed, 80% of the tasks are completed.\nMounting an S3 bucket into a pod using FUSE allows you to access the data as if it were on the local disk. The mount is a pointer to an S3 location, so the data is never synced locally. Once mounted, any pod can read or even write from that directory without the need for explicit keys.\nHowever, it can be used to import and parse large amounts of data into a database.\n..read on Shared S3 Storage how to configure it.\n"
},
{
	"uri": "https://gardener.github.io/website/045_contribute/20_documentation/20_style/",
	"title": "Style Guide",
	"tags": [],
	"description": "",
	"content": " This page gives writing style guidelines for the Gardener documentation. These are guidelines, not rules. Use your best judgment, and feel free to propose changes to this document in a pull request.\nLanguage Gardener documentation uses US English.\nDocumentation formatting standards Use camel case for API objects When you refer to an API object, use the same uppercase and lowercase letters that are used in the actual object name. Typically, the names of API objects use camel case.\nDon\u0026rsquo;t split the API object name into separate words. For example, use PodTemplateList, not Pod Template List.\nRefer to API objects without saying \u0026ldquo;object,\u0026rdquo; unless omitting \u0026ldquo;object\u0026rdquo; leads to an awkward construction.\n DoDon't The Pod has two containers.The pod has two containers. The Deployment is responsible for ...The Deployment object is responsible for ... A PodList is a list of Pods.A Pod List is a list of pods. The two ContainerPorts ...The two ContainerPort objects ... The two ContainerStateTerminated objects ...The two ContainerStateTerminateds ...  Use angle brackets for placeholders Use angle brackets for placeholders. Tell the reader what a placeholder represents.\n Display information about a pod:\nkubectl describe pod \nwhere \u0026lt;pod-name\u0026gt; is the name of one of your pods.\n  Use bold for user interface elements  DoDon't Click Fork.Click \"Fork\". Select Other.Select 'Other'.  Use italics to define or introduce new terms  DoDon't A cluster is a set of nodes ...A \"cluster\" is a set of nodes ... These components form the control plane.These components form the control plane.  Use code style for filenames, directories, and paths  DoDon't Open the envars.yaml file.Open the envars.yaml file. Go to the /docs/tutorials directory.Go to the /docs/tutorials directory. Open the /_data/concepts.yaml file.Open the /_data/concepts.yaml file.  Use the international standard for punctuation inside quotes  DoDon't events are recorded with an associated \"stage\".events are recorded with an associated \"stage.\" The copy is called a \"fork\".The copy is called a \"fork.\"  Inline code formatting Use code style for inline code and commands For inline code in an HTML document, use the \u0026lt;code\u0026gt; tag. In a Markdown document, use the backtick (`).\n DoDon't The kubectl run command creates a Deployment.The \"kubectl run\" command creates a Deployment. For declarative management, use kubectl apply.For declarative management, use \"kubectl apply\".  Use code style for object field names  DoDon't Set the value of the replicas field in the configuration file.Set the value of the \"replicas\" field in the configuration file. The value of the exec field is an ExecAction object.The value of the \"exec\" field is an ExecAction object.  Use normal style for string and integer field values For field values of type string or integer, use normal style without quotation marks.\n DoDon't Set the value of imagePullPolicy to Always.Set the value of imagePullPolicy to \"Always\". Set the value of image to nginx:1.8.Set the value of image to nginx:1.8. Set the value of the replicas field to 2.Set the value of the replicas field to 2.  Code snippet formatting Don\u0026rsquo;t include the command prompt  DoDon't kubectl get pods$ kubectl get pods  Separate commands from output Verify that the pod is running on your chosen node:\nkubectl get pods --output=wide  The output is similar to this:\nNAME READY STATUS RESTARTS AGE IP NODE nginx 1/1 Running 0 13s 10.200.0.4 worker0  Versioning Kubernetes examples Code examples and configuration examples that include version information should be consistent with the accompanying text. Identify the Kubernetes version in the Before you begin section.\nTo specify the Kubernetes version for a task or tutorial page, include min-kubernetes-server-version in the front matter of the page.\nIf the example YAML is in a standalone file, find and review the topics that include it as a reference. Verify that any topics using the standalone YAML have the appropriate version information defined. If a stand-alone YAML file is not referenced from any topics, consider deleting it instead of updating it.\nFor example, if you are writing a tutorial that is relevant to Kubernetes version 1.8, the front-matter of your markdown file should look something like:\n--- title: \u0026lt;your tutorial title here\u0026gt; min-kubernetes-server-version: v1.8 ---  In code and configuration examples, do not include comments about alternative versions. Be careful to not include incorrect statements in your examples as comments, such as:\napiVersion: v1 # earlier versions use... kind: Pod ...  "
},
{
	"uri": "https://gardener.github.io/website/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.github.io/website/tutorials/",
	"title": "Tutorials",
	"tags": [],
	"description": "",
	"content": "Initial Consideration There is a big difference between installing Kubernetes and using Kubernetes as a developer     Administrator The admin section is for anyone setup or administering a Gardener Landscape. It assumes some familiarity with concepts of IaaS   Developer You don’t have to understand all the internals of Kubernetes; however, basic knowledge of the architecture is helpful for understanding how to deploy and debug your applications. In this section we offer the best pratices in the context of gardener and service/application developement.      "
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/howto/prometheus/",
	"title": "Using Prometheus and Grafana to monitor K8s",
	"tags": [],
	"description": "How to deploy and configure Prometheus and Grafana to scrape and monitor kubelet container metrics",
	"content": " Disclaimer This post is meant to give a basic end-to-end description for deploying and using Prometheus and Grafana. Both applications offer a wide range flexibility which needs to be considered in case you have specific requirenments. Such details are not in the scope of this post.\nIntroduction Prometheus is an open-source systems monitoring and alerting toolkit for recording any purely numeric time series. It fits both machine-centric monitoring as well as monitoring of highly dynamic service-oriented architectures. In a world of microservices, its support for multi-dimensional data collection and querying is a particular strength. Prometheus was accepted as the second hosted project of the Cloud Native Computing Foundation, Kubernetes is the first. It offers a tight integration into the architecture of Kubernetes which makes the monitoring very easy.\nThese main characteristics makes Prometheus a good match for monitoring Kubernetes cluster: - Pull-based monitoring\nPrometheus is a pull-based monitoring system, which means that the Prometheus server dynamically discover and pull metrics from your services, e.g. apps running in Kubernetes.\n Labels\nPrometheus and Kubernetes share the same labels (key-value) concept that can be used to select objects in the system.\nPrometheus uses these labels to identify time series and can use sets of label matchers in the query language (PromQL) to select the time series to aggregate over.\n Exporters\nThere are a lot of exporters available which allow the integration of e.g. databases or even other monitoring systems in case these components do not offer already a prometheus client API. One prominent exporter is the so called node-exporter, which allows to monitor hardware and OS related metrics of Unix systems.\n Powerful query language\nPrometheus own query language PromQL lets the user select and aggregate time series data in real time. The result of an expression can either be shown as a graph, viewed as tabular data in Prometheus\u0026rsquo;s expression browser, or consumed by external systems via the HTTP API.\n  Query examples are listed e.g. on Prometheus Query Examples.\nOne very popular open-source visualization tool not only for Prometheus is Grafana. Grafana is an metric analytics and visualization suite. It is most commonly used for visualizing time series data for infrastructure and application analytics but many use it in other domains including industrial sensors, home automation, weather, and process control [see Grafana Documentation].\nGrafana accesses the data via Data Sources. The continuously growing list of supported back ends includes as well Prometheus.\nNext, a Dashboard is created using different kind of visualization building blocks (panels), e.g. Graph and Dashlist.\nThis post is about to describe an End-To-End scenario including the deployment of Prometheus and basic configurations in a K8s cluster as it is provided by the Gardener.\nWhen accessing the Prometheus UI via its service URL https://\u0026lt;your K8s FQN\u0026gt;/api/v1/namespaces/\u0026lt;your-prometheus-namespace\u0026gt;/services/prometheus-prometheus-server:80/proxy an isue (as of end of November 2017) with relative URLs generated by Prometheus leads to missing elements on the web page. As a workaround run kubectl port-forward -n \u0026lt;your-prometheus-namespace\u0026gt; \u0026lt;prometheus-pod\u0026gt; 9090:9090 on your client and access the Prometheus UI from there with your locally installed web browser. This issue is not relevant in case you use the service type LoadBalancer.\nPreparation The deployment of Prometheus and Grafana are based on Helm charts.\nBefore deploying the Helm charts make sure that the Helm settings described in this HowTo are implemented.\nThe K8s clusters provided by the Gardener use the Kubernetes feature role based authorization (RBAC). To authorize Prometheus\u0026rsquo;s node-exporter to access hardware and OS relevant metrics of your clusters worker nodes specific artifacts need to be deployed.\nBind the prometheus service account to the sapcloud:monitoring:prometheus cluster role by running the command kubectl apply -f crbinding.yaml.\ncrbinding.yaml\napiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: \u0026lt;your-prometheus-name\u0026gt; roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: sapcloud:monitoring:prometheus subjects: - kind: ServiceAccount name: prometheus-prometheus-server # Given by Helm chart namespace: \u0026lt;your-prometheus-namespace\u0026gt;  Deployment of Prometheus and Grafana Only minor changes are needed to deploy Prometheus and Grafana based on Helm charts.\nPaste the following yaml content into e.g. a file called values.yaml and deploy Prometheus via helm install --name \u0026lt;your-prometheus-name\u0026gt; --namespace \u0026lt;your-prometheus-namespace\u0026gt; stable/prometheus -f values.yaml\nTypically, Prometheus and Grafana are deployed in the same namespace. There is no technical reason behind this so feel free to choose different namespaces.\nvalues.yaml for Prometheus:\nrbac: create: true # K8s Shoot clusters run RBAC enabled nodeExporter: enabled: false # The node-exporter is already deployed per default serverFiles: prometheus.yml: global: scrape_interval: 30s scrape_timeout: 30s rule_files: - /etc/config/rules - /etc/config/alerts scrape_configs: - job_name: 'kube-kubelet' honor_labels: false scheme: https tls_config: # This is needed because the kubelets' certificates are not are generated # for a specific pod IP insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - target_label: __metrics_path__ replacement: /metrics - source_labels: [__meta_kubernetes_node_address_InternalIP] target_label: instance - action: labelmap regex: __meta_kubernetes_node_label_(.+) # Important! Enable this only if your cluster is \u0026gt;= 1.7.3. Otherwise your can remove it. - job_name: 'kube-kubelet-cadvisor' honor_labels: false scheme: https tls_config: # This is needed because the kubelets' certificates are not are generated # for a specific pod IP insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - target_label: __metrics_path__ replacement: /metrics/cadvisor - source_labels: [__meta_kubernetes_node_address_InternalIP] target_label: instance - action: labelmap regex: __meta_kubernetes_node_label_(.+) # Example scrape config for probing services via the Blackbox Exporter. # # The relabeling allows the actual service scrape endpoint to be configured # via the following annotations: # # * `prometheus.io/probe`: Only probe services that have a value of `true` - job_name: 'kubernetes-services' metrics_path: /probe params: module: [http_2xx] kubernetes_sd_configs: - role: service relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__address__] target_label: __param_target - target_label: __address__ replacement: blackbox - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] target_label: kubernetes_name # Example scrape config for pods # # The relabeling allows the actual pod scrape endpoint to be configured via the # following annotations: # # * `prometheus.io/scrape`: Only scrape pods that have a value of `true` # * `prometheus.io/path`: If the metrics path is not `/metrics` override this. # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`. - job_name: 'kubernetes-pods' kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: (.+):(?:\\d+);(\\d+) replacement: ${1}:${2} target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name # Scrape config for service endpoints. # # The relabeling allows the actual service scrape endpoint to be configured # via the following annotations: # # * `prometheus.io/scrape`: Only scrape services that have a value of `true` # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need # to set this to `https` \u0026amp; most likely set the `tls_config` of the scrape config. # * `prometheus.io/path`: If the metrics path is not `/metrics` override this. # * `prometheus.io/port`: If the metrics are exposed on a different port to the # service then set this appropriately. - job_name: 'kubernetes-service-endpoints' kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: (.+)(?::\\d+);(\\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name # Add your additional configuration here...  Next, deploy Grafana. Since the deployment in this post is based on the Helm default values, the settings below are set explicitly in case the default changed. As for Prometheus deploy Grafana via helm install --name grafana --namespace \u0026lt;your-prometheus-namespace\u0026gt; stable/grafana -f values.yaml. Here, the same namespace is chosen for Prometheus and for Grafana.\nvalues.yaml for Grafana:\nserver: ingress: enabled: false service: type: ClusterIP  Check the running state of the pods via the Kubernetes Dashboard or via kubectl get pods -n \u0026lt;your-prometheus-namespace\u0026gt;. In case of not properly running pods check the log files of the pod(s) in question.\nThe text output of Helm after the deployment of Prometheus and Grafana contains very useful information, e.g. the user and password of the Grafana Admin user. The credentials are stored as secrets in the namespace \u0026lt;your-prometheus-namespace\u0026gt; and could be decoded via kubectl get secret --namespace \u0026lt;my-grafana-namespace\u0026gt; grafana-grafana -o jsonpath=\u0026quot;{.data.grafana-admin-password}\u0026quot; | base64 --decode ; echo.\nBasic functional tests The UI of Prometheus and/ Grafana might change which could result in different namings of some of the menu items mentioned below.\nTo access the web UI of both applications use port forwarding of port 9090, since Prometheus web UI is broken when used via K8s HTTP proxy as of November, 2017. More details in this HowTo.\nPort forwarding could be done by\nkubectl port-forward -n \u0026lt;your-prometheus-namespace\u0026gt; \u0026lt;your-prometheus-server-pod\u0026gt; 9090:9090  After port forwarding of port 9090 of your Prometheus server pod enter the web UI of Prometheus http://localhost:9090 in your web browser. Select Graph from the top tab and enter e.g. to show the overall CPU usage for a server (see Prometheus Query Examples)\n100 * (1 - avg by(instance)(irate(node_cpu{mode='idle'}[5m])))  You should get some data, visualized e.g. through a time series graph.\nTo visualize the same data via Grafana, port forward the port 3000 of your Grafana pod and enter the URL of Grafanas web UI http://localhost:3000 in your browser and enter the credentials of the Admin user (see above).\nNext, you need to enter the server name of your Prometheus deployment. This name is shown directly after the installation via helm. By running\nhelm status \u0026lt;your-prometheus-name\u0026gt;  you get again the output presented right after the installation. Below this server name is referenced by \u0026lt;your-preometheus-server-name\u0026gt;.\nFirst, you need to add your Prometheus server as data source. - select Dashboards \u0026rightarrow; Data Sources - select Add data source - enter resp. select\nName: \u0026lt;your-prometheus-datasource-name\u0026gt;\nType: Prometheus\nURL: http://\u0026lt;your-prometheus-server-name\u0026gt;\n_Access: proxy\n- select Save \u0026amp; Test\nIn case of failure check the URL of your Prometheus e.g. via the K8s Dashboard.\nTo add a Graph follow these steps: - in the left corner, select Dashboards \u0026rightarrow; New to create a new dashboard - select Graph to create a new graph - next, select the Panel Title \u0026rightarrow; Edit - select your Prometheus Data Source in the drop down list - enter 100 * (1 - avg by(instance)(irate(node_cpu{mode='idle'}[5m]))) in the entry field A - select the floppy disk symbol (Save) on top\nNow you should have a very basic Prometheus and Grafana setting working in your K8s cluster.\nThe next steps could e.g. be to enable your applications for monitoring by implementing the Prometheus client API.\nLinks  Prometheus Prometheus Helm Chart Prometheus and Kubernetes: A Perfect Match Grafana Grafana Helm Chart  "
},
{
	"uri": "https://gardener.github.io/website/010-blog/2018_week_05/",
	"title": "We are hiring",
	"tags": [],
	"description": "",
	"content": "    \u0026bull;\u0026bull;\u0026bull;\nkubectl describe jobs gardener\u0026crarr; # if you want to join the next generation cloud native  # movement and bring relevant experience in Linux, container # technologies or networking, we look forward to meeting you. \u0026nbsp;  kubectl contact us \u0026Iota;       .blink { animation: blink-animation 1s steps(5, start) infinite; -webkit-animation: blink-animation 1s steps(5, start) infinite; } @keyframes blink-animation { to { visibility: hidden; } } @-webkit-keyframes blink-animation { to { visibility: hidden; } } .shell-wrap { width: 700px; box-shadow: 0 0 30px rgba(0,0,0,0.4); -webkit-border-radius: 3px; -moz-border-radius: 3px; border-radius: 3px; background:#333; } .shell-top-bar { text-align: left; color: black; padding: 5px 0; margin: 0; font-size: 4em; background: transparent; line-height: 0.6em; padding: 0px; padding-left: 8px; } .shell-body { margin: 0; padding: 5px; list-style: none; color: #0CA391; font: 0.8em 'Andale Mono', Consolas, 'Courier New'; line-height: 1.6em; font-size:1em; } .shell-command { color : #E45C3A; } .shell-comment { color: gray; } .shell-body li:before { content: '$'; position: absolute; left: 0; top: 0; color:white; } .shell-body li { word-wrap: break-word; position: relative; padding: 0 0 0 15px; margin-bottom:0px; }  "
},
{
	"uri": "https://gardener.github.io/website/blog/",
	"title": "Whats New",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.github.io/website/components/gardenctl/",
	"title": "gardenctl",
	"tags": [],
	"description": "",
	"content": " Gardenctl \nWhat is gardenctl? gardenctl is a command-line client for administrative purposes for the Gardener. It facilitates the administration of or one many garden, seed and shoot clusters, e.g. to check for issues which occured in one of these clusters. Details about the concept behind the Gardener are described in the Gardener wiki.\nHow to build it Currently, there are no binary builds available, so you need to build it from source,go get it or build the docker image from Dockerfile. Please keep in mind to use an up to date version of golang.\nPrerequisites To build gardenctl from sources you need to have a running Golang environment with dep as dependency management system. Moreover, since gardenctl allows to execute kubectl as well as a running kubectl installation is recommended, but not required. Please check this description for further details.\nBuild gardenctl From source First, you need to create a target folder structure before cloning and building gardenctl.\nmkdir -p ~/go/src/github.com/gardener cd ~/go/src/github.com/gardener git clone /website/030-architecture/15_gardenctl/ cd gardenctl go build gardenctl.go  In case dependencies are missing, run dep ensure and build gardenctl again via go build gardenctl.go.\nAfter the successful build you get the executable gardenctl in the the directory ~/go/src/github.com/gardener/gardenctl. Next, make it available by moving the executable to e.g. /usr/local/bin.\nsudo mv gardenctl /usr/local/bin  gardenctl supports auto completion. This recommended feature is bound to gardenctl or the alias g. To configure it you can run:\necho \u0026quot;gardenctl completion \u0026amp;\u0026amp; source gardenctl_completion.sh \u0026amp;\u0026amp; rm gardenctl_completion.sh\u0026quot; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc  Via go tools First install gardenctl via the go get command.\ngo get github.com/gardener/gardenctl  It will locate the binary under $GOPATH/bin/gardenctl. To generate the auto completion and add it to your ~/.bashrc file, run the following command:\necho \u0026quot;$GOPATH/bin/gardenctl completion \u0026amp;\u0026amp; source gardenctl_completion.sh \u0026amp;\u0026amp; rm gardenctl_completion.sh\u0026quot; \u0026gt;\u0026gt; ~/.bashrc  Via Dockerfile First clone the repository as described in the the build step \u0026ldquo;From source\u0026rdquo;. As next step add the garden \u0026ldquo;config\u0026rdquo; file and \u0026ldquo;clusters\u0026rdquo; folder with the corresponding kubeconfig files for the garden cluster. Then build the container image via docker build -t gardener/gardenctl:v1 . in the cloned repository and run a shell in the image with docker run -it gardener/gardenctl:v1 /bin/bash.\nConfigure gardenctl gardenctl requires a configuration file. The default location is in ~/.garden/config, but it can be overwritten with the environment variable GARDENCONFIG.\nHere an example file:\ngardenClusters: - name: dev kubeConfig: ~/clusters/dev/kubeconfig.yaml - name: prod kubeConfig: ~/clusters/prod/kubeconfig.yaml  The path to the kubeconfig files of a garden cluster can be relative by using the ~ (tilde) expansion or absolute.\ngardenctl caches some information, e.g. the garden project names. The location of this cache is per default $GARDENCTL_HOME/cache. If GARDENCTL_HOME is not set, ~/.garden is assumed.\ngardenctl makes it easy to get additional information of your IaaS provider by using the secrets stored in the corresponding projects in the Gardener. To use this functionality, the CLIs of the IaaS providers need to be available.\nPlease check the IaaS provider documentation for more details about their CLIs. - aws - az - gcloud - openstack\nMoreover, gardenctl offers auto completion. To use it, the command\ngardenctl completion  creates the file gardenctl_completion.sh which can then be sourced later on via\nsource gardenctl_completion.sh  Please keep in mind that the auto completion is bound to gardenctl or the alias g.\nUse gardenctl gardenctl requires the definition of a target, e.g. garden, project, seed or shoot. The following commands, e.g. gardenctl ls shoots usees the target definition as a context for getting the information.\nTargets represent a hierarchical structure of resources. On top, there is/are the garden/s. E.g. in case you setup a development and a production garden, you would have two entries in your ~/.garden/config. Via gardenctl ls gardens you get a list of the available gardens.\n gardenctl get target\nDisplays the current target gardenctl target [garden|project|seed|shoot]\nSet the target e.g. to a garden. It is as well possible to set the target directly to a element deeper in the hierarchy, e.g. to a shoot. gardenctl drop target\nDrop the deepest target.  Examples of basic usage:  List all seed cluster  gardenctl ls seeds List all projects with shoot cluster  gardenctl ls projects Target a seed cluster  gardenctl target seed-gce-dev Target a project  gardenctl target garden-vora Open prometheus ui for a targeted shoot-cluster  gardenctl show prometheus Execute an aws command on a targeted aws shoot cluster  gardenctl aws ec2 describe-instances or  gardenctl aws ec2 describe-instances --no-cache without locally caching credentials Target a shoot directly and get all kube-dns pods in kube-system namespace  gardenctl target myshoot gardenctl kubectl get pods -- -n kube-system | grep kube-dns List all cluster with an issue  gardenctl ls issues Drop an element from target stack  gardenctl drop Open a shell to a cluster node  gardenctl shell nodename  Advanced usage based on JsonQuery The following examples are based on jq. The Json Query Playground offers a convenient environment to test the queries.\nBelow a list of examples:\n List the project name, shoot name and the state for all projects with issues bash gardenctl ls issues -o json | jq '.issues[] | { project: .project, shoot: .shoot, state: .status.lastOperation.state }'  Print all issues of a single project e.g. garden-myproject bash gardenctl ls issues -o json | jq '.issues[] | if (.project==\u0026quot;garden-myproject\u0026quot;) then . else empty end'  Print all issues with error state \u0026ldquo;Error\u0026rdquo; bash gardenctl ls issues -o json | jq '.issues[] | if (.status.lastOperation.state==\u0026quot;Error\u0026quot;) then . else empty end'  Print all issues with error state not equal \u0026ldquo;Succeded\u0026rdquo; bash gardenctl ls issues -o json | jq '.issues[] | if (.status.lastOperation.state!=\u0026quot;Succeeded\u0026quot;) then . else empty end'  Print createdBy information (typically email addresses) of all shoots bash gardenctl k get shoots -- -n garden-core -o json | jq -r \u0026quot;.items[].metadata | {email: .annotations.\\\u0026quot;garden.sapcloud.io/createdBy\\\u0026quot;, name: .name, namespace: .namespace}\u0026quot;   Here a few on cluster analysis:\n Which states are there and how many clusters are in this state?\ngardenctl ls issues -o json | jq '.issues | group_by( .status.lastOperation.state ) | .[] | {state:.[0].status.lastOperation.state, count:length}'  Get all clusters in state Failed\ngardenctl ls issues -o json | jq '.issues[] | if (.status.lastOperation.state==\u0026quot;Failed\u0026quot;) then . else empty end'   "
},
{
	"uri": "https://gardener.github.io/website/components/kubify/",
	"title": "kubify",
	"tags": [],
	"description": "",
	"content": " Terraform Template to Setup a Kubernetes Cluster on OpenStack/AWS/Azure This project contains a terraform environment to setup a kubernetes cluster in various IaaS environments. The following IaaS layers are supported yet:\n Openstack AWS Azure  Kubify supports cluster setup, recovery from an etcd backup file and rolling of cluster nodes. A cluster update is possible as long as the contained components can just be redeployed and the update is done by a rolling update by the kubernetes controllers. The update of the kubelets is supported without rolling the nodes.\nThe target cluster can be enriched by a set of predefinied addons that comes out of the box with this template project.\nFor the technical launch of a cluster bootkube is used on CoreOS VMs.\nThe setup is completely handled by a single terraform project, which uses specific terraform modules handling IaaS related tasks. Recovery is handled by a specific bootstrap etcd deployment together with the regular bootkube mechanism.\nKubify is designed to be used as a git submodule for a cluster specific project that hosts a specific cluster\u0026rsquo;s configuration and the selection of a cluster variant (openstack/aws/azure). Here also the terraform state is stored which enables performing cluster modifications later based on the same terraform project.\nCluster Environment Bastion and Node Access Remote access to cluster nodes can be restricted by a bastion VM. Using a bastion allows omitting public access to every cluster node. Node access will automatically use the bastion host. It is also possible to assign public IP addresses to every node, in this case no bastion host is required.\nBootstrapping All nodes are fully configured by a cloud-init file. Active parts are handled by systemd services. The kubelet is run by using the kubelet wrapper mechanism provided by CoreOS. Therefore always CoreOS images are used for the cluster nodes. The kubelet version is controlled by a dedicated file. In case this is changed the kubelet service is automatically restarted. This can be used to implement version upgrades of kubelets without requiring reinstallation of nodes.\nThe cluster is bootstrapped using bootkube. Bootstrapping is always done by the first (index 0) master node. Therefore a bootkube service is configured on master nodes. It automatically starts when an appropriate configuration file is present. The bootkube configuration is prepared by terraform and propagated to the first master node by a remote provisioner. This is done exactly once. On subsequent calls to terraform an appropriate auto terraform configuration file is generated which disables the bootstrap resource in order to avoid repeated bootstrapping.\nRecovery Kubify supports cluster recovery from an etcd backup file. Recovery uses the terraform project used to setup the cluster by enabling the recovery mode and executing terraform apply, again. In recovery mode the master nodes and volumes are recreated and the cluster bootstrap is reinitiated using the etcd backup in order to initialize the bootstrap etcd cluster. This is the only difference compared to an initial cluster setup.\nETCD The current setup uses a self hosted etcd approach supported by bootkube and the etcd operator. The etcd is scaled with the master nodes and uses a persistent volume for their persistence. For bootrapped clusters the etcd operator does not support kubernetes volumes, therefore a VM volume for master nodes is created and used to back the host path used by the etcd cluster\nAPI Server The API Server can only run on master nodes. It uses the node port 443. This is used to create a load balancer for this port and the set of master nodes.\nIngress The same mechanism is used to statically provide a load balancer for the nginx ingress controller for http(s) access. The nginx pods are running on the worker nodes, only. Here the ports 80 and 443 are used as node ports on the worker nodes.\nBasically nginx could also use a load balancer created by a service object, but then it would not be possible to generate the DNS and load balancer environment in advance using terraform. This could be changed if either the public IP assigned to the load balancer could be configured by the service object or if there would be a DNS controller in Kubernetes. But then the constraint for the IaaS layer is to offer (???) load balancers with dedicated public IPs. This is, for example, not the case for AWS (???). Always using statically provided load balancers for the ingress should works for all environments.\nRolling updates of nodes. If the configuration of master or worker nodes require a recreation of VMs, this is done by upating each node with a separate call to terraform apply one by one. With every successfull execution the update of the next master/worker node is triggered. This is completely implemented by the terraform configuration files and by keeping the actual state in a separate json file which can later again be read as a terraform variable file (auto file). Configuration variables allow to configure the behaviour of node rolling.\nSetup a new cluster  Create a new git repository representing the desired new cluster This repository is used to keep track of the used terraform template version, the terraform configuration and state of the cluster.   mkdir \u0026lt;cluster-name\u0026gt; cd \u0026lt;cluster-name\u0026gt; git init git remote add origin \u0026lt;git repository url\u0026gt;   Prepare the cluster project content  Add a .gitignore file\n .terraform/ terraform/ gen/ tmp/ .sw[opq]   Add the kubify repository as a git submodule with the name k8s.   git submodule add /website/030-architecture/30_kubify/ k8s git submodule init   Select the cluster IaaS variant to be used Create a symbolic link variant pointing to the variant to use. Variants are stored in folder variants inside the submodule residing in folder k8s.  Currently three variants are supported - openstack - aws - azure\nFor example:\n ln -s k8s/variants/openstack variant  Starting from terraform 0.10.6 symbolic link handling does not work correctly anymore. Here you cannot use the direct variant link anymore. Instead run\n k8s/bin/prepare \u0026lt;variant name\u0026gt;  to select the variant for your cluster project. If the variant has already been configured earlier, the argument can be omitted. This script generates a copy of the k8s module and replaces the indirect link variants/current by a direct link to the desired variant, instead of using the variant link in the cluster project. Whenever a new version of Kubify (this project) is used for the cluster project, this copy must be updated by calling prepare again.\n Add the configuration values for your cluster (terraform.tfvars)  For openstack this could look like this:\n os_user_name=\u0026quot;\u0026lt;your user\u0026gt;\u0026quot; os_password=\u0026quot;\u0026lt;your password\u0026gt;\u0026quot; os_auth_url=\u0026quot;\u0026lt;your keystone auth url\u0026gt;\u0026quot; os_tenant_name=\u0026quot;\u0026lt;your tenant name\u0026gt;\u0026quot; os_domain_name=\u0026quot;\u0026lt;your openstack domain name\u0026gt;\u0026quot; os_region=\u0026quot;\u0026lt;your region\u0026gt;\u0026quot; os_fip_pool_name=\u0026quot;\u0026lt;your fip pool name\u0026gt;\u0026quot; cluster_name=\u0026quot;\u0026lt;your cluster name\u0026gt;\u0026quot; cluster_type=\u0026quot;\u0026lt;your cluster type\u0026gt;\u0026quot; # DNS dns = { dns_type = \u0026quot;route53\u0026quot; access_key=\u0026quot;\u0026lt;route53 access key\u0026gt;\u0026quot; secret_key=\u0026quot;\u0026lt;route53 secret key\u0026gt;\u0026quot; hosted_zone_id=\u0026quot;\u0026lt;route53 hosted zone id\u0026gt;\u0026quot; hosted_zone_domain=\u0026quot;\u0026lt;domain\u0026gt;\u0026quot; } # cluster size master = { count=3 flavor_name=\u0026quot;medium_4_8\u0026quot; } worker = { count=3 flavor_name=\u0026quot;medium_2_4\u0026quot; }   Setup the cluster Now you can call regular terraform commands for the project variant   terraform init variant terraform plan variant terraform apply variant  Helper commands  Terraform commands There are shortcuts available including the get step during development:   k8s/bin/plan k8s/bin/apply k8s/bin/tf \u0026lt;terraform command\u0026gt;  This command sequence automatically runs the prepare step and the terraform get step, if required.\n Access to master, worker and bastion nodes   k8s/bin/master [\u0026lt;n\u0026gt; [\u0026lt;shell command to execute on master\u0026gt;]] k8s/bin/worker [\u0026lt;n\u0026gt; [\u0026lt;shell command to execute on worker\u0026gt;]] k8s/bin/master list # list all IPs of VMs intended for master nodes k8s/bin/worker list # list all IPs of VMs intended for worker nodes k8s/bin/bastion   Access to the api server The config file for kubectl can be found in gen/assets/auth/kubeconfig. Or just call  k8s/bin/k \u0026lt;options\u0026gt; # for kubectl k8s/bin/ks \u0026lt;options\u0026gt; # for kubectl -n kube-system `k8s/bin/kurl # for curl /   Quick Setup Alternatively you can copy the project templates prepared for your IaaS environment of choice: - for Openstack: https://github.com/gardener/kubify-openstack-template.git - for AWS: https://github.com/gardener/kubify-aws-template.git\nAfterwards you should set the origin to your upstream git repository. It is strongly recommended to maintain such a cluster project keeping track of the actual state of your cluster.\nPlease remember: you should use a PRIVATE repository, because it contains secrets and access keys for the cluster AND the selected IaaS environment.\nThen continue with the configuration steps above.\nIt uses the same submodule, so don\u0026rsquo;t forget to call\ngit submodule update --init --recursive  Configuration Settings First of all there are required settings for every IaaS environment: - Openstack - AWS - Azure\nIn addition there are various general settings that are independent of the chosen platform variant. Most of them have defaults that typically don\u0026rsquo;t need to be changed.\nThe folloing variables are required for every cluster:\n|Name|Meaning| |\u0026ndash;|\u0026ndash;| |cluster_name|Name of the cluster. Used for DNS entries, the kubernetes cluster and for labelling the IaaS elements| |cluster_type|Type key of the cluster (for example: seed or infra). It ts used for the DNS entries and labeling the IaaS elements| |dashboard_creds|The credential file the kubernetes dashboard created by htpasswd. Unfortunately terraform does not support bcrypt up to now| |master|Settings for master nodes (see below)| |worker|Settimgs for worker nodes (see below)| |bastion|Settimgs for bastion VM (see below)|\nmaster and worker use the following common structure\n|Name|Meaning| |\u0026ndash;|\u0026ndash;| |count|Number of nodes (for master this should be an odd number, because it is also used to scale the cluster etcd service| |flavor_name|Optional: flavor used for the VM| |image_name|Optional: image used for the VM| |assign_fips|Optional: Always assign public IP to nodes| |root_volume_size|Optional: Size in GB of root volume (default: 50)| |volume_size|Optional: Size in GB of persistent volume for master (default: 20), should not be used for worker| |update_mode|Optional: Update mode for VMs| |generation|Optional: explicit node generation count to enforce recreation|\nFor the bastion settings the following structure is used\n|Name|Meaning| |\u0026ndash;|\u0026ndash;| |flavor_name|Optional: flavor used for the VM| |image_name|Optional: image used for the VM| |user_name|Optional: user name of initial admin user of image|\nDNS configuration is done via the input map dns. It contains an entry dns_type to select the desired DNS provider and additional configuration entries required for that provider.\nSo far only route53 is supported as DNS provider. Therefore dedicated AWS credentials with appropriate permissions are required.\nConfiguration of the route53 DNS provider:\n|Name|Meaning| |\u0026ndash;|\u0026ndash;| |access_key|AWS access key for route53 access| |secret_key|AWS secret key for route53 access| |hosted_zone_id|Id of AWS hosted zone to use for the DNS entries| |hosted_zone_domain|Optional: domain name of hosted zone. Required if no base_domain or domain_name is configured for cluster. It is used to generate a unique domain name for clusters generated with kubify in this domain.|\nEtcd can be configured to create backups using the etcd operator. This can also be configured with the terraform project using the input map variable etcd_backup. The folowing backup modes are supported: - s3: AWS S3 - pv: Persistent volume in cluster The default is pv. An appropriate kubernetes storage class is always created by the platform specific parts.\nThe mode is configured with the map entry storage_type. Other entries are used to configure the selected storage type.\nFor s3 AWS credentials must be configured:\n|Name|Meaning| |\u0026ndash;|\u0026ndash;| |access_key|AWS access key for S3 access| |secret_key|AWS secret key for S3 access|\nBesides those mandatory settings there are various optional settings.\nAddons Several addons can be chosen for optional deployment. For addons helm deployment manifests are supported. Therefore the standard control plane is extended by the helm controller and tiller deployment.\n|Name|Meaning| |\u0026ndash;|\u0026ndash;| |addons|map with addon and their configuration settings (see addons) |deploy_tiller|addons may now use helm manifests. If this is not requited the tiller/helm operator deployment can be disabled. (default: true)|\nGeneral settings |Name|Meaning| |\u0026ndash;|\u0026ndash;| |ca_cert_pem|Root certificate to be used for the cluster. By default generated for the dedicated cluster| |ca_key_pem|Key for the root certificate. Must always be configured together with ca_cert_pem| |base_domain|Base domain used for the cluster instead of generating it. The cluster name is still prepended| |domain_name|Domain name of the cluster. Sub domains are created for api, ingress,\u0026hellip;.| |additional_domains|List of additional domain names used for certificate generation|\nNode settings These are otional settings for all kinds of nodes\n|Name|Meaning| |\u0026ndash;|\u0026ndash;| |root_certs_file|File containing additional root certificates to be installed on the nodes| |dns_nameservers|List of DNS servers to configure for the subnets and/or VMs| |flavor_name|Default flavor for all kinds of VMs| |subnet_cidr|IP range for node subnet (defaulted by IaaS modules)|\nIt is possible to specify IaaS specific names (or search pattern for AWS). Additionally some mappings for preconfigured names come out of the box with this project:\n ubuntu-16.04 coreos-1520.6.0 (not on Azure) coreos-1548.3.0 (Azure)  Actual settings can be found here. With every new release more preconfigured settings are delivered with Kubify.\n|Name|Meaning| |\u0026ndash;|\u0026ndash;| |use_bastion|Use bastion host to avoid public node IPs if possible (depends on IAAS environment)| |use_lbaas|For testing purposes switch off load balancers| |configure_additional_dns|Create DNS entries for additional_domains|\nKubernetes Configuration |Name|Meaning| |\u0026ndash;|\u0026ndash;| |oidc_issuer_domain|OIDC configuration for API server| |oidc_issuer_subdomain|OIDC configuration for API server| |oidc_client_id|OIDC configuration for API server, default: kube-kubectl| |oidc_username_claim|OIDC configuration for API server, default: email| |oidc_groups_claim|OIDC configuration for API server, default: groups| |service_cidr|Kubenetes cluster ip range (services) (default: 10.241.0.0/17)| |pod_cidr|Kubenetes pod ip range (default: 10.241.128.0/17)| |event_ttl|Time to live for events (default: 48h0m0s)|\nVM update |Name|Meaning| |\u0026ndash;|\u0026ndash;| |node_update_mode|Standard node update mode (Roll, All or None)| |update_kubelet|handle kubelet update (default: false)|\nAdditional node kind specific settings can be set using the master and worker configuration variables.\nRecovery |Name|Meaning| |\u0026ndash;|\u0026ndash;| |recover_cluster|Enable cluster recovery mode if set to `true| |etcd_backup_file|Local path to the etcd backup file|\nComponent versions Versions are coming with the releases of this project. It is possible to override various versions, but there is no guarantee a selected combination of component versions works flawlessly (and with this terraform script) Actual versions are defined here.\nVersions can be configured as member of a map variable versions. The following keys are supported:\n|Name|Meaning| |\u0026ndash;|\u0026ndash;| |image_name|image used for cluster VMs (master and nodes)| |bastion_image_name|image used for bastion VMs| |kubernetes_version|kubernetes version| |dns_version|DNS version| |flannel_version|flannel version| |cni_version|CNI version| |etcd_version|etcd version| |etcd_operator_version|etcd operator version| |bootkube|bootkube image name (default: quay.io/coreos/bootkube)| |bootkube_version|bootkube version| |kubernetes_hyperkube|image base name (default: gcr.io/google_containers/hyperkube)| |kubernetes_hyperkube_patch|image name suffix (default: _coreos.0)| |nginx_version|nginx version| |lego_version|lego version|\nCluster Recovery Store your etcd backup file somewhere in the filesystem and add the following settings to your terraform.tfvars file.\nrecover_cluster = \u0026quot;true\u0026quot; etcd_backup_file= \u0026quot;\u0026lt;your etcd.backup file path\u0026gt;\u0026quot;  Now just re-run terraform apply variant (or try plan first).\nThis recreates the master volumes and nodes and then recreates the cluster using the backup as initial etcd content. All manual changes to your deployment of the standard components will be lost, because all standard components (not only the control plane) will be updated to the version configured in the terraform project.\nMigrating to new Versions of the Kubify Project Every new version of this project comes with a migration script migration.mig that is used by the command migrate. It is used to adapt terraform state files generated by former versions of this project before they can be used with the actual version. With this mechanism it is possible to change the structure of the terraform project for new versions without loosing cluster projects based on older versions. Nevertheless there might still be changes that cannot be migrated. The migration script then prompts an information about the latest commit, that can be used with the actual state version.\nFor this purpose the state structure used by this project is versioned (in modules/instance/version.tf). Running the project generates a file structure_version containing the actual version. Additionally the version is also contained in the state.auto.tfvars file.\nThe migration of the actual state file to the lastest structure version is done version by version until the actual version is reached.\nThe utility command apply, plan and tf implicitly do the state migration if required. If terraform is called manually the migration steps have be explicitly done by using the migrate utility command. It can always be called, because it checks if an action is required before touching the state file.\nThe migration command provides some utility functions used by the migration script. After defining those functions it just calls the migration script as regular shell script. The utility functions are based on the state manipulation sub commands of the terraform command.\n"
},
{
	"uri": "https://gardener.github.io/website/010-blog/2018_week_08/",
	"title": "tail -f /var/log/my-application.log",
	"tags": [],
	"description": "",
	"content": "One thing that always bothered me was that I couldn\u0026rsquo;t get logs of several pods at once with kubectl. A simple tail -f \u0026lt;path-to-logfile\u0026gt; isn\u0026rsquo;t possible at all. Certainly you can use kubectl logs -f \u0026lt;pod-id\u0026gt;, but it doesn\u0026rsquo;t help if you want to monitor more than one pod at a time.\nThis is something you really need a lot, at least if you run several instances of a pod behind a deployment. This is even more so if you don\u0026rsquo;t have a Kibana setup or similar.\nLuckily, there are smart developers out there who always come up with solutions. The finding of the week is a small bash script that allows you to aggregate log files of several pods at the same time in a simple way. The script is called kubetail and is available at https://github.com/johanhaleby/kubetail.\n"
},
{
	"uri": "https://gardener.github.io/website/050-tutorials/content/howto/tail-logfile/",
	"title": "tail -f /var/log/my-application.log",
	"tags": [],
	"description": "Aggregate log files from different pods",
	"content": " Problem One thing that always bothered me was that I couldn\u0026rsquo;t get logs of several pods at once with kubectl. A simple tail -f \u0026lt;path-to-logfile\u0026gt; isn\u0026rsquo;t possible at all. Certainly you can use kubectl logs -f \u0026lt;pod-id\u0026gt;, but it doesn\u0026rsquo;t help if you want to monitor more than one pod at a time.\nThis is something you really need a lot, at least if you run several instances of a pod behind a deployment. This is even more so if you don\u0026rsquo;t have a Kibana setup or similar.\nSolution Luckily, there are smart developers out there who always come up with solutions. The finding of the week is a small bash script that allows you to aggregate log files of several pods at the same time in a simple way. The script is called kubetail and is available at https://github.com/johanhaleby/kubetail.\n"
}]